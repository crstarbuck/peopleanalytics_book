<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Statistical Fundamentals | An Introduction to the People Analytics Lifecycle: With Applications in R and Data Studio</title>
  <meta name="description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Statistical Fundamentals | An Introduction to the People Analytics Lifecycle: With Applications in R and Data Studio" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="github-repo" content="crstarbuck/peopleanalytics-lifecycle-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Statistical Fundamentals | An Introduction to the People Analytics Lifecycle: With Applications in R and Data Studio" />
  
  <meta name="twitter:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  

<meta name="author" content="Craig Starbuck" />


<meta name="date" content="2022-01-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="research-methods.html"/>
<link rel="next" href="measurement-sampling.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i><b>1</b> Foreword</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>3</b> Getting Started</a><ul>
<li class="chapter" data-level="3.1" data-path="getting-started.html"><a href="getting-started.html#guiding-principles"><i class="fa fa-check"></i><b>3.1</b> Guiding Principles</a></li>
<li class="chapter" data-level="3.2" data-path="getting-started.html"><a href="getting-started.html#tools"><i class="fa fa-check"></i><b>3.2</b> Tools</a></li>
<li class="chapter" data-level="3.3" data-path="getting-started.html"><a href="getting-started.html#d-framework"><i class="fa fa-check"></i><b>3.3</b> 4D Framework</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="research-methods.html"><a href="research-methods.html"><i class="fa fa-check"></i><b>4</b> Research Methods</a></li>
<li class="chapter" data-level="5" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html"><i class="fa fa-check"></i><b>5</b> Statistical Fundamentals</a><ul>
<li class="chapter" data-level="5.1" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#populations-samples"><i class="fa fa-check"></i><b>5.1</b> Populations &amp; Samples</a></li>
<li class="chapter" data-level="5.2" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#univariate-analysis"><i class="fa fa-check"></i><b>5.2</b> Univariate Analysis</a><ul>
<li class="chapter" data-level="5.2.1" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>5.2.1</b> Measures of Central Tendency</a></li>
<li class="chapter" data-level="5.2.2" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#measures-of-spread"><i class="fa fa-check"></i><b>5.2.2</b> Measures of Spread</a></li>
<li class="chapter" data-level="5.2.3" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#bivariate-analysis"><i class="fa fa-check"></i><b>5.2.3</b> Bivariate Analysis</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#inferential-statistics"><i class="fa fa-check"></i><b>5.3</b> Inferential Statistics</a><ul>
<li class="chapter" data-level="5.3.1" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#introduction-to-probability"><i class="fa fa-check"></i><b>5.3.1</b> Introduction to Probability</a></li>
<li class="chapter" data-level="5.3.2" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#central-limit-theorem"><i class="fa fa-check"></i><b>5.3.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3.3" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#confidence-intervals"><i class="fa fa-check"></i><b>5.3.3</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="statistical-fundamentals.html"><a href="statistical-fundamentals.html#exercises"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="measurement-sampling.html"><a href="measurement-sampling.html"><i class="fa fa-check"></i><b>6</b> Measurement &amp; Sampling</a></li>
<li class="chapter" data-level="7" data-path="data-preparation.html"><a href="data-preparation.html"><i class="fa fa-check"></i><b>7</b> Data Preparation</a><ul>
<li class="chapter" data-level="7.1" data-path="data-preparation.html"><a href="data-preparation.html#data-wrangling"><i class="fa fa-check"></i><b>7.1</b> Data Wrangling</a></li>
<li class="chapter" data-level="7.2" data-path="data-preparation.html"><a href="data-preparation.html#feature-engineering"><i class="fa fa-check"></i><b>7.2</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="analysis-of-differences.html"><a href="analysis-of-differences.html"><i class="fa fa-check"></i><b>8</b> Analysis of Differences</a><ul>
<li class="chapter" data-level="8.1" data-path="analysis-of-differences.html"><a href="analysis-of-differences.html#comparing-2-distributions"><i class="fa fa-check"></i><b>8.1</b> Comparing 2 Distributions</a></li>
<li class="chapter" data-level="8.2" data-path="analysis-of-differences.html"><a href="analysis-of-differences.html#comparing-3-distributions"><i class="fa fa-check"></i><b>8.2</b> Comparing 3+ Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="inferential-models.html"><a href="inferential-models.html"><i class="fa fa-check"></i><b>9</b> Inferential Models</a><ul>
<li class="chapter" data-level="9.1" data-path="inferential-models.html"><a href="inferential-models.html#regression"><i class="fa fa-check"></i><b>9.1</b> Regression</a></li>
<li class="chapter" data-level="9.2" data-path="inferential-models.html"><a href="inferential-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>9.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="9.2.1" data-path="inferential-models.html"><a href="inferential-models.html#parameter-estimation"><i class="fa fa-check"></i><b>9.2.1</b> Parameter Estimation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="inferential-models.html"><a href="inferential-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>9.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="9.3.1" data-path="inferential-models.html"><a href="inferential-models.html#moderation"><i class="fa fa-check"></i><b>9.3.1</b> Moderation</a></li>
<li class="chapter" data-level="9.3.2" data-path="inferential-models.html"><a href="inferential-models.html#mediation"><i class="fa fa-check"></i><b>9.3.2</b> Mediation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="inferential-models.html"><a href="inferential-models.html#polynomial-regression"><i class="fa fa-check"></i><b>9.4</b> Polynomial Regression</a></li>
<li class="chapter" data-level="9.5" data-path="inferential-models.html"><a href="inferential-models.html#logistic-regression"><i class="fa fa-check"></i><b>9.5</b> Logistic Regression</a><ul>
<li class="chapter" data-level="9.5.1" data-path="inferential-models.html"><a href="inferential-models.html#binomial"><i class="fa fa-check"></i><b>9.5.1</b> Binomial</a></li>
<li class="chapter" data-level="9.5.2" data-path="inferential-models.html"><a href="inferential-models.html#multinomial"><i class="fa fa-check"></i><b>9.5.2</b> Multinomial</a></li>
<li class="chapter" data-level="9.5.3" data-path="inferential-models.html"><a href="inferential-models.html#ordinal"><i class="fa fa-check"></i><b>9.5.3</b> Ordinal</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="inferential-models.html"><a href="inferential-models.html#hierarchical-models"><i class="fa fa-check"></i><b>9.6</b> Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="predictive-models.html"><a href="predictive-models.html"><i class="fa fa-check"></i><b>10</b> Predictive Models</a><ul>
<li class="chapter" data-level="10.1" data-path="predictive-models.html"><a href="predictive-models.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>10.1</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="10.2" data-path="predictive-models.html"><a href="predictive-models.html#cross-validation"><i class="fa fa-check"></i><b>10.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="10.3" data-path="predictive-models.html"><a href="predictive-models.html#balancing-classes"><i class="fa fa-check"></i><b>10.3</b> Balancing Classes</a></li>
<li class="chapter" data-level="10.4" data-path="predictive-models.html"><a href="predictive-models.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="automated-machine-learning-automl.html"><a href="automated-machine-learning-automl.html"><i class="fa fa-check"></i><b>11</b> Automated Machine Learning (AutoML)</a></li>
<li class="chapter" data-level="12" data-path="unsupervised-learning-models.html"><a href="unsupervised-learning-models.html"><i class="fa fa-check"></i><b>12</b> Unsupervised Learning Models</a><ul>
<li class="chapter" data-level="12.1" data-path="unsupervised-learning-models.html"><a href="unsupervised-learning-models.html#factor-analysis"><i class="fa fa-check"></i><b>12.1</b> Factor Analysis</a></li>
<li class="chapter" data-level="12.2" data-path="unsupervised-learning-models.html"><a href="unsupervised-learning-models.html#clustering"><i class="fa fa-check"></i><b>12.2</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>13</b> Data Visualization</a></li>
<li class="chapter" data-level="14" data-path="data-storytelling.html"><a href="data-storytelling.html"><i class="fa fa-check"></i><b>14</b> Data Storytelling</a></li>
<li class="chapter" data-level="15" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i><b>15</b> Bibliography</a></li>
<li class="chapter" data-level="16" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>16</b> Appendix</a><ul>
<li class="chapter" data-level="16.1" data-path="appendix.html"><a href="appendix.html#exercise-solutions"><i class="fa fa-check"></i><b>16.1</b> Exercise Solutions</a></li>
<li class="chapter" data-level="16.2" data-path="appendix.html"><a href="appendix.html#d-framework-1"><i class="fa fa-check"></i><b>16.2</b> 4D Framework</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to the People Analytics Lifecycle</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-fundamentals" class="section level1">
<h1><span class="header-section-number">5</span> Statistical Fundamentals</h1>
<p>This chapter surveys a number of univariate, bivariate, and inferential analysis concepts that underpin the more complex multivariate approaches in subsequent chapters of this book.</p>
<div id="populations-samples" class="section level2">
<h2><span class="header-section-number">5.1</span> Populations &amp; Samples</h2>
<p>The goal of research is to understand a population based on data from a subset of population members. In practice, it is often not feasible to collect data from every member of a population, so sampling enables us to calculate statistics based on a smaller sample – such as people willing to participate in a survey – in order to learn about the parameters of the larger population.</p>
<p>Another important concept is the sampling frame. While the population represents the entire group of interest, the sampling frame represents the subset of the population to which the researcher has access. In an ideal setting, the population and sampling frame are the same, but they are often different in practice. For example, a professor may be interested in understanding student sentiment about a new school policy but only has access to collect data from students in the courses she teaches. In this case, the entire student body is the population but the students she has access to (those in the courses she teaches) represent the sampling frame. The sample is the subset of the sampling frame that ultimately participates in the research (e.g,. those who complete a survey or participate in a focus group).</p>
</div>
<div id="univariate-analysis" class="section level2">
<h2><span class="header-section-number">5.2</span> Univariate Analysis</h2>
<p>Descriptive statistics are rudimentary analysis techniques that help describe and summarize data in a meaningful way. Descriptive statistics do not allow us to draw any conclusions beyond the available data but are helpful in interpreting the data at hand.</p>
<p>There are two categories of descriptive statistics: (1) measures of central tendency and (2) measures of spread. Measures of central tendency describe the central position in a set of data, while measures of spread describe how dispersed the data are.</p>
<div id="measures-of-central-tendency" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Measures of Central Tendency</h3>
<p><b>Mean</b></p>
<p>Perhaps the most intuitive measure of central tendency is the mean, which is often referred to as the average. The mean of a sample is denoted by <span class="math inline">\(\bar{x}\)</span> and is defined by:</p>
<p><span class="math display">\[ \bar{X} = \frac{\sum_{i=1}^{n} x_{i}}{n} \]</span></p>
<p>The mean of a set of numeric values can be calculated using the mean() function in R:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="co"># Fill vector x with integers</span></a>
<a class="sourceLine" id="cb3-2" data-line-number="2">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"></a>
<a class="sourceLine" id="cb3-4" data-line-number="4"><span class="co"># Calculate average of vector x</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5"><span class="kw">mean</span>(x)</a></code></pre></div>
<pre><code>## [1] 6.9</code></pre>
<p><b>Median</b></p>
<p>The median represents the midpoint in a sorted vector of numbers. For vectors with an even number of values, the median is the average of the middle two numbers; it is simply the middle number for vectors with an odd number of values. When the distribution of data is skewed, or there is an extreme value like we observe in vector x, the median is a better measure of central tendency.</p>
<p>The median() function in R can be used to handle the sorting and midpoint selection:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="co"># Calculate median of vector x</span></a>
<a class="sourceLine" id="cb5-2" data-line-number="2"><span class="kw">median</span>(x)</a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<p>In this example, the median is only 2 compared with the mean of 6.9 (which is not really representative of any of the values in vector x). Large deltas between mean and median values are evidence of outliers.</p>
<p><b>Mode</b></p>
<p>The mode is the most frequent number in a set of values.</p>
<p>While mean() and median() are standard functions in R, mode() returns the internal storage mode of the object rather than the statistical mode of the data. We can easily create a function to return the statistical mode(s):</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="co"># Create function to calculate statistical mode(s)</span></a>
<a class="sourceLine" id="cb7-2" data-line-number="2">stat.mode &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb7-3" data-line-number="3">  ux &lt;-<span class="st"> </span><span class="kw">unique</span>(x)</a>
<a class="sourceLine" id="cb7-4" data-line-number="4">  tab &lt;-<span class="st"> </span><span class="kw">tabulate</span>(<span class="kw">match</span>(x, ux))</a>
<a class="sourceLine" id="cb7-5" data-line-number="5">  ux[tab <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(tab)]</a>
<a class="sourceLine" id="cb7-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb7-7" data-line-number="7"></a>
<a class="sourceLine" id="cb7-8" data-line-number="8"><span class="co"># Return mode(s) of vector x</span></a>
<a class="sourceLine" id="cb7-9" data-line-number="9"><span class="kw">stat.mode</span>(x)</a></code></pre></div>
<pre><code>## [1] 1 2</code></pre>
<p>In this case, we have a bimodal distribution since both 1 and 2 occur most frequently.</p>
<p><b>Range</b></p>
<p>The range is the difference between the maximum and minimum values in a set of numbers.</p>
<p>The range() function in R returns the minimum and maximum numbers:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="co"># Return lowest and highest values of vector x</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2"><span class="kw">range</span>(x)</a></code></pre></div>
<pre><code>## [1]  1 50</code></pre>
<p>We can leverage the max() and min() functions to calculate the difference between these values:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="co"># Calculate range of vector x</span></a>
<a class="sourceLine" id="cb11-2" data-line-number="2"><span class="kw">max</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 49</code></pre>
</div>
<div id="measures-of-spread" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Measures of Spread</h3>
<p><b>Variance</b></p>
<p>Variance is a measure of the variability around the average value. Variance is calculated using the average of squared differences from the mean.</p>
<p>Variance of a population is defined by:</p>
<p><span class="math display">\[ \sigma^{2} = \frac{\sum (X_{i}-\mu)^{2}}{N} \]</span></p>
<p>Variance of a sample is defined by:</p>
<p><span class="math display">\[ s^{2} = \frac{\sum (x_{i}-\bar{x})^{2}}{n-1} \]</span></p>
<p>It is important to note that since differences are squared, the variance is always non-negative. In addition, we cannot compare these squared differences to the arithmetic mean since the units are different. For example, if we calculate the variance of total working years measured in USD, variance should be expressed as USD squared while the mean exists in the original USD unit of measurement.</p>
<p>In R, the sample variance can be calculated using the var() function:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="co"># Load library for data wrangling</span></a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a></code></pre></div>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 4.0.2</code></pre>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="co"># Read employee tenure data</span></a>
<a class="sourceLine" id="cb15-2" data-line-number="2">tenure &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_lifecycle_book/master/data/tables/tbl_tenure.csv&quot;</span>)</a>
<a class="sourceLine" id="cb15-3" data-line-number="3"></a>
<a class="sourceLine" id="cb15-4" data-line-number="4"><span class="co"># Calculate sample variance for total working years</span></a>
<a class="sourceLine" id="cb15-5" data-line-number="5"><span class="kw">var</span>(tenure<span class="op">$</span>total_working_years)</a></code></pre></div>
<pre><code>## [1] 60.54056</code></pre>
<p>Sample statistics are the default in R. Since the population variance differs from the sample variance by a factor of <code>var(vector) * (n - 1) / n</code>, it is simple to convert output from var() to the population variance:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># Store number of observations</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2">n =<span class="st"> </span><span class="kw">nrow</span>(data)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3"></a>
<a class="sourceLine" id="cb17-4" data-line-number="4"><span class="co"># Calculate population variance for total working years</span></a>
<a class="sourceLine" id="cb17-5" data-line-number="5"><span class="kw">var</span>(tenure<span class="op">$</span>total_working_years) <span class="op">*</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>n</a></code></pre></div>
<pre><code>## numeric(0)</code></pre>
<p><b>Standard Deviation</b></p>
<p>The standard deviation is simply the square root of the variance, as defined by:</p>
<p><span class="math display">\[ s = \sqrt{\frac{\sum (x_{i} - \bar{x})^{2}}{N - 1}} \]</span></p>
<p>Since a squared value can be converted back to its original units by taking its square root, the standard deviation expresses variability around the mean in the variable’s original units.</p>
<p>In R, the sample standard deviation can be calculated using the sd() function:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="co"># Calculate sample standard deviation for total working years</span></a>
<a class="sourceLine" id="cb19-2" data-line-number="2"><span class="kw">sd</span>(tenure<span class="op">$</span>total_working_years)</a></code></pre></div>
<pre><code>## [1] 7.780782</code></pre>
<p>Since the population standard deviation differs from the sample standard deviation by a factor of <code>sd(vector) * sqrt((n - 1) / n)</code>, it is simple to convert output from sd() to the population standard deviation:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="co"># Calculate population standard deviation for total working years</span></a>
<a class="sourceLine" id="cb21-2" data-line-number="2"><span class="kw">sd</span>(tenure<span class="op">$</span>total_working_years) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>((n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>n)</a></code></pre></div>
<pre><code>## numeric(0)</code></pre>
<p><b>Quartiles</b></p>
<p>Quartiles are a staple of exploratory data analysis (EDA). A quartile is a type of quantile that partitions data into four equally sized parts (with regard to the number of data points in each quartile, not the range of values) after ordering the data. Quartiles also correspond to percentiles. For example, Q1 is the 25th percentile – the value below which 25% of values exist.</p>
<p>The Interquartile Range (IQR) represents the difference between Q3 and Q1 cutpoint values (the middle two quartiles). The IQR is sometimes used to detect extreme values in a distribution. Values less than <code>Q1 - 1.5 * IQR</code> or greater than <code>Q3 + 1.5 * IQR</code> are generally considered outliers.</p>
<p>In R, the quantile() function returns the values that bookend each quartile:</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1"><span class="co"># Return quartiles for total working years</span></a>
<a class="sourceLine" id="cb23-2" data-line-number="2"><span class="kw">quantile</span>(tenure<span class="op">$</span>total_working_years)</a></code></pre></div>
<pre><code>##   0%  25%  50%  75% 100% 
##    0    6   10   15   40</code></pre>
<p>We can return a specific percentile value using the <code>probs</code> argument in the quantile() function:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="co"># Return 80th percentile total working years value</span></a>
<a class="sourceLine" id="cb25-2" data-line-number="2"><span class="kw">quantile</span>(tenure<span class="op">$</span>total_working_years, <span class="dt">probs =</span> <span class="fl">.8</span>)</a></code></pre></div>
<pre><code>## 80% 
##  17</code></pre>
<p>In addition, the summary() function returns several common descriptive statistics for an object:</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="co"># Return common descriptives</span></a>
<a class="sourceLine" id="cb27-2" data-line-number="2"><span class="kw">summary</span>(tenure<span class="op">$</span>total_working_years)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00    6.00   10.00   11.28   15.00   40.00</code></pre>
</div>
<div id="bivariate-analysis" class="section level3">
<h3><span class="header-section-number">5.2.3</span> Bivariate Analysis</h3>
<p><b>Covariance</b></p>
<p>While variance provides an understanding of how values for a single variable vary, covariance is an unstandardized measure of how two variables vary together. Values can range from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>, and these values can be used to understand the direction of the linear relationship between variables. Positive covariance values indicate that the variables vary in the same direction (e.g., tend to increase or decrease together), while negative covariance values indicate that the variables vary in opposite directions (e.g., when one increases, the other decreases, or vice versa).</p>
<p>Covariance is defined by:</p>
<p><span class="math display">\[ cov_{x,y} = \frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{N-1} \]</span></p>
<p>It’s important to note that while covariance aids our understanding of the direction of the relationship between two variables, we cannot use it to understand the strength of the relationship since it is unstandardized. Due to differences in variables’ units of measurement, the strength of the relationship between two variables with large covariance could be weak, while the strength of the relationship between another pair of variables with small covariance could be strong.</p>
<p>In R, we can compute the covariance between a pair of numeric variables using the cov() function:</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb29-1" data-line-number="1"><span class="co"># Calculate covariance between total working years and age using complete observations (missing values will cause issues if not addressed)</span></a>
<a class="sourceLine" id="cb29-2" data-line-number="2"><span class="kw">cov</span>(tenure<span class="op">$</span>total_working_years, tenure<span class="op">$</span>years_at_company, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>## [1] 30.62278</code></pre>
<p>In this example, using the default Pearson method, the covariance between total working years and age is 453179.1. Since the value is positive, we know that total working years is generally higher for older employees and lower for younger employees.</p>
<p>Since total working years and age are both measured on continuous scales, the default Pearson method for evaluating the linear relationship is appropriate. However, people analytics often involves working with ordinal non-linear data (e.g., survey data measured on Likert-type scales, numerically coded education levels), and for these data the Spearman method for rank-ordered variables should be used. We can override the default method using the <code>method</code> argument in the cov() function call:</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb31-1" data-line-number="1"><span class="co"># Read survey response data</span></a>
<a class="sourceLine" id="cb31-2" data-line-number="2">responses &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_lifecycle_book/master/data/tables/tbl_survey_responses.csv&quot;</span>)</a>
<a class="sourceLine" id="cb31-3" data-line-number="3"></a>
<a class="sourceLine" id="cb31-4" data-line-number="4"><span class="co"># Calculate covariance between job satisfaction and work-life balance scores using Spearman&#39;s method</span></a>
<a class="sourceLine" id="cb31-5" data-line-number="5"><span class="kw">cov</span>(responses<span class="op">$</span>job_satisfaction, responses<span class="op">$</span>relationship_satisfaction, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>## [1] -2455.475</code></pre>
<p><b>Correlation</b></p>
<p>While covariance is an unstandardized measure of the direction of a relationship between variables, correlation provides a standardized measure that can be used to quantify both the direction and strength of the relationship. Correlation coefficients range from -1 to 1, where -1 indicates a perfectly negative relationship, 1 indicates a perfectly positive relationship, and 0 indicates the absence of a relationship. Pearson’s correlation coefficient <span class="math inline">\(r\)</span> is defined by:</p>
<p><span class="math display">\[ r_{x,y} = \frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sqrt{\sum_(x_{i}-\bar{x})^2\sum_(y_{i}-\bar{y})^2}} \]</span></p>
<p>In R, Pearson’s correlation coefficient <span class="math inline">\(r\)</span> can be calculated using the cor() function:</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb33-1" data-line-number="1"><span class="co"># Calculate the correlation between total working years and years at company</span></a>
<a class="sourceLine" id="cb33-2" data-line-number="2"><span class="kw">cor</span>(tenure<span class="op">$</span>total_working_years, tenure<span class="op">$</span>years_at_company, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>## [1] 0.6238746</code></pre>
<p>While we knew the relationship between total working years and years at company is positive based on the positive covariance coefficient, Pearson’s <span class="math inline">\(r\)</span> of .8 indicates that the strength of the positive association is strong (<span class="math inline">\(r\)</span> = 1 is perfectly positive).</p>
<p>As we’ve covered, Pearson’s <span class="math inline">\(r\)</span> is not appropriate for non-linear ordinal data. For this, we can leverage Spearman’s <span class="math inline">\(\rho\)</span>, which is a standardized nonparametric measure of the relationship between two rank-ordered variables. Spearman’s <span class="math inline">\(\rho\)</span> is defined as:</p>
<p><span class="math display">\[ \rho = 1- {\frac {6 \sum d_i^2}{n(n^2 - 1)}} \]</span></p>
<p>Like the cov() function, we can override the default Pearson method in the cor() function using the <code>method</code> argument:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="co"># Calculate the correlation between Belonging and Culture scores using Spearman&#39;s method</span></a>
<a class="sourceLine" id="cb35-2" data-line-number="2"><span class="kw">cor</span>(responses<span class="op">$</span>job_satisfaction, responses<span class="op">$</span>relationship_satisfaction, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>## [1] -0.01467855</code></pre>
<p>The <span class="math inline">\(\rho\)</span> coefficient indicates that the negative relationship between job satisfaction and relationship satisfaction is quite weak relative to the relationship we observed between total working years and years at company. Nevertheless, there is some degree of negative covariance between these variables.</p>
<p>Rather than looking at individual pairwise relationships, we can easily produce a correlation matrix for many variables.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="co"># Generate a correlation matrix between response columns</span></a>
<a class="sourceLine" id="cb37-2" data-line-number="2"><span class="kw">cor</span>(responses[, <span class="dv">2</span><span class="op">:</span><span class="dv">5</span>], <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>##                           environment_satisfaction job_satisfaction
## environment_satisfaction               1.000000000     -0.002992782
## job_satisfaction                      -0.002992782      1.000000000
## relationship_satisfaction              0.005353458     -0.014678554
## worklife_balance                       0.003679113      0.006420656
##                           relationship_satisfaction worklife_balance
## environment_satisfaction                0.005353458      0.003679113
## job_satisfaction                       -0.014678554      0.006420656
## relationship_satisfaction               1.000000000      0.005766111
## worklife_balance                        0.005766111      1.000000000</code></pre>
<p>Based on this correlation matrix, there is little or no relationship between responses on these four survey variables. The values down the diagonal are 1 because these represent the correlation between each variable and itself. You may also notice that the information above and below the diagonal is identical and, therefore, redundant.</p>
<p>A great library for visualizing correlation matrices is corrplot. Several arguments can be specified to surface the coefficient and various visual representations of the relationships among variables. The <code>hclust</code> argument orders variables based on similarities in the strength of relationships:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1"><span class="co"># Load library for correlation visuals</span></a>
<a class="sourceLine" id="cb39-2" data-line-number="2"><span class="kw">library</span>(corrplot)</a>
<a class="sourceLine" id="cb39-3" data-line-number="3"></a>
<a class="sourceLine" id="cb39-4" data-line-number="4"><span class="co"># Store correlation matrix to object M</span></a>
<a class="sourceLine" id="cb39-5" data-line-number="5">M &lt;-<span class="st"> </span><span class="kw">cor</span>(tenure[, <span class="dv">2</span><span class="op">:</span><span class="dv">6</span>], <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a>
<a class="sourceLine" id="cb39-6" data-line-number="6"></a>
<a class="sourceLine" id="cb39-7" data-line-number="7"><span class="co"># Visualize correlation matrix</span></a>
<a class="sourceLine" id="cb39-8" data-line-number="8"><span class="kw">corrplot</span>(M, <span class="dt">method =</span> <span class="st">&#39;shade&#39;</span>, <span class="dt">type =</span> <span class="st">&quot;upper&quot;</span>, <span class="dt">order =</span> <span class="st">&quot;hclust&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>It’s important to remember that correlation is not causation. Correlations can be spurious (variables related by chance), and drawing conclusions based on correlations – especially without drawing on theory – can be dangerous. Here are two examples of nearly perfect associations between variables that are likely related only by chance:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-23"></span>
<img src="graphics/spurious_corr_maine_divorce.png" alt="Correlation between Maine Divorce Rate and Margarine Consumption" width="480" />
<p class="caption">
Figure 5.1: Correlation between Maine Divorce Rate and Margarine Consumption
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-24"></span>
<img src="graphics/spurious_corr_mozzarella_cheese.png" alt="Correlation between Mozzarella Cheese Consumption and Civil Engineering Doctorate Conferrals" width="480" />
<p class="caption">
Figure 5.2: Correlation between Mozzarella Cheese Consumption and Civil Engineering Doctorate Conferrals
</p>
</div>
</div>
</div>
<div id="inferential-statistics" class="section level2">
<h2><span class="header-section-number">5.3</span> Inferential Statistics</h2>
<p>The objective of inferential statistics is to make inferences – with some degree of confidence – about a population based on available sample data. Several related concepts underpin this goal and will be covered here.</p>
<div id="introduction-to-probability" class="section level3">
<h3><span class="header-section-number">5.3.1</span> Introduction to Probability</h3>
</div>
<div id="central-limit-theorem" class="section level3">
<h3><span class="header-section-number">5.3.2</span> Central Limit Theorem</h3>
<p>The Central Limit Theorem (CLT) is a mainstay of statistics and probability and fundamental to understanding the mechanics of inferential analysis techniques we will cover later in this book. The CLT was initially coined by a French-born mathematician named Abraham De Moivre in the 1700s. While initially unpopular, it was later reintroduced and attracted new interest from theorists and academics (Daw &amp; Pearson, 1972).</p>
<p>The CLT states that the average of independent random variables, when increased in number, tend to follow a normal distribution. The distribution of sample means approaches a normal distribution regardless of the shape of the population distribution from which the samples are drawn. This is important because the normal distribution has properties that can be used to test the likelihood that an observed difference or relationship in a sample is also present in the population.</p>
<p>If we need to estimate the average value of a population, for example, we can lean on the CLT and normal distribution properties to obtain the range of values that are likely to include the true population average. This is how confidence intervals are defined, which enable us to make reasonable inferences about the unknown population parameters based on sample data.</p>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-25"></span>
<img src="graphics/normal_distribution.png" alt="The Empirical Rule" width="686" />
<p class="caption">
Figure 5.3: The Empirical Rule
</p>
</div>
<p><br /></p>
<p>Let’s begin with an intuitive example of CLT. Imagine that we have a reliable way to measure how fun a population is on a 100-point scale, where 100 indicates maximum fun (life of the party) and 1 indicates relative boringness. Consider that a small statistics conference is in progress at a nearby convention center, and there are 40 statisticians in attendance. In a separate room at the same convention center, there is also a group of 40 random people (non-statisticians) who are gathered to discuss some less interesting topic. Our job is to walk into one of the rooms and determine – on the basis of the “fun” factor alone – whether we have entered the statistics conference or the other, less interesting gathering of non-statisticians.</p>
<p>Let’s assume the mean fun score and standard deviation of these two groups are known; the group of statisticians have, on average, a fun score of 85 with a standard deviation of 2, while the group of non-statisticians are a bit less fun with a mean score of 65 and a standard deviation of 4. With a known population mean and standard deviation, the standard error (the standard deviation of the sample means) provides the ability to calculate the probability that the sample (the room of 40 people) belongs to the population of interest (fellow statisticians).</p>
<p>Herein lies the beauty of the CLT: roughly 68 percent of sample means will lie within one standard error of the population mean, roughly 95 percent within two standard errors of the population mean, and roughly 99 percent within three standard errors of the population mean. Therefore, any room whose members have an average fun score that is not within two standard errors of the population mean (between 81 and 89 for our statisticians) is statistically unlikely to be the group of statisticians for which we are searching. This is because in less than 5 in 100 cases could we randomly draw a ‘reasonably sized’ sample of statisticians with average funness so extremely different from the population average.</p>
<p>Because small samples lend to anamalies, we could – by chance – select a single person who happens to fall in the tails (relatively boring or extremely fun); however, as the sample size increases, it becomes more and more likely that the observed average reflects the average of the larger population. It would be virtually impossible (in less than 1 in 100 cases) to draw a random sample of statisticians from the population with average funness that is not within three standard errors of the population mean (between 79 and 91). Therefore, if we find that the room of people have an average fun score of 75, we will likely have far more fun in the other room!</p>
<p>Let’s now see the CLT in action by simulating a random uniform population distribution from which we can draw random samples. Remember, the shape of the population distribution does not matter; we could simulate an Exponential, Gamma, Poisson, Binomial, or other distribution and observe the same behavior.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1"><span class="co"># Load library for data viz</span></a>
<a class="sourceLine" id="cb40-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a></code></pre></div>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.0.2</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1"><span class="co"># Set seed for reproducible random distribution</span></a>
<a class="sourceLine" id="cb42-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb42-3" data-line-number="3"></a>
<a class="sourceLine" id="cb42-4" data-line-number="4"><span class="co"># Generate uniform population distribution with 1000 values ranging from 1 to 100</span></a>
<a class="sourceLine" id="cb42-5" data-line-number="5">rand.unif &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>, <span class="dt">min =</span> <span class="dv">1</span>, <span class="dt">max =</span> <span class="dv">100</span>)</a></code></pre></div>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1"><span class="co"># Calculate population mean</span></a>
<a class="sourceLine" id="cb43-2" data-line-number="2"><span class="kw">mean</span>(rand.unif)</a></code></pre></div>
<pre><code>## [1] 51.22007</code></pre>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1"><span class="co"># Produce histogram to visualize population distribution</span></a>
<a class="sourceLine" id="cb45-2" data-line-number="2"><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb45-3" data-line-number="3"><span class="st">  </span><span class="kw">aes</span>(rand.unif) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb45-4" data-line-number="4"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;N&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb45-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">colour =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">size =</span> <span class="fl">.1</span>, <span class="dt">fill =</span> <span class="st">&quot;#262626&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-28"></span>
<img src="_main_files/figure-html/unnamed-chunk-28-1.png" alt="Uniform Population Distribution (N = 1000)" width="672" />
<p class="caption">
Figure 5.4: Uniform Population Distribution (N = 1000)
</p>
</div>
<p>As expected, these randomly generated data are uniformly distributed. Next, we will draw 100 random samples of various sizes and plot the average of each.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1"><span class="co"># Define number of samples to draw from population distribution</span></a>
<a class="sourceLine" id="cb46-2" data-line-number="2">samples &lt;-<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb46-3" data-line-number="3"></a>
<a class="sourceLine" id="cb46-4" data-line-number="4"><span class="co"># Populate vector with sample sizes</span></a>
<a class="sourceLine" id="cb46-5" data-line-number="5">sample_n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">25</span>,<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb46-6" data-line-number="6"></a>
<a class="sourceLine" id="cb46-7" data-line-number="7"><span class="co"># Initialize empty data frame to hold sample means</span></a>
<a class="sourceLine" id="cb46-8" data-line-number="8">sample_means =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb46-9" data-line-number="9"></a>
<a class="sourceLine" id="cb46-10" data-line-number="10"><span class="co"># Set seed for reproducible random samples</span></a>
<a class="sourceLine" id="cb46-11" data-line-number="11"><span class="kw">set.seed</span>(<span class="dv">456</span>)</a>
<a class="sourceLine" id="cb46-12" data-line-number="12"></a>
<a class="sourceLine" id="cb46-13" data-line-number="13"><span class="co"># For each n, draw random samples</span></a>
<a class="sourceLine" id="cb46-14" data-line-number="14"><span class="cf">for</span> (n <span class="cf">in</span> sample_n) {</a>
<a class="sourceLine" id="cb46-15" data-line-number="15">  </a>
<a class="sourceLine" id="cb46-16" data-line-number="16">  <span class="cf">for</span> (draw <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>samples) {</a>
<a class="sourceLine" id="cb46-17" data-line-number="17">    </a>
<a class="sourceLine" id="cb46-18" data-line-number="18">      <span class="co"># Store sample means in data frame</span></a>
<a class="sourceLine" id="cb46-19" data-line-number="19">      sample_means &lt;-<span class="st"> </span><span class="kw">rbind</span>(sample_means, <span class="kw">cbind.data.frame</span>(</a>
<a class="sourceLine" id="cb46-20" data-line-number="20">                            <span class="dt">n =</span> n, </a>
<a class="sourceLine" id="cb46-21" data-line-number="21">                            <span class="dt">x_bar =</span> <span class="kw">mean</span>(<span class="kw">sample</span>(rand.unif, n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="ot">NULL</span>))))</a>
<a class="sourceLine" id="cb46-22" data-line-number="22">  }</a>
<a class="sourceLine" id="cb46-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb46-24" data-line-number="24"></a>
<a class="sourceLine" id="cb46-25" data-line-number="25"><span class="co"># Produce histograms to visualize distributions of sample means</span></a>
<a class="sourceLine" id="cb46-26" data-line-number="26">sample_means <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb46-27" data-line-number="27"><span class="st">  </span><span class="kw">aes</span>(<span class="dt">x =</span> x_bar, <span class="dt">fill =</span> n) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb46-28" data-line-number="28"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;x-bar&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;n&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb46-29" data-line-number="29"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">colour =</span> <span class="st">&quot;white&quot;</span>, <span class="dt">size =</span> <span class="fl">.1</span>, <span class="dt">fill =</span> <span class="st">&quot;#262626&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb46-30" data-line-number="30"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>n)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-29"></span>
<img src="_main_files/figure-html/unnamed-chunk-29-1.png" alt="Distribution of 100 Sample Means of Varied Size" width="672" />
<p class="caption">
Figure 5.5: Distribution of 100 Sample Means of Varied Size
</p>
</div>
<p>Per the CLT, we can see that as n increases, the sample means become more normally distributed.</p>
</div>
<div id="confidence-intervals" class="section level3">
<h3><span class="header-section-number">5.3.3</span> Confidence Intervals</h3>
<p>A Confidence Interval (CI) is a range of values that likely includes an unknown population value, as defined by:</p>
<p><span class="math display">\[ CI = \bar{X} \pm z \frac{\sigma}{\sqrt{n}} \]</span></p>
<p>A related concept that is fundamental to estimating CIs is the standard error. The standard error (SE) is the standard deviation of sample means. While the standard deviation is a measure of variability for random variables, the variability captured by the SE reflects how representative the sample is of the population. Since sample statistics will approach the actual population parameters as the size of the sample increases, the SE and sample size are inversely related; that is, the SE decreases as the sample size increases. The SE is defined by:</p>
<p><span class="math display">\[ SE = \frac{\sigma}{\sqrt{n}} \]</span></p>
<p>Next, let’s validate that our simulated distribution of sample means adheres to the properties of normally distributed data per the Empirical Rule:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1"><span class="co"># Store sample means with n = 50</span></a>
<a class="sourceLine" id="cb47-2" data-line-number="2">x_bars &lt;-<span class="st"> </span>sample_means[sample_means<span class="op">$</span>n <span class="op">==</span><span class="st"> </span><span class="dv">50</span>, <span class="st">&quot;x_bar&quot;</span>]</a>
<a class="sourceLine" id="cb47-3" data-line-number="3"></a>
<a class="sourceLine" id="cb47-4" data-line-number="4"><span class="co"># Store sample size</span></a>
<a class="sourceLine" id="cb47-5" data-line-number="5">n &lt;-<span class="st"> </span><span class="kw">length</span>(x_bars)</a>
<a class="sourceLine" id="cb47-6" data-line-number="6"></a>
<a class="sourceLine" id="cb47-7" data-line-number="7"><span class="co"># Calculate percent of sample means within +/- 2 SEs</span></a>
<a class="sourceLine" id="cb47-8" data-line-number="8"><span class="kw">length</span>(x_bars[x_bars <span class="op">&lt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars) <span class="op">&amp;</span><span class="st"> </span>x_bars <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars)]) <span class="op">/</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a></code></pre></div>
<pre><code>## [1] 97</code></pre>
<p>97% of sample means are within 2 SEs, which is roughly what we expect per the characteristics of the normal distribution.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="co"># Calculate percent of sample means within +/- 3 SEs</span></a>
<a class="sourceLine" id="cb49-2" data-line-number="2"><span class="kw">length</span>(x_bars[x_bars <span class="op">&lt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars) <span class="op">&amp;</span><span class="st"> </span>x_bars <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars)]) <span class="op">/</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a></code></pre></div>
<pre><code>## [1] 100</code></pre>
<p>All of the sample means are within 3 SEs, indicating that it would be highly unlikely – nearly impossible even – to observe a sample mean <code>from the same population</code> that falls outside this interval.</p>
<p>Now, let’s illustrate the relationship between CIs and standard errors:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="co"># Calculate 95% CI</span></a>
<a class="sourceLine" id="cb51-2" data-line-number="2">ci95_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x_bars) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(n))</a>
<a class="sourceLine" id="cb51-3" data-line-number="3">ci95_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x_bars) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(n))</a></code></pre></div>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb52-1" data-line-number="1"><span class="co"># Print lower bound for 95% CI</span></a>
<a class="sourceLine" id="cb52-2" data-line-number="2">ci95_lower_bound</a></code></pre></div>
<pre><code>## [1] 50.76792</code></pre>
<div class="sourceCode" id="cb54"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb54-1" data-line-number="1"><span class="co"># Print upper bound for 95% CI</span></a>
<a class="sourceLine" id="cb54-2" data-line-number="2">ci95_upper_bound</a></code></pre></div>
<pre><code>## [1] 52.29547</code></pre>
<p>The 95% CI corresponds to a z-score of 1.96, which is simply the number of SEs we expect to include the true population average – at least in 95 of 100 random samples taken from said population. In this example, we know that our population mean is 51.2, which is covered by our 95% CI (50.8 - 52.3). Note that our CI narrows with larger samples since our confidence that the range includes the population average increases with more data.</p>
<p>Next, let’s look at a 99% CI. We will use a z-score of 2.58 for this:</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1"><span class="co"># Calculate 99% CI</span></a>
<a class="sourceLine" id="cb56-2" data-line-number="2">ci99_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">-</span><span class="st"> </span><span class="fl">2.58</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x_bars) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(n))</a>
<a class="sourceLine" id="cb56-3" data-line-number="3">ci99_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">+</span><span class="st"> </span><span class="fl">2.58</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x_bars) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(n))</a></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1"><span class="co"># Print lower bound for 99% CI</span></a>
<a class="sourceLine" id="cb57-2" data-line-number="2">ci99_lower_bound</a></code></pre></div>
<pre><code>## [1] 50.52632</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1"><span class="co"># Print upper bound for 99% CI</span></a>
<a class="sourceLine" id="cb59-2" data-line-number="2">ci99_upper_bound</a></code></pre></div>
<pre><code>## [1] 52.53707</code></pre>
<p>Like the 95% CI, this slightly wider 99% CI (50.5 - 52.5) also includes the true population mean of 51.2.</p>
<p><b>Hypothesis Testing</b></p>
<p>Hypothesis testing is how we leverage CIs to test whether a significant difference or relationship exists in the data. Sir Ronald Fisher invented what is known as the null hypothesis, which states that there is no relationship/difference; disprove me if you can! The null hypothesis is defined by:</p>
<p><span class="math display">\[ H_0: \mu_A = \mu_B \]</span></p>
<p>The objective of hypothesis testing is to determine if there is sufficient evidence to reject the null hypothesis in favor of an alternative hypothesis. The null hypothesis always states that there is ‘nothing’ of significance. For example, if we want to test whether an intervention has an effect on an outcome in a population, the null hypothesis states that there is no effect. If we want to test whether there is a difference in average scores between two groups in a population, the null hypothesis states that there is no difference.</p>
<p>An alternative hypothesis may simply state that there is a difference or relationship in the population, or it may specify the expected direction (e.g., Population A has a significantly ‘larger’ or ‘smaller’ average value than Population B; Variable A is ‘positively’ or ‘negatively’ related to Variable B):</p>
<p><span class="math display">\[ H_A: \mu_A \neq \mu_B \]</span></p>
<p><span class="math display">\[ H_A: \mu_A &lt; \mu_B \]</span></p>
<p><span class="math display">\[ H_A: \mu_A &gt; \mu_B \]</span></p>
<p><b>Alpha</b></p>
<p>The alpha level of a hypothesis test, denoted by <span class="math inline">\(\alpha\)</span>, represents the probability of obtaining observed results due to chance if the null hypothesis is true. In other words, <span class="math inline">\(\alpha\)</span> is the probability of rejecting the null hypothesis (and therefore claiming that there is a significant difference or relationship) when in fact we should have failed to reject it because there is insufficient evidence to support the alternative hypothesis.</p>
<p><span class="math inline">\(\alpha\)</span> is often set at .05 but is sometimes set at a more rigorous .01. An <span class="math inline">\(\alpha\)</span> of .05 corresponds to a 95% CI (1 - .05), and .01 to a 99% CI (1 - .01). With non-directional alternative hypotheses, we must divide <span class="math inline">\(\alpha\)</span> by 2 (i.e., we could observe a significant result in either tail of the distribution), while one-tailed tests position the rejection region entirely within one tail based on what is being hypothesized.</p>
<p>At the .05 level, we would conclude that a finding is statistically significant if the chance of observing a value at least as extreme as the one observed is less than 1 in 20 if the null hypothesis is true. Note that we observed this behavior with our simulated distribution of sample means. In our example, we found that in 97 of 100 cases, the sample mean was within our 95% CI. While we can draw a more extreme value by chance with repeated attempts, we should only expect a mean outside the 95% CI less than 1 in every 20 times. Moreover, we should generally only expect a mean outside the 99% CI less than 1 in every 100 times.</p>
<p><b>Beta</b></p>
<p>Another key value is Beta, denoted by <span class="math inline">\(\beta\)</span>, which relates to the power of the analysis. Simply put, power reflects our ability to find a difference or relationship if there is one. Power is calculated by 1 - <span class="math inline">\(\beta\)</span>.</p>
<p><b>Type I &amp; II Errors</b></p>
<p>A Type I Error is a false positive, wherein we conclude that there is a significant difference or relationship when there is not. A Type II Error is a false negative, wherein we fail to capture a significant finding. <span class="math inline">\(\alpha\)</span> represents our chance of making a Type I Error, while <span class="math inline">\(\beta\)</span> represents our chance of making a Type II Error. I once had a professor explain that committing a Type I error is a shame, while committing a Type II error is a pity, and I’ve found this to be a helpful way to remember what each type of error represents.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-38"></span>
<img src="graphics/hypothesis_testing_errors.png" alt="Type I and II Errors" width="349" />
<p class="caption">
Figure 5.6: Type I and II Errors
</p>
</div>
<p><b>P-Values</b></p>
<p>In statistical tests, the p-value is referenced to determine whether the null hypothesis can be rejected. The p-value represents the probability of obtaining a result at least as extreme as the one observed if the null hypothesis is true. As a general rule, if p &lt; .05, we can confidently reject the null hypothesis and conclude that the observed difference or relationship was unlikely a chance observation.</p>
<p>While statistical significance helps us understand the probability of observing results by chance when there is actually no difference or effect in the population, it does not tell us anything about the size of the difference or effect. Analysis should never be reduced to inspecting p-values; in fact, p-values have been the subject of much controversy among researchers and practitioners in recent years. This book will cover how to interpret results of statistical tests to surface the story and determine if there is anything ‘practically’ significant among statistically significant findings.</p>
</div>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">5.4</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Parameters are descriptions or characteristics of a sample, while statistics are descriptions or characteristics of a population.
<br />A. True
<br />B. False</p></li>
<li><p>Which of the following is an example of a null hypothesis, where “µ” reflects the mean of a population?
<br />A. µA = µB
<br />B. µA ≠ µB
<br />C. µA &lt; µB
<br />D. µA &gt; µB
<br />E. None of the above; all are examples of alternative hypotheses.</p></li>
<li><p>Which of the following describes a Type I Error?
<br />A. Failing to reject the null hypothesis when it is false
<br />B. Rejecting the null hypothesis when it is true
<br />C. Reporting something as significant when nothing of significance is present (a shame)
<br />D. Failing to detect something of significance (a pity)
<br />E. Both B and C
<br />F. Both A and D</p></li>
<li><p>100 randomly selected employees in the Marketing department of an organization participated in a survey on career pathing for marketing professionals. What is the sample and what is the population sampled in this case?
<br />A. Sample: 100 employees who completed the survey, Population: All employees in the organization
<br />B. Sample: 100 employees who completed the survey, Population: Marketing employees
<br />C. Sample: All Marketing professionals, Population: All employees in the organization
<br />D. Sample: All employees in the organization, Population: Employees across all companies globally</p></li>
<li><p>The primary purpose of inferential statistics is to make inferences about a population based on sample data. Inferential statistics allows these inferences to be made with defined levels of confidence that what is observed in a sample is also characteristic of the larger population.
<br />A. True
<br />B. False</p></li>
<li><p>The median tends to be a better measure of central tendency since the mean is more sensitive to extreme values (outliers)?
<br />A. True
<br />B. False</p></li>
<li><p>The standard deviation represents the ‘average’ amount by which individual values for a variable deviate (or vary) from the mean. A large standard deviation indicates there is considerable spread in the data, whereas a small standard deviation indicates the mean is fairly representative of the data.
<br />A. True
<br />B. False</p></li>
<li><p>A positively skewed distribution has its largest allocation to the left and a negative distribution to the right.
<br />A. True
<br />B. False</p></li>
<li><p>The mean alone is sufficient for understanding the distribution of a set of data.
<br />A. True
<br />B. False</p></li>
<li><p>Quartiles are useful for understanding the distribution of data.
<br />A. True
<br />B. False</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="research-methods.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="measurement-sampling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/r/03_statistical_fundamentals.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
