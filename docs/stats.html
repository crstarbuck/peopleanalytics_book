<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Statistical Fundamentals | A Guide through the People Analytics Lifecycle: With Applications in R</title>
  <meta name="description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Statistical Fundamentals | A Guide through the People Analytics Lifecycle: With Applications in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="github-repo" content="crstarbuck/peopleanalytics-lifecycle-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Statistical Fundamentals | A Guide through the People Analytics Lifecycle: With Applications in R" />
  
  <meta name="twitter:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  

<meta name="author" content="Craig Starbuck" />


<meta name="date" content="2022-02-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="measurement-sampling.html"/>
<link rel="next" href="data-preparation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Guide through the People Analytics Lifecycle: With Applications in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i><b>1</b> Foreword</a></li>
<li class="chapter" data-level="2" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>3</b> Getting Started</a><ul>
<li class="chapter" data-level="3.1" data-path="getting-started.html"><a href="getting-started.html#guiding-principles"><i class="fa fa-check"></i><b>3.1</b> Guiding Principles</a></li>
<li class="chapter" data-level="3.2" data-path="getting-started.html"><a href="getting-started.html#tools"><i class="fa fa-check"></i><b>3.2</b> Tools</a></li>
<li class="chapter" data-level="3.3" data-path="getting-started.html"><a href="getting-started.html#d-framework"><i class="fa fa-check"></i><b>3.3</b> 4D Framework</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="research-methods.html"><a href="research-methods.html"><i class="fa fa-check"></i><b>4</b> Research Methods</a></li>
<li class="chapter" data-level="5" data-path="measurement-sampling.html"><a href="measurement-sampling.html"><i class="fa fa-check"></i><b>5</b> Measurement &amp; Sampling</a><ul>
<li class="chapter" data-level="5.1" data-path="measurement-sampling.html"><a href="measurement-sampling.html#populations-samples"><i class="fa fa-check"></i><b>5.1</b> Populations &amp; Samples</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="stats.html"><a href="stats.html"><i class="fa fa-check"></i><b>6</b> Statistical Fundamentals</a><ul>
<li class="chapter" data-level="6.1" data-path="stats.html"><a href="stats.html#univariate-analysis"><i class="fa fa-check"></i><b>6.1</b> Univariate Analysis</a><ul>
<li class="chapter" data-level="6.1.1" data-path="stats.html"><a href="stats.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>6.1.1</b> Measures of Central Tendency</a></li>
<li class="chapter" data-level="6.1.2" data-path="stats.html"><a href="stats.html#measures-of-spread"><i class="fa fa-check"></i><b>6.1.2</b> Measures of Spread</a></li>
<li class="chapter" data-level="6.1.3" data-path="stats.html"><a href="stats.html#bivariate-analysis"><i class="fa fa-check"></i><b>6.1.3</b> Bivariate Analysis</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="stats.html"><a href="stats.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a><ul>
<li class="chapter" data-level="6.2.1" data-path="stats.html"><a href="stats.html#introduction-to-probability"><i class="fa fa-check"></i><b>6.2.1</b> Introduction to Probability</a></li>
<li class="chapter" data-level="6.2.2" data-path="stats.html"><a href="stats.html#central-limit-theorem"><i class="fa fa-check"></i><b>6.2.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="6.2.3" data-path="stats.html"><a href="stats.html#confidence-intervals"><i class="fa fa-check"></i><b>6.2.3</b> Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="stats.html"><a href="stats.html#parametric-vs.nonparametric-tests"><i class="fa fa-check"></i><b>6.3</b> Parametric vs. Nonparametric Tests</a></li>
<li class="chapter" data-level="6.4" data-path="stats.html"><a href="stats.html#exercises"><i class="fa fa-check"></i><b>6.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-preparation.html"><a href="data-preparation.html"><i class="fa fa-check"></i><b>7</b> Data Preparation</a><ul>
<li class="chapter" data-level="7.1" data-path="data-preparation.html"><a href="data-preparation.html#data-wrangling"><i class="fa fa-check"></i><b>7.1</b> Data Wrangling</a></li>
<li class="chapter" data-level="7.2" data-path="data-preparation.html"><a href="data-preparation.html#feature-engineering"><i class="fa fa-check"></i><b>7.2</b> Feature Engineering</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="aod.html"><a href="aod.html"><i class="fa fa-check"></i><b>8</b> Analysis of Differences</a><ul>
<li class="chapter" data-level="8.1" data-path="aod.html"><a href="aod.html#comparing-2-distributions"><i class="fa fa-check"></i><b>8.1</b> Comparing 2 Distributions</a></li>
<li class="chapter" data-level="8.2" data-path="aod.html"><a href="aod.html#comparing-3-distributions"><i class="fa fa-check"></i><b>8.2</b> Comparing 3+ Distributions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="inferential-models.html"><a href="inferential-models.html"><i class="fa fa-check"></i><b>9</b> Inferential Models</a><ul>
<li class="chapter" data-level="9.1" data-path="inferential-models.html"><a href="inferential-models.html#regression"><i class="fa fa-check"></i><b>9.1</b> Regression</a></li>
<li class="chapter" data-level="9.2" data-path="inferential-models.html"><a href="inferential-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>9.2</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="9.2.1" data-path="inferential-models.html"><a href="inferential-models.html#parameter-estimation"><i class="fa fa-check"></i><b>9.2.1</b> Parameter Estimation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="inferential-models.html"><a href="inferential-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>9.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="9.3.1" data-path="inferential-models.html"><a href="inferential-models.html#moderation"><i class="fa fa-check"></i><b>9.3.1</b> Moderation</a></li>
<li class="chapter" data-level="9.3.2" data-path="inferential-models.html"><a href="inferential-models.html#mediation"><i class="fa fa-check"></i><b>9.3.2</b> Mediation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="inferential-models.html"><a href="inferential-models.html#polynomial-regression"><i class="fa fa-check"></i><b>9.4</b> Polynomial Regression</a></li>
<li class="chapter" data-level="9.5" data-path="inferential-models.html"><a href="inferential-models.html#logistic-regression"><i class="fa fa-check"></i><b>9.5</b> Logistic Regression</a><ul>
<li class="chapter" data-level="9.5.1" data-path="inferential-models.html"><a href="inferential-models.html#binomial"><i class="fa fa-check"></i><b>9.5.1</b> Binomial</a></li>
<li class="chapter" data-level="9.5.2" data-path="inferential-models.html"><a href="inferential-models.html#multinomial"><i class="fa fa-check"></i><b>9.5.2</b> Multinomial</a></li>
<li class="chapter" data-level="9.5.3" data-path="inferential-models.html"><a href="inferential-models.html#ordinal"><i class="fa fa-check"></i><b>9.5.3</b> Ordinal</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="inferential-models.html"><a href="inferential-models.html#hierarchical-models"><i class="fa fa-check"></i><b>9.6</b> Hierarchical Models</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="predictive-models.html"><a href="predictive-models.html"><i class="fa fa-check"></i><b>10</b> Predictive Models</a><ul>
<li class="chapter" data-level="10.1" data-path="predictive-models.html"><a href="predictive-models.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>10.1</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="10.2" data-path="predictive-models.html"><a href="predictive-models.html#cross-validation"><i class="fa fa-check"></i><b>10.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="10.3" data-path="predictive-models.html"><a href="predictive-models.html#balancing-classes"><i class="fa fa-check"></i><b>10.3</b> Balancing Classes</a></li>
<li class="chapter" data-level="10.4" data-path="predictive-models.html"><a href="predictive-models.html#model-performance"><i class="fa fa-check"></i><b>10.4</b> Model Performance</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="automated-machine-learning-automl.html"><a href="automated-machine-learning-automl.html"><i class="fa fa-check"></i><b>11</b> Automated Machine Learning (AutoML)</a></li>
<li class="chapter" data-level="12" data-path="unsupervised-learning-models.html"><a href="unsupervised-learning-models.html"><i class="fa fa-check"></i><b>12</b> Unsupervised Learning Models</a><ul>
<li class="chapter" data-level="12.1" data-path="unsupervised-learning-models.html"><a href="unsupervised-learning-models.html#factor-analysis"><i class="fa fa-check"></i><b>12.1</b> Factor Analysis</a></li>
<li class="chapter" data-level="12.2" data-path="unsupervised-learning-models.html"><a href="unsupervised-learning-models.html#clustering"><i class="fa fa-check"></i><b>12.2</b> Clustering</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>13</b> Data Visualization</a></li>
<li class="chapter" data-level="14" data-path="data-storytelling.html"><a href="data-storytelling.html"><i class="fa fa-check"></i><b>14</b> Data Storytelling</a></li>
<li class="chapter" data-level="15" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i><b>15</b> Bibliography</a></li>
<li class="chapter" data-level="16" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>16</b> Appendix</a><ul>
<li class="chapter" data-level="16.1" data-path="appendix.html"><a href="appendix.html#exercise-solutions"><i class="fa fa-check"></i><b>16.1</b> Exercise Solutions</a></li>
<li class="chapter" data-level="16.2" data-path="appendix.html"><a href="appendix.html#d-framework-1"><i class="fa fa-check"></i><b>16.2</b> 4D Framework</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Guide through the People Analytics Lifecycle</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="stats" class="section level1">
<h1><span class="header-section-number">6</span> Statistical Fundamentals</h1>
<p>This chapter reviews essential univariate, bivariate, and inferential analysis concepts that underpin the more complex multivariate approaches in subsequent chapters of this book.</p>
<div id="univariate-analysis" class="section level2">
<h2><span class="header-section-number">6.1</span> Univariate Analysis</h2>
<p><strong>Descriptive statistics</strong> are rudimentary analysis techniques that help describe and summarize data in a meaningful way. Descriptive statistics do not allow us to draw any conclusions beyond the available data but are helpful in interpreting the data at hand.</p>
<p>There are two categories of descriptive statistics: (a) <strong>measures of central tendency</strong> describe the central position in a set of data; and (b) <strong>measures of spread</strong> describe how dispersed the data are.</p>
<div id="measures-of-central-tendency" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Measures of Central Tendency</h3>
<p><strong>Mean</strong></p>
<p>Perhaps the most intuitive measure of central tendency is the <strong>mean</strong>, which is often referred to as the average. The mean of a sample is denoted by <span class="math inline">\(\bar{x}\)</span> and is defined by:</p>
<p><span class="math display">\[ \bar{x} = \frac{\sum x_{i}}{n} \]</span></p>
<p>The population mean is denoted by <span class="math inline">\(\mu\)</span> and is defined by:</p>
<p><span class="math display">\[ \mu = \frac{\sum x_{i}}{N} \]</span></p>
<p>The mean of a set of numeric values can be calculated using the mean() function in R:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="co"># Fill vector x with integers</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb4-3" data-line-number="3"></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="co"># Calculate average of vector x</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="kw">mean</span>(x)</a></code></pre></div>
<pre><code>## [1] 6.9</code></pre>
<p><strong>Median</strong></p>
<p>The <strong>median</strong> represents the midpoint in a sorted vector of numbers. For vectors with an even number of values, the median is the average of the middle two numbers; it is simply the middle number for vectors with an odd number of values. When the distribution of data is skewed, or there is an extreme value like we observe in vector x, the median is a better measure of central tendency.</p>
<p>The median() function in R can be used to handle the sorting and midpoint selection:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="co"># Calculate median of vector x</span></a>
<a class="sourceLine" id="cb6-2" data-line-number="2"><span class="kw">median</span>(x)</a></code></pre></div>
<pre><code>## [1] 2</code></pre>
<p>In this example, the median is only 2 while the mean is 6.9 (which is not really representative of any of the values in vector <span class="math inline">\(x\)</span>). Large deltas between mean and median values are evidence of outliers.</p>
<p><strong>Mode</strong></p>
<p>The <strong>mode</strong> is the most frequent number in a set of values.</p>
<p>While mean() and median() are standard functions in R, mode() returns the internal storage mode of the object rather than the statistical mode of the data. We can easily create a function to return the statistical mode(s):</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb8-1" data-line-number="1"><span class="co"># Create function to calculate statistical mode(s)</span></a>
<a class="sourceLine" id="cb8-2" data-line-number="2">stat.mode &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb8-3" data-line-number="3">  ux &lt;-<span class="st"> </span><span class="kw">unique</span>(x)</a>
<a class="sourceLine" id="cb8-4" data-line-number="4">  tab &lt;-<span class="st"> </span><span class="kw">tabulate</span>(<span class="kw">match</span>(x, ux))</a>
<a class="sourceLine" id="cb8-5" data-line-number="5">  ux[tab <span class="op">==</span><span class="st"> </span><span class="kw">max</span>(tab)]</a>
<a class="sourceLine" id="cb8-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb8-7" data-line-number="7"></a>
<a class="sourceLine" id="cb8-8" data-line-number="8"><span class="co"># Return mode(s) of vector x</span></a>
<a class="sourceLine" id="cb8-9" data-line-number="9"><span class="kw">stat.mode</span>(x)</a></code></pre></div>
<pre><code>## [1] 1 2</code></pre>
<p>In this case, we have a bimodal distribution since both 1 and 2 occur most frequently.</p>
<p><strong>Range</strong></p>
<p>The <strong>range</strong> is the difference between the maximum and minimum values in a set of numbers.</p>
<p>The range() function in R returns the minimum and maximum numbers:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="co"># Return lowest and highest values of vector x</span></a>
<a class="sourceLine" id="cb10-2" data-line-number="2"><span class="kw">range</span>(x)</a></code></pre></div>
<pre><code>## [1]  1 50</code></pre>
<p>We can leverage the max() and min() functions to calculate the difference between these values:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="co"># Calculate range of vector x</span></a>
<a class="sourceLine" id="cb12-2" data-line-number="2"><span class="kw">max</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 49</code></pre>
<p>In people analytics, there are many conventional descriptive metrics – largely counts, percentages, and ratios cut by time series (day, month, quarter, year) and categorical dimensions (department, job, location, tenure band). Here is a sample of common measures:</p>
<ul>
<li>Time to Fill: average days between job requisition posting and offer acceptance</li>
<li>Offer Acceptance Rate: percent of offers extended to candidates that are accepted</li>
<li>Pass-Through Rate: percent of candidates in a particular stage of the recruiting process who passed through to the next stage</li>
<li>Progress to Goal: percent of approved positions that have been filled</li>
<li>cNPS/eNPS: candidate and employee NPS (-100 to 100)</li>
<li>Headcount: counts and percent of workforce across worker types (employee, intern, contingent)</li>
<li>Diversity: counts and percent of workforce across gender, ethnicity, and generational cohorts</li>
<li>Positions: count and percent of open, committed, and filled seats</li>
<li>Hires: counts and rates</li>
<li>Career Moves: counts and rates</li>
<li>Turnover: counts and rates (usually terms / average headcount over the period)</li>
<li>Workforce Growth: net changes over time, accounting for hires, internal transfers, and exits</li>
<li>Span of Control: ratio of people leaders to individual contributors</li>
<li>Layers/Tiers: average and median number of layers removed from CEO</li>
<li>Engagement: average score or top-box favorability score</li>
</ul>
</div>
<div id="measures-of-spread" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Measures of Spread</h3>
<p><strong>Variance</strong></p>
<p><strong>Variance</strong> is a measure of variability in the data. Variance is calculated using the average of squared differences – or deviations – from the mean.</p>
<p>Variance of a population is defined by:</p>
<p><span class="math display">\[ \sigma^{2} = \frac{\sum (X_{i}-\mu)^{2}}{N} \]</span></p>
<p>Variance of a sample is defined by:</p>
<p><span class="math display">\[ s^{2} = \frac{\sum (x_{i}-\bar{x})^{2}}{n-1} \]</span></p>
<p>It is important to note that since differences are squared, the variance is always non-negative. In addition, we cannot compare these squared differences to the arithmetic mean since the units are different. For example, if we calculate the variance of annual compensation measured in USD, variance should be expressed as USD squared while the mean exists in the original USD unit of measurement.</p>
<p>In R, the sample variance can be calculated using the var() function:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="co"># Load library for data wrangling</span></a>
<a class="sourceLine" id="cb14-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3"></a>
<a class="sourceLine" id="cb14-4" data-line-number="4"><span class="co"># Read employee demographics data</span></a>
<a class="sourceLine" id="cb14-5" data-line-number="5">demographics &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_lifecycle_book/master/data/files/employee_demographics.csv&quot;</span>)</a>
<a class="sourceLine" id="cb14-6" data-line-number="6"></a>
<a class="sourceLine" id="cb14-7" data-line-number="7"><span class="co"># Calculate sample variance for annual compensation</span></a>
<a class="sourceLine" id="cb14-8" data-line-number="8"><span class="kw">var</span>(demographics<span class="op">$</span>annual_comp)</a></code></pre></div>
<pre><code>## [1] 1876688425</code></pre>
<p>Sample statistics are the default in R. Since the population variance differs from the sample variance by a factor of <span class="math inline">\(s^2 * (\frac{n - 1}{n})\)</span>, it is simple to convert output from var() to the population variance:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="co"># Store number of observations</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2">n =<span class="st"> </span><span class="kw">length</span>(demographics<span class="op">$</span>annual_comp)</a>
<a class="sourceLine" id="cb16-3" data-line-number="3"></a>
<a class="sourceLine" id="cb16-4" data-line-number="4"><span class="co"># Calculate population variance for annual compensation</span></a>
<a class="sourceLine" id="cb16-5" data-line-number="5"><span class="kw">var</span>(demographics<span class="op">$</span>annual_comp) <span class="op">*</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>n</a></code></pre></div>
<pre><code>## [1] 1876303464</code></pre>
<p><strong>Standard Deviation</strong></p>
<p>The <strong>standard deviation</strong> is simply the square root of the variance, as defined by:</p>
<p><span class="math display">\[ s = \sqrt{\frac{\sum (x_{i} - \bar{x})^{2}}{n - 1}} \]</span></p>
<p>Since a squared value can be converted back to its original units by taking its square root, the standard deviation expresses variability around the mean in the variable’s original units.</p>
<p>In R, the sample standard deviation can be calculated using the sd() function:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="co"># Calculate sample standard deviation for annual compensation</span></a>
<a class="sourceLine" id="cb18-2" data-line-number="2"><span class="kw">sd</span>(demographics<span class="op">$</span>annual_comp)</a></code></pre></div>
<pre><code>## [1] 43320.76</code></pre>
<p>Since the population standard deviation differs from the sample standard deviation by a factor of <span class="math inline">\(s * \sqrt (\frac{n - 1}{n})\)</span>, it is simple to convert output from sd() to the population standard deviation:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="co"># Calculate population standard deviation for annual compensation</span></a>
<a class="sourceLine" id="cb20-2" data-line-number="2"><span class="kw">sd</span>(demographics<span class="op">$</span>annual_comp) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>((n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>n)</a></code></pre></div>
<pre><code>## [1] 43316.32</code></pre>
<p><strong>Quartiles</strong></p>
<p><strong>Quartiles</strong> are a staple of exploratory data analysis (EDA). A quartile is a type of quantile that partitions data into four equally sized parts after ordering the data. Note that each partition is equally sized with respect to the number of data points – not the range of values in each. Quartiles are also related to <strong>percentiles</strong>. For example, Q1 is the 25th percentile – the value at or below which 25% of values lie. Percentiles are likely more familiar than quartiles, as percentiles show up in the height and weight measurements of babies, performance on standardized tests like the SAT and GRE, among other things.</p>
<p>The <strong>Interquartile Range (IQR)</strong> represents the difference between Q3 and Q1 cut point values (the middle two quartiles). The IQR is sometimes used to detect extreme values in a distribution; values less than <span class="math inline">\(Q1 - 1.5 * IQR\)</span> or greater than <span class="math inline">\(Q3 + 1.5 * IQR\)</span> are generally considered outliers.</p>
<p>In R, the quantile() function returns the values that bookend each quartile:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="co"># Return quartiles for annual compensation</span></a>
<a class="sourceLine" id="cb22-2" data-line-number="2"><span class="kw">quantile</span>(demographics<span class="op">$</span>annual_comp)</a></code></pre></div>
<pre><code>##       0%      25%      50%      75%     100% 
##  50016.0  87946.0 125522.0 163262.5 199968.0</code></pre>
<p>Based on this output, we know that 25% of people in our data earn annual compensation of 87,946 USD or less, 125,522 USD is the median annual compensation, and 75% of people earn annual compensation of 163,263 USD or less.</p>
<p>We can also return a specific percentile value using the <code>probs</code> argument in the quantile() function. For example, if we want to know the 80th percentile annual compensation value, we can execute the following:</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="co"># Return 80th percentile annual compensation value</span></a>
<a class="sourceLine" id="cb24-2" data-line-number="2"><span class="kw">quantile</span>(demographics<span class="op">$</span>annual_comp, <span class="dt">probs =</span> <span class="fl">.8</span>)</a></code></pre></div>
<pre><code>##      80% 
## 170196.4</code></pre>
<p>In addition, the summary() function returns several common descriptive statistics for an object:</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="co"># Return common descriptives</span></a>
<a class="sourceLine" id="cb26-2" data-line-number="2"><span class="kw">summary</span>(demographics<span class="op">$</span>annual_comp)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   50016   87946  125522  125369  163262  199968</code></pre>
<p><strong>Skewness</strong></p>
<p><strong>Skewness</strong> is a measure of the horizontal distance between the mode and mean – a representation of symmetric distortion. In most practical settings, data are not normally distributed. That is, the data are skewed either positively (right-tailed distribution) or negatively (left-tailed distribution). The coefficient of skewness is one of many ways in which we can asertain the degree of skew in the data. The skewness of sample data is defined as:</p>
<p><span class="math display">\[ Sk = \frac{1}{n} \frac{\sum(x_i-\bar{x})^3}{s^3} \]</span></p>
<p>A positive skewness coefficient indicates positive skew, while a negative coefficient indicates negative skew. The order of descriptive statistics can also be leveraged to ascertain the direction of skew in the data:</p>
<ul>
<li>Positive skewness: mode &lt; median &lt; mean</li>
<li>Negative skewness: mode &gt; median &gt; mean</li>
<li>Symmetrical distribution: mode = median = mean</li>
</ul>
<p>Figure <a href="stats.html#fig:skewness">6.1</a> illustrates the placement of these descriptive statistics in each of the three types of distributions:</p>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:skewness"></span>
<img src="graphics/skewness.png" alt="Skewness" width="490" />
<p class="caption">
Figure 6.1: Skewness
</p>
</div>
<p><br /></p>
<p>The magnitude of skewness can be determined by measuring the distance between the mode and mean relative to the variable’s scale. Alternatively, we can simply evaluate this using the coefficient of skewness:</p>
<ul>
<li>If skewness is between -0.5 - 0.5, the data are fairly symmetrical.</li>
<li>If skewness is between -0.5 and -1 or 0.5 and 1, the data are moderately skewed.</li>
<li>If skewness is &lt; -1 or &gt; 1, the data are highly skewed.</li>
</ul>
<p>Since there is not a base R function for skewness, we can leverage the moments library to calculate skewness:</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="co"># Load library</span></a>
<a class="sourceLine" id="cb28-2" data-line-number="2"><span class="kw">library</span>(moments)</a>
<a class="sourceLine" id="cb28-3" data-line-number="3"></a>
<a class="sourceLine" id="cb28-4" data-line-number="4"><span class="co"># Calculate skewness for org tenure, rounded to three significant figures</span></a>
<a class="sourceLine" id="cb28-5" data-line-number="5"><span class="kw">round</span>(<span class="kw">skewness</span>(demographics<span class="op">$</span>org_tenure), <span class="dv">3</span>)</a></code></pre></div>
<pre><code>## [1] 0.562</code></pre>
<p><strong>Statistical Moments</strong>, after which this library was named, play an important role in specifying the appropriate probability distribution for a set of data. Moments are a set of statistical parameters used to describe the characteristics of a distribution. Skewness is the third statistical moment in the set; hence the sum of cubed differences and cubic polynomial in the denominator of the formula above. The complete set of moments comprises: (1) expected value or mean, (2) variance and standard deviation, (3) skewness, and (4) kurtosis.</p>
<p>We can verify that the skewness() function from the moments library returns the expected value (per the aforementioned formula) by validating against a manual calculation:</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="co"># Store components of skewness calculation</span></a>
<a class="sourceLine" id="cb30-2" data-line-number="2">n =<span class="st"> </span><span class="kw">length</span>(demographics<span class="op">$</span>org_tenure)</a>
<a class="sourceLine" id="cb30-3" data-line-number="3">x =<span class="st"> </span>demographics<span class="op">$</span>org_tenure</a>
<a class="sourceLine" id="cb30-4" data-line-number="4">x_bar =<span class="st"> </span><span class="kw">mean</span>(demographics<span class="op">$</span>org_tenure)</a>
<a class="sourceLine" id="cb30-5" data-line-number="5">s =<span class="st"> </span><span class="kw">sd</span>(demographics<span class="op">$</span>org_tenure)</a>
<a class="sourceLine" id="cb30-6" data-line-number="6"></a>
<a class="sourceLine" id="cb30-7" data-line-number="7"><span class="co"># Calculate skewness manually, rounded to three significant figures</span></a>
<a class="sourceLine" id="cb30-8" data-line-number="8"><span class="kw">round</span>(<span class="dv">1</span><span class="op">/</span>n <span class="op">*</span><span class="st"> </span>(<span class="kw">sum</span>((x <span class="op">-</span><span class="st"> </span>x_bar)<span class="op">^</span><span class="dv">3</span>) <span class="op">/</span><span class="st"> </span>s<span class="op">^</span><span class="dv">3</span>), <span class="dv">3</span>)</a></code></pre></div>
<pre><code>## [1] 0.562</code></pre>
<p>A skewness coefficient of .562 indicates that organization tenure is moderately and positively skewed. We can visualize the data to confirm the expected right-tailed distribution:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1"><span class="co"># Load library for data viz</span></a>
<a class="sourceLine" id="cb32-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a></code></pre></div>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.0.2</code></pre>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="co"># Produce histogram to visualize sample distribution</span></a>
<a class="sourceLine" id="cb34-2" data-line-number="2"><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb34-3" data-line-number="3"><span class="st">  </span><span class="kw">aes</span>(demographics<span class="op">$</span>org_tenure) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb34-4" data-line-number="4"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Organization Tenure&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb34-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb34-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb34-7" data-line-number="7"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:org-tenure-distribution"></span>
<img src="_main_files/figure-html/org-tenure-distribution-1.png" alt="Organization Tenure Distribution" width="672" />
<p class="caption">
Figure 6.2: Organization Tenure Distribution
</p>
</div>
<p><strong>Kurtosis</strong></p>
<p>While skewness provides information on the symmetry of a distribution, <strong>kurtosis</strong> provides information on the heaviness of a distribution’s tails (“tailedness”). Kurtosis is the fourth statistical moment, defined by:</p>
<p><span class="math display">\[ K = \frac{1}{n} \frac{\sum(x_i-\bar{x})^4}{s^4} \]</span></p>
<p>Note that the quartic functions characteristic of the fourth statistical moment are the only differences from the skewness formula we reviewed in the prior section (which featured cubic functions).</p>
<p>The terms <strong>leptokurtic</strong> and <strong>platykurtic</strong> are often used to describe distributions with light and heavy tails, respectively. “Platy-” in platykurtic is the same root as “platypus”, and I’ve found it helpful to recall the characteristics of the flat platypus when characterizing frequency distributions as platkurtic (wide and flat) vs. its antithesis, leptokurtic (tall and skinny). The normal (or Gaussian) distribution is referred to as a <strong>mesokurtic</strong> distribution in the context of kurtosis.</p>
<p>Figure <a href="stats.html#fig:kurtosis">6.3</a> illustrates the three kurtosis categorizations:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kurtosis"></span>
<img src="graphics/kurtosis.jpg" alt="Kurtosis" width="709" />
<p class="caption">
Figure 6.3: Kurtosis
</p>
</div>
<p>Kurtosis is measured relative to a normal distribution. Normal distributions have a kurtosis coefficient of 3. Therefore, the kurtosis coefficient is greater than 3 for leptokurtic distributions and less than 3 for platykurtic distributions.</p>
<p>The moments library can also be used to calculate kurtosis in R:</p>
<div class="sourceCode" id="cb35"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb35-1" data-line-number="1"><span class="co"># Calculate kurtosis for org tenure, rounded to two significant figures</span></a>
<a class="sourceLine" id="cb35-2" data-line-number="2"><span class="kw">round</span>(<span class="kw">kurtosis</span>(demographics<span class="op">$</span>org_tenure), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 2.26</code></pre>
<p>We can verify that the kurtosis() function returns the expected value (per the aforementioned formula) by validating against a manual calculation:</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1"><span class="co"># Calculate kurtosis manually, rounded to two significant figures</span></a>
<a class="sourceLine" id="cb37-2" data-line-number="2"><span class="kw">round</span>(<span class="dv">1</span><span class="op">/</span>n <span class="op">*</span><span class="st"> </span>(<span class="kw">sum</span>((x <span class="op">-</span><span class="st"> </span>x_bar)<span class="op">^</span><span class="dv">4</span>) <span class="op">/</span><span class="st"> </span>s<span class="op">^</span><span class="dv">4</span>), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 2.26</code></pre>
<p>As we saw in Figure <a href="stats.html#fig:org-tenure-distribution">6.2</a>, there is a long right tail about the organization tenure distribution. However, due to the moderate skew, there is no left tail. Therefore, our kurtosis coefficient is &lt; 3 since the presence of the right tail is offset by the lack of a left tail. This is why it is important not to characterize a distribution based on a single isolated metric; we need the complete set of statistical moments to fully understand the distribution of data.</p>
</div>
<div id="bivariate-analysis" class="section level3">
<h3><span class="header-section-number">6.1.3</span> Bivariate Analysis</h3>
<p><strong>Covariance</strong></p>
<p>While variance provides an understanding of how values for a single variable vary, <strong>covariance</strong> is an unstandardized measure of how two variables vary together. Values can range from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>, and these values can be used to understand the direction of the linear relationship between variables. Positive covariance values indicate that the variables vary in the same direction (e.g., tend to increase or decrease together), while negative covariance values indicate that the variables vary in opposite directions (e.g., when one increases, the other decreases, or vice versa).</p>
<p>Covariance of a sample is defined by:</p>
<p><span class="math display">\[ cov_{x,y} = \frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{n-1} \]</span></p>
<p>It’s important to note that while covariance aids our understanding of the direction of the relationship between two variables, we cannot use it to understand the strength of the association since it is unstandardized. Due to differences in variables’ units of measurement, the strength of the relationship between two variables with large covariance could be weak, while the strength of the relationship between another pair of variables with small covariance could be strong.</p>
<p>In R, we can compute the covariance between a pair of numeric variables by passing the two vectors into the cov() function:</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1"><span class="co"># Calculate sample covariance between annual compensation and age using complete observations (missing values will cause issues if not addressed)</span></a>
<a class="sourceLine" id="cb39-2" data-line-number="2"><span class="kw">cov</span>(demographics<span class="op">$</span>annual_comp, demographics<span class="op">$</span>age, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>## [1] 453179.1</code></pre>
<p>In this example, using the default Pearson method, the covariance between annual compensation and age is 453179.1. The positive value indicates that annual compensation is generally higher for older employees and lower for younger employees.</p>
<p>Just as we multiplied the sample variance by <span class="math inline">\((n - 1) / n\)</span> to obtain the population variance, we can apply the same approach to convert the sample covariance returned by cov() to the population covariance:</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb41-1" data-line-number="1"><span class="co"># Calculate population covariance between annual compensation and age</span></a>
<a class="sourceLine" id="cb41-2" data-line-number="2"><span class="kw">cov</span>(demographics<span class="op">$</span>annual_comp, demographics<span class="op">$</span>age, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>) <span class="op">*</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>n</a></code></pre></div>
<pre><code>## [1] 453086.1</code></pre>
<p>The examples thus far have only examined associations between two variables at a time. However, rather than looking at isolated pairwise relationships, we can produce a covariance matrix to surface associations among many variables by passing a dataframe or matrix object into the cov() function:</p>
<div class="sourceCode" id="cb43"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb43-1" data-line-number="1"><span class="co"># Generate a correlation matrix among continuous variables</span></a>
<a class="sourceLine" id="cb43-2" data-line-number="2"><span class="kw">cov</span>(demographics[, <span class="kw">c</span>(<span class="st">&quot;annual_comp&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;org_tenure&quot;</span>, <span class="st">&quot;job_tenure&quot;</span>)], <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>##              annual_comp          age   org_tenure  job_tenure
## annual_comp 1.876688e+09 453179.07281 138551.67644 61576.73603
## age         4.531791e+05    168.38661     41.56006    17.96902
## org_tenure  1.385517e+05     41.56006     26.03270    11.04572
## job_tenure  6.157674e+04     17.96902     11.04572    13.29996</code></pre>
<p>Using the default Pearson method, the cov() function will return sample variances for each variable down the diagonal, since covariance is not applicable in the context of a variable with itself. We can confirm by producing the variance for age:</p>
<div class="sourceCode" id="cb45"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb45-1" data-line-number="1"><span class="co"># Return sample variance for age</span></a>
<a class="sourceLine" id="cb45-2" data-line-number="2"><span class="kw">var</span>(demographics<span class="op">$</span>age)</a></code></pre></div>
<pre><code>## [1] 168.3866</code></pre>
<p>As expected, the variance for age (<span class="math inline">\(s^{2} = 168.39\)</span>) matches the value found in the age x age cell of the covariance matrix.</p>
<p><strong>Correlation</strong></p>
<p><strong>Correlation</strong> is a scaled form of covariance. While covariance provides an unstandardized measure of the direction of a relationship between variables, correlation provides a standardized measure that can be used to quantify both the direction and strength of bivariate relationships. Correlation coefficients range from -1 to 1, where -1 indicates a perfectly negative association, 1 indicates a perfectly positive association, and 0 indicates the absence of an association. <strong>Pearson’s product-moment correlation coefficient</strong> <span class="math inline">\(r\)</span> is defined by:</p>
<p><span class="math display">\[ r_{x,y} = \frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sqrt{\sum_(x_{i}-\bar{x})^2\sum_(y_{i}-\bar{y})^2}} \]</span></p>
<p>In R, Pearson’s <span class="math inline">\(r\)</span> can be calculated using the cor() function:</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb47-1" data-line-number="1"><span class="co"># Calculate the correlation between annual compensation and age</span></a>
<a class="sourceLine" id="cb47-2" data-line-number="2"><span class="kw">cor</span>(demographics<span class="op">$</span>annual_comp, demographics<span class="op">$</span>age, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>## [1] 0.8061577</code></pre>
<p>While we already know that the relationship between annual compensation and age is positive based on the positive covariance coefficient, Pearson’s <span class="math inline">\(r\)</span> of .81 indicates that the strength of the positive association is strong (<span class="math inline">\(r\)</span> = 1 is perfectly positive). Though there are no absolute rules for categorizing the strength of relationships, as thresholds often vary by domain, the following is a general rule of thumb for interpreting the strength of bivariate associations:</p>
<ul>
<li>Weak = Absolute value of correlation coefficients between 0 and .3</li>
<li>Moderate = Absolute value of correlation coefficients between .4 and .6</li>
<li>Strong = Absolute value of correlation coefficients between .7 and 1</li>
</ul>
<p>There are several correlation coefficients, and the measurement scale of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> determine the appropriate type:</p>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-26"></span>
<img src="graphics/correlation_table.png" alt="Proper Applications of Correlation Coefficients" width="704" />
<p class="caption">
Figure 6.4: Proper Applications of Correlation Coefficients
</p>
</div>
<p><br /></p>
<p>Pearson’s <span class="math inline">\(r\)</span> can be used when both variables are measured on continuous scales or when one is continuous and the other is dichotomous (point-biserial correlation).</p>
<p>When one or both variables are ordinal, we can leverage <strong>Spearman’s</strong> <span class="math inline">\(\rho\)</span> or <strong>Kendall’s</strong> <span class="math inline">\(\tau\)</span>, which are both standardized nonparametric measures of the association between one or two rank-ordered variables. Let’s look at Spearman’s <span class="math inline">\(\rho\)</span>, which is defined as:</p>
<p><span class="math display">\[ \rho = 1 - {\frac {6 \sum d_i^2}{n(n^2 - 1)}} \]</span></p>
<p>We can override the default Pearson method in the cor() function to implement a specific form of rank correlation using the <code>method</code> argument:</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" data-line-number="1"><span class="co"># Calculate the correlation between job level and education level using Spearman&#39;s method</span></a>
<a class="sourceLine" id="cb49-2" data-line-number="2"><span class="kw">cor</span>(demographics<span class="op">$</span>job_level, demographics<span class="op">$</span>education, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>, <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>## [1] 0.6749776</code></pre>
<p>The <span class="math inline">\(\rho\)</span> coefficient of .67 indicates that the positive association we observed between job level and education level is moderate-to-strong. We could also pass <code>method = &quot;kendall&quot;</code> to this cor() function to implement Kendall’s <span class="math inline">\(\tau\)</span>.</p>
<p>The <strong>Phi Coefficient</strong> (<span class="math inline">\(\phi\)</span>), sometimes referred to as the mean square contingency coefficient or Matthews correlation in ML, can be used to understand the association between two dichotomous variables. For a 2x2 table for two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-27"></span>
<img src="graphics/phi_coefficient_formula.png" alt="2x2 Table for Random Variables x and y" width="395" />
<p class="caption">
Figure 6.5: 2x2 Table for Random Variables x and y
</p>
</div>
<p>The <span class="math inline">\(\phi\)</span> coefficient is defined as:</p>
<p><span class="math display">\[ \phi = {\frac {(AD-BC)}{\sqrt{(A+B)(C+D)(A+C)(B+D)}}} \]</span></p>
<p>To illustrate, let’s examine whether there is a relationship between gender and performance after transforming performance from its ordinal form to a dichotomous variable (high vs. low performance). We can leverage the <code>psych</code> library to calculate <span class="math inline">\(\phi\)</span> in R:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" data-line-number="1"><span class="co"># Load library for Phi Coefficient</span></a>
<a class="sourceLine" id="cb51-2" data-line-number="2"><span class="kw">library</span>(psych)</a>
<a class="sourceLine" id="cb51-3" data-line-number="3"></a>
<a class="sourceLine" id="cb51-4" data-line-number="4"><span class="co"># Set females to 1 and everything else to 0</span></a>
<a class="sourceLine" id="cb51-5" data-line-number="5">demographics<span class="op">$</span>gender_code &lt;-<span class="st"> </span><span class="kw">ifelse</span>(demographics<span class="op">$</span>gender <span class="op">==</span><span class="st"> &#39;Female&#39;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb51-6" data-line-number="6"></a>
<a class="sourceLine" id="cb51-7" data-line-number="7"><span class="co"># Set high performers (3 and above) to 1 and everything else to 0</span></a>
<a class="sourceLine" id="cb51-8" data-line-number="8">demographics<span class="op">$</span>performance_code &lt;-<span class="st"> </span><span class="kw">ifelse</span>(demographics<span class="op">$</span>performance <span class="op">&lt;</span><span class="st"> </span><span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb51-9" data-line-number="9"></a>
<a class="sourceLine" id="cb51-10" data-line-number="10"><span class="co"># Create a 2x2 contingency table</span></a>
<a class="sourceLine" id="cb51-11" data-line-number="11">contingency_tbl &lt;-<span class="st"> </span><span class="kw">table</span>(demographics<span class="op">$</span>gender_code, demographics<span class="op">$</span>performance_code)</a>
<a class="sourceLine" id="cb51-12" data-line-number="12"></a>
<a class="sourceLine" id="cb51-13" data-line-number="13"><span class="co"># Calculate the Phi Coefficient between dichotomous variables</span></a>
<a class="sourceLine" id="cb51-14" data-line-number="14"><span class="kw">phi</span>(contingency_tbl)</a></code></pre></div>
<pre><code>## [1] 0.01</code></pre>
<p><span class="math inline">\(\phi\)</span> is essentially 0, which means performance ratings are distributed equitably across gender categories (good news!).</p>
<p>A correlation matrix can be produced to surface associations among many variables by passing a dataframe or matrix object into the cor() function:</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" data-line-number="1"><span class="co"># Generate a correlation matrix among continuous variables</span></a>
<a class="sourceLine" id="cb53-2" data-line-number="2"><span class="kw">cor</span>(demographics[, <span class="kw">c</span>(<span class="st">&quot;annual_comp&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;org_tenure&quot;</span>, <span class="st">&quot;job_tenure&quot;</span>)], <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a></code></pre></div>
<pre><code>##             annual_comp       age org_tenure job_tenure
## annual_comp   1.0000000 0.8061577  0.6268392  0.3897584
## age           0.8061577 1.0000000  0.6277154  0.3797042
## org_tenure    0.6268392 0.6277154  1.0000000  0.5936211
## job_tenure    0.3897584 0.3797042  0.5936211  1.0000000</code></pre>
<p>Based on this correlation matrix, there are several moderate and strong pairwise associations in the data. The values down the diagonal are 1 because these represent the correlation between each variable and itself. You may also notice that the information above and below the diagonal is identical and, therefore, redundant.</p>
<p>A great R library for visualizing correlation matrices is corrplot. Several arguments can be specified for various visual representations of the relationships among variables:</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" data-line-number="1"><span class="co"># Load library for correlation visuals</span></a>
<a class="sourceLine" id="cb55-2" data-line-number="2"><span class="kw">library</span>(corrplot)</a>
<a class="sourceLine" id="cb55-3" data-line-number="3"></a>
<a class="sourceLine" id="cb55-4" data-line-number="4"><span class="co"># Store correlation matrix to object M</span></a>
<a class="sourceLine" id="cb55-5" data-line-number="5">M &lt;-<span class="st"> </span><span class="kw">cor</span>(demographics[, <span class="kw">c</span>(<span class="st">&quot;annual_comp&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;org_tenure&quot;</span>, <span class="st">&quot;job_tenure&quot;</span>)], <span class="dt">use =</span> <span class="st">&quot;complete.obs&quot;</span>)</a>
<a class="sourceLine" id="cb55-6" data-line-number="6"></a>
<a class="sourceLine" id="cb55-7" data-line-number="7"><span class="co"># Visualize correlation matrix</span></a>
<a class="sourceLine" id="cb55-8" data-line-number="8"><span class="kw">corrplot.mixed</span>(M, <span class="dt">order =</span> <span class="st">&#39;AOE&#39;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>It’s important to remember that correlation is not causation. Correlations can be spurious (variables related by chance), and drawing conclusions based on bivariate associations alone – especially in the absence of sound theoretical underpinnings – can be dangerous. Here are two examples of nearly perfect correlations between variables for which there is likely no true direct association:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-31"></span>
<img src="graphics/spurious_corr_maine_divorce.png" alt="Correlation between Maine Divorce Rate and Margarine Consumption (r = .99)" width="478" />
<p class="caption">
Figure 6.6: Correlation between Maine Divorce Rate and Margarine Consumption (r = .99)
</p>
</div>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-32"></span>
<img src="graphics/spurious_corr_mozzarella_cheese.png" alt="Correlation between Mozzarella Cheese Consumption and Civil Engineering Doctorate Conferrals (r = .96)" width="478" />
<p class="caption">
Figure 6.7: Correlation between Mozzarella Cheese Consumption and Civil Engineering Doctorate Conferrals (r = .96)
</p>
</div>
<p>In addition, covariance and correlation alone are not sufficient for determining whether an observed association in sample data is also present in the population. To understand the likelihood that patterns observed in sample data are also present in the larger population of interest, we need to move beyond descriptive measures.</p>
</div>
</div>
<div id="inferential-statistics" class="section level2">
<h2><span class="header-section-number">6.2</span> Inferential Statistics</h2>
<p>The objective of <strong>inferential statistics</strong> is to make inferences – with some degree of confidence – about a population based on available sample data. Several related concepts underpin this goal and will be covered here.</p>
<div id="introduction-to-probability" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Introduction to Probability</h3>
<p><strong>Randomness</strong> and uncertainty exist all around us. In <strong>probability theory</strong>, random phenomena refers to events or experiments whose outcomes cannot be predicted with certainty (Pishro-Nik, 2014). If you’ve taken a course in probability, there is a good chance you have considered the case of a fair coin flip – one of the most intuitive applications of probability. In the absence of information on how the coin is flipped, we cannot be certain of the outcome. What we can be certain of is that with a large number of coin flips, the proportion of heads will become increasingly close to 50%, or <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<p>The <strong>Law of Large Numbers (LLN)</strong> is an important theorem for building an intuitive understanding of how probability relates to the statistical inference concepts we will cover. In the case of a fair coin flip, it is possible to observe many consecutive heads by chance. This is because small samples can lend to anamalies. However, as the number of flips increases, we will undoubtedly observe an increasing number of tails; we expect a roughly equal number of heads and tails with a large enough number of flips.</p>
<p><strong>Conditional probability</strong> reflects the probability conditioned on the occurrence of a previous event or outcome. For example, we may find that the proportion of heads is greater or less than <span class="math inline">\(\frac{1}{2}\)</span> with a large number of fair coin flips when the coin is consistently heads up when flipped. The outcome is, therefore, conditioned on the fixed – rather than random – positioning of the coin when flipped.</p>
<p>Formally, <strong>Bayes’ Theorem (alternatively, Bayes’ Rule)</strong> states that for any two events A and B where the probability of A is not 0 (<span class="math inline">\(P(A) \neq 0\)</span>):</p>
<p><span class="math display">\[ P(B \vert A) = \frac{P(A \vert B) P(B)}{P(A)} \]</span></p>
<p>Bayes’ Rule allows us to more accurately predict the outcome by conditioning the probability on known factors rather than assuming all events operate under the same conditions. Bayes’ Rule is pervasive in people analytics, as the probability of outcomes can vary widely based on a person’s age, tenure, education, job, perceptions, relationships, and many other factors. For example, if we consider a company with 100 terminations over a 12-month period and average headcount of 1,000, the probability of attrition not conditioned on any other factor is 10%, or <span class="math inline">\(\frac{1}{10}\)</span>. Aside from trending this probability over time to identify if attrition is becoming more or less of a concern, this isn’t too helpful at the broader company level. However, if we condition the probability of attrition on an event – such as a recent manager exit – and find that the probability of attrition among those whose manager has left in the last six months is 70%, or <span class="math inline">\(\frac{7}{10}\)</span>, this is far more actionable (and concerning).</p>
</div>
<div id="central-limit-theorem" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Central Limit Theorem</h3>
<p>The <strong>Central Limit Theorem (CLT)</strong> is a mainstay of statistics and probability and fundamental to understanding the mechanics of multivariate inferential analysis techniques we will cover later in this book. The CLT was initially coined by a French-born mathematician named Abraham De Moivre in the 1700s. While initially unpopular, it was later reintroduced and attracted new interest from theorists and academics (Daw &amp; Pearson, 1972).</p>
<p>The CLT states that the average of independent random variables, when increased in number, tend to follow a normal (or Gaussian) distribution. The distribution of sample means approaches a normal distribution regardless of the shape of the population distribution from which the samples are drawn. This is important because the normal distribution has properties that can be used to test the likelihood that an observed value, difference, or relationship in a sample is also present in the population.</p>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-33"></span>
<img src="graphics/normal_distribution.png" alt="The Empirical Rule" width="686" />
<p class="caption">
Figure 6.8: The Empirical Rule
</p>
</div>
<p><br /></p>
<p>Let’s begin with an intuitive example of CLT. Imagine that we have a reliable way to measure how fun a population is on a 100-point scale, where 100 indicates maximum fun (life of the party) and 1 indicates maximum boringness. Consider that a small statistics conference is in progress at a nearby convention center, and there are 40 statisticians in attendance. In a separate room at the same convention center, there is also a group of 40 random people (non-statisticians) who are gathered to discuss some less interesting topic. Our job is to walk into one of the rooms and determine – on the basis of the “fun” factor alone – whether we have entered the statistics conference or the other, less interesting gathering of non-statisticians.</p>
<p>Instinctively, we already know the statisticians will be more fun than the other group. However, let’s assume we need the mean fun score and standard deviation of these two groups for this example. The group of statisticians have, on average, a fun score of 85 with a standard deviation of 2, while the group of non-statisticians are a bit less fun with a mean score of 65 and a standard deviation of 4. With a known population mean and standard deviation, the standard error (the standard deviation of the sample means) provides the ability to calculate the probability that the sample (the room of 40 people) belongs to the population of interest (fellow statisticians).</p>
<p>Herein lies the beauty of the CLT: roughly 68 percent of sample means will lie within one standard error of the population mean, roughly 95 percent within two standard errors of the population mean, and roughly 99 percent within three standard errors of the population mean. Therefore, any room whose members have an average fun score that is not within two standard errors of the population mean (between 81 and 89 for our statisticians) is statistically unlikely to be the group of statisticians for which we are searching. This is because in less than 5 in 100 cases could we randomly draw a ‘reasonably sized’ sample of statisticians with average funness so extremely different from the population average.</p>
<p>Because small samples lend to anomalies, we could – by chance – select a single person who happens to fall in the tails (extremely boring or extremely fun); however, as the sample size increases, it becomes more and more likely that the observed average reflects the average of the larger population. It would be virtually impossible (in less than 1 in 100 cases) to draw a random sample of statisticians from the population with average funness that is not within three standard errors of the population mean (between 79 and 91). Therefore, if we find that the room of people have an average fun score of 75, we will likely have far more fun in the other room!</p>
<p>Let’s now see the CLT in action by simulating a random uniform population distribution from which we can draw random samples. Remember, the shape of the population distribution does not matter; we could simulate an Exponential, Gamma, Poisson, Binomial, or other distribution and observe the same behavior.</p>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb56-1" data-line-number="1"><span class="co"># Set seed for reproducible random distribution</span></a>
<a class="sourceLine" id="cb56-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb56-3" data-line-number="3"></a>
<a class="sourceLine" id="cb56-4" data-line-number="4"><span class="co"># Generate uniform population distribution with 1000 values ranging from 1 to 100</span></a>
<a class="sourceLine" id="cb56-5" data-line-number="5">rand.unif &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>, <span class="dt">min =</span> <span class="dv">1</span>, <span class="dt">max =</span> <span class="dv">100</span>)</a></code></pre></div>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" data-line-number="1"><span class="co"># Calculate population mean</span></a>
<a class="sourceLine" id="cb57-2" data-line-number="2"><span class="kw">mean</span>(rand.unif)</a></code></pre></div>
<pre><code>## [1] 51.22007</code></pre>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" data-line-number="1"><span class="co"># Calculate population variance</span></a>
<a class="sourceLine" id="cb59-2" data-line-number="2">N =<span class="st"> </span><span class="kw">length</span>(rand.unif)</a>
<a class="sourceLine" id="cb59-3" data-line-number="3"><span class="kw">var</span>(rand.unif) <span class="op">*</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>N</a></code></pre></div>
<pre><code>## [1] 830.3155</code></pre>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" data-line-number="1"><span class="co"># Produce histogram to visualize population distribution</span></a>
<a class="sourceLine" id="cb61-2" data-line-number="2"><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb61-3" data-line-number="3"><span class="st">  </span><span class="kw">aes</span>(rand.unif) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb61-4" data-line-number="4"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb61-5" data-line-number="5"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb61-6" data-line-number="6"><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb61-7" data-line-number="7"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-37"></span>
<img src="_main_files/figure-html/unnamed-chunk-37-1.png" alt="Uniform Population Distribution (N = 1000)" width="672" />
<p class="caption">
Figure 6.9: Uniform Population Distribution (N = 1000)
</p>
</div>
<p>As expected, these randomly generated data are uniformly distributed. Next, we will draw 100 random samples of various sizes and plot the average of each.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1"><span class="co"># Define number of samples to draw from population distribution</span></a>
<a class="sourceLine" id="cb62-2" data-line-number="2">samples &lt;-<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb62-3" data-line-number="3"></a>
<a class="sourceLine" id="cb62-4" data-line-number="4"><span class="co"># Populate vector with sample sizes</span></a>
<a class="sourceLine" id="cb62-5" data-line-number="5">sample_n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">25</span>,<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb62-6" data-line-number="6"></a>
<a class="sourceLine" id="cb62-7" data-line-number="7"><span class="co"># Initialize empty data frame to hold sample means</span></a>
<a class="sourceLine" id="cb62-8" data-line-number="8">sample_means =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb62-9" data-line-number="9"></a>
<a class="sourceLine" id="cb62-10" data-line-number="10"><span class="co"># Set seed for reproducible random samples</span></a>
<a class="sourceLine" id="cb62-11" data-line-number="11"><span class="kw">set.seed</span>(<span class="dv">456</span>)</a>
<a class="sourceLine" id="cb62-12" data-line-number="12"></a>
<a class="sourceLine" id="cb62-13" data-line-number="13"><span class="co"># For each n, draw random samples</span></a>
<a class="sourceLine" id="cb62-14" data-line-number="14"><span class="cf">for</span> (n <span class="cf">in</span> sample_n) {</a>
<a class="sourceLine" id="cb62-15" data-line-number="15">  </a>
<a class="sourceLine" id="cb62-16" data-line-number="16">  <span class="cf">for</span> (draw <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>samples) {</a>
<a class="sourceLine" id="cb62-17" data-line-number="17">    </a>
<a class="sourceLine" id="cb62-18" data-line-number="18">      <span class="co"># Store sample means in data frame</span></a>
<a class="sourceLine" id="cb62-19" data-line-number="19">      sample_means &lt;-<span class="st"> </span><span class="kw">rbind</span>(sample_means, <span class="kw">cbind.data.frame</span>(</a>
<a class="sourceLine" id="cb62-20" data-line-number="20">                            <span class="dt">n =</span> n, </a>
<a class="sourceLine" id="cb62-21" data-line-number="21">                            <span class="dt">x_bar =</span> <span class="kw">mean</span>(<span class="kw">sample</span>(rand.unif, n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="ot">NULL</span>))))</a>
<a class="sourceLine" id="cb62-22" data-line-number="22">  }</a>
<a class="sourceLine" id="cb62-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb62-24" data-line-number="24"></a>
<a class="sourceLine" id="cb62-25" data-line-number="25"><span class="co"># Produce histograms to visualize distributions of sample means, grouped by n-count</span></a>
<a class="sourceLine" id="cb62-26" data-line-number="26">sample_means <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb62-27" data-line-number="27"><span class="st">  </span><span class="kw">aes</span>(<span class="dt">x =</span> x_bar, <span class="dt">fill =</span> n) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb62-28" data-line-number="28"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;x-bar&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb62-29" data-line-number="29"><span class="st">  </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb62-30" data-line-number="30"><span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb62-31" data-line-number="31"><span class="st">  </span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb62-32" data-line-number="32"><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>n)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-38"></span>
<img src="_main_files/figure-html/unnamed-chunk-38-1.png" alt="Distribution of 10,000 Sample Means of Varied Size" width="672" />
<p class="caption">
Figure 6.10: Distribution of 10,000 Sample Means of Varied Size
</p>
</div>
<p>Per the CLT, we can see that as n increases, the sample means become more normally distributed.</p>
</div>
<div id="confidence-intervals" class="section level3">
<h3><span class="header-section-number">6.2.3</span> Confidence Intervals</h3>
<p>A <strong>Confidence Interval (CI)</strong> represents a range of values likely to include a population parameter of interest (usually <span class="math inline">\(\mu\)</span>). A related concept that is fundamental to estimating CIs is the <strong>standard error (SE)</strong>, which is the standard deviation of sample means. While the standard deviation is a measure of variability for random variables, the variability captured by the SE reflects how representative the sample is of the population. Since sample statistics will approach the actual population parameters as the size of the sample increases, the SE and sample size are inversely related; that is, the SE decreases as the sample size increases. The SE is defined by:</p>
<p><span class="math display">\[ SE = \frac{\sigma}{\sqrt{n}} \]</span></p>
<p>Next, let’s validate that our simulated distribution of sample means adheres to the properties of normally distributed data per the Empirical Rule:</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" data-line-number="1"><span class="co"># Store sample means with n = 50</span></a>
<a class="sourceLine" id="cb63-2" data-line-number="2">x_bars &lt;-<span class="st"> </span>sample_means[sample_means<span class="op">$</span>n <span class="op">==</span><span class="st"> </span><span class="dv">50</span>, <span class="st">&quot;x_bar&quot;</span>]</a>
<a class="sourceLine" id="cb63-3" data-line-number="3"></a>
<a class="sourceLine" id="cb63-4" data-line-number="4"><span class="co"># Store sample size</span></a>
<a class="sourceLine" id="cb63-5" data-line-number="5">n &lt;-<span class="st"> </span><span class="kw">length</span>(x_bars)</a>
<a class="sourceLine" id="cb63-6" data-line-number="6"></a>
<a class="sourceLine" id="cb63-7" data-line-number="7"><span class="co"># Calculate percent of sample means within +/- 2 SEs</span></a>
<a class="sourceLine" id="cb63-8" data-line-number="8"><span class="kw">length</span>(x_bars[x_bars <span class="op">&lt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars) <span class="op">&amp;</span><span class="st"> </span>x_bars <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars)]) <span class="op">/</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a></code></pre></div>
<pre><code>## [1] 95.35</code></pre>
<p>97% of sample means are within 2 SEs, which is roughly what we expect per the characteristics of the normal distribution.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" data-line-number="1"><span class="co"># Calculate percent of sample means within +/- 3 SEs</span></a>
<a class="sourceLine" id="cb65-2" data-line-number="2"><span class="kw">length</span>(x_bars[x_bars <span class="op">&lt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars) <span class="op">&amp;</span><span class="st"> </span>x_bars <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars)]) <span class="op">/</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a></code></pre></div>
<pre><code>## [1] 99.79</code></pre>
<p>All of the sample means are within 3 SEs, indicating that it would be highly unlikely – nearly impossible even – to observe a sample mean ‘from the same population’ that falls outside this interval.</p>
<p>Now, let’s illustrate the relationship between CIs and standard errors using sample data from our uniform population distribution. In our example, both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are known and our sample size <span class="math inline">\(n\)</span> is at least 30; therefore, we can use a <strong>Z-Test</strong> to calculate the 95% CI. A <span class="math inline">\(z\)</span> score of 1.96 corresponds to the 95% CI for a two-tailed distribution (i.e., we are looking for significantly different values in either the larger or smaller direction) – the range of values we would expect to include <span class="math inline">\(\mu\)</span> in at least 95 of 100 random samples taken from the population:</p>
<p>The CI in this case is defined by:</p>
<p><span class="math display">\[ CI = \bar{x} \pm z_{\alpha/_2} \frac{\sigma}{\sqrt{n}} \]</span></p>
<p>Let’s randomly take <span class="math inline">\(n\)</span> = 100 from the population, and compute sample statistics to estimate the 95% CI:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" data-line-number="1"><span class="co"># Set seed for reproducible random samples</span></a>
<a class="sourceLine" id="cb67-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">456</span>)</a>
<a class="sourceLine" id="cb67-3" data-line-number="3"></a>
<a class="sourceLine" id="cb67-4" data-line-number="4"><span class="co"># Sample 100 values from uniform population distribution</span></a>
<a class="sourceLine" id="cb67-5" data-line-number="5">x &lt;-<span class="st"> </span><span class="kw">sample</span>(rand.unif, <span class="dv">100</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb67-6" data-line-number="6"></a>
<a class="sourceLine" id="cb67-7" data-line-number="7"><span class="co"># Calculate 95% CI</span></a>
<a class="sourceLine" id="cb67-8" data-line-number="8">ci95_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a>
<a class="sourceLine" id="cb67-9" data-line-number="9">ci95_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a></code></pre></div>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="co"># Print lower bound for 95% CI</span></a>
<a class="sourceLine" id="cb68-2" data-line-number="2">ci95_lower_bound</a></code></pre></div>
<pre><code>## [1] 47.90733</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1"><span class="co"># Print upper bound for 95% CI</span></a>
<a class="sourceLine" id="cb70-2" data-line-number="2">ci95_upper_bound</a></code></pre></div>
<pre><code>## [1] 58.98773</code></pre>
<p>Our known <span class="math inline">\(\mu\)</span> is 51.2, which is covered by our 95% CI (47.9 - 59.0). Per the CLT, in less than 5% of cases would we expect to draw a random sample from the population that results in a 95% CI which does not include <span class="math inline">\(\mu\)</span>. Note that our CI narrows with larger samples since our confidence that the range includes <span class="math inline">\(\mu\)</span> increases with more data.</p>
<p>Next, let’s look at a 99% CI. We will enter 2.576 for <span class="math inline">\(z\)</span>:</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1"><span class="co"># Calculate 99% CI</span></a>
<a class="sourceLine" id="cb72-2" data-line-number="2">ci99_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span><span class="fl">2.576</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a>
<a class="sourceLine" id="cb72-3" data-line-number="3">ci99_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">2.576</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a></code></pre></div>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" data-line-number="1"><span class="co"># Print lower bound for 99% CI</span></a>
<a class="sourceLine" id="cb73-2" data-line-number="2">ci99_lower_bound</a></code></pre></div>
<pre><code>## [1] 46.16612</code></pre>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" data-line-number="1"><span class="co"># Print upper bound for 99% CI</span></a>
<a class="sourceLine" id="cb75-2" data-line-number="2">ci99_upper_bound</a></code></pre></div>
<pre><code>## [1] 60.72893</code></pre>
<p>Like the 95% CI, this slightly wider 99% CI (46.2 - 60.7) also includes our <span class="math inline">\(\mu\)</span> of 51.2.</p>
<p>If <span class="math inline">\(\sigma\)</span> is not known, and/or we have a small sample (<span class="math inline">\(n\)</span> &lt; 30), we need to use a <strong>T-Test</strong> to calculate the CIs. In a people analytics setting, the reality is that population parameters are often unknown. For example, if we knew how engagement scores vary in the employee population, there would be no need to survey a sample of employees and make inferences about said population.</p>
<p>As we will see, the T-Test underpins many statistical tests and models germane to the people analytics discipline since we are often working with small datasets, so it is important to understand the mechanics. As shown in Figure <a href="stats.html#fig:t-distribution">6.11</a>, the <span class="math inline">\(t\)</span> distribution is increasingly wider and shorter relative to the normal distribution as the sample size decreases; this is also characteristic of the sampling distribution of means for smaller samples we observed in our CLT example. Specifically, <strong>degrees of freedom (df)</strong> is used to determine the shape of the probability distribution. Degrees of freedom represents the number of observations in the data that are free to vary when estimating statistical parameters, which is a function of the sample size (<span class="math inline">\(n - 1\)</span>). For example, if we could choose 1 of 5 projects to work on each day between Monday and Friday, we would only be able to <em>choose</em> 4 out of the 5 days; on Friday, only 1 project would remain to be selected, so our degrees of freedom (the number of days in which we have a choice between projects) would be 4.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:t-distribution"></span>
<img src="graphics/t_distribution.png" alt="t Distribution Shape by Degrees of Freedom" width="860" />
<p class="caption">
Figure 6.11: t Distribution Shape by Degrees of Freedom
</p>
</div>
<p>When estimating the CI for smaller samples, we need to leverage the wider, more platykurtic <span class="math inline">\(t\)</span> distribution to achieve greater accuracy. Therefore, the CI for a two-tailed test in this case is defined by:</p>
<p><span class="math display">\[ CI = \bar{x} \pm t_{\alpha/_2} \frac{\sigma}{\sqrt{n}} \]</span></p>
<p>Let’s compare CIs calculated using a T-Test to those calculated using the Z-Test. While a fixed <span class="math inline">\(z\)</span> score can be used for each CI level when <span class="math inline">\(n\)</span> &gt; 30, the <span class="math inline">\(t\)</span> statistic varies based on both the CI level and <span class="math inline">\(df\)</span>. We can reference the following table to obtain the <span class="math inline">\(t\)</span> statistic for each CI level:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-47"></span>
<img src="graphics/t-table.png" alt="Critical Values of Student's t Distribution" width="330" />
<p class="caption">
Figure 6.12: Critical Values of Student’s t Distribution
</p>
</div>
<p>For illustrative purposes, let’s draw a smaller sample of <span class="math inline">\(n\)</span> = 25 from our uniform population distribution and calculate the 95% CI using the <span class="math inline">\(t\)</span> statistic from the table (<span class="math inline">\(df\)</span> = 24):</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" data-line-number="1"><span class="co"># Set seed for reproducible random samples</span></a>
<a class="sourceLine" id="cb77-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">456</span>)</a>
<a class="sourceLine" id="cb77-3" data-line-number="3"></a>
<a class="sourceLine" id="cb77-4" data-line-number="4"><span class="co"># Sample 25 values from uniform population distribution</span></a>
<a class="sourceLine" id="cb77-5" data-line-number="5">x &lt;-<span class="st"> </span><span class="kw">sample</span>(rand.unif, <span class="dv">25</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb77-6" data-line-number="6"></a>
<a class="sourceLine" id="cb77-7" data-line-number="7"><span class="co"># Calculate 95% CI</span></a>
<a class="sourceLine" id="cb77-8" data-line-number="8">ci95_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span><span class="fl">2.064</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>))</a>
<a class="sourceLine" id="cb77-9" data-line-number="9">ci95_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">2.064</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>))</a></code></pre></div>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" data-line-number="1"><span class="co"># Print lower bound for 95% CI</span></a>
<a class="sourceLine" id="cb78-2" data-line-number="2">ci95_lower_bound</a></code></pre></div>
<pre><code>## [1] 35.24305</code></pre>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb80-1" data-line-number="1"><span class="co"># Print upper bound for 95% CI</span></a>
<a class="sourceLine" id="cb80-2" data-line-number="2">ci95_upper_bound</a></code></pre></div>
<pre><code>## [1] 59.60959</code></pre>
<p>As expected, the 95% CI using the <span class="math inline">\(t\)</span> statistic is much wider (35.2 - 59.6), acknowledging the increased uncertainty in estimating population parameters given this smaller sample. To increase our confidence to the 99% level, the interval widens even further (30.9 - 63.9):</p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb82-1" data-line-number="1"><span class="co"># Calculate 99% CI</span></a>
<a class="sourceLine" id="cb82-2" data-line-number="2">ci99_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span><span class="fl">2.797</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>))</a>
<a class="sourceLine" id="cb82-3" data-line-number="3">ci99_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">2.797</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>))</a></code></pre></div>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="co"># Print lower bound for 99% CI</span></a>
<a class="sourceLine" id="cb83-2" data-line-number="2">ci99_lower_bound</a></code></pre></div>
<pre><code>## [1] 30.91633</code></pre>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="co"># Print upper bound for 99% CI</span></a>
<a class="sourceLine" id="cb85-2" data-line-number="2">ci99_upper_bound</a></code></pre></div>
<pre><code>## [1] 63.93631</code></pre>
<p><strong>Hypothesis Testing</strong></p>
<p><strong>Hypothesis testing</strong> is how we leverage CIs to test whether a significant difference or relationship exists in the data. Sir Ronald Fisher invented what is known as the null hypothesis, which states that there is no relationship/difference; disprove me if you can! The null hypothesis is defined by:</p>
<p><span class="math display">\[ H_0: \mu_A = \mu_B \]</span></p>
<p>The objective of hypothesis testing is to determine if there is sufficient evidence to reject the null hypothesis in favor of an alternative hypothesis. The null hypothesis always states that there is ‘nothing’ of significance. For example, if we want to test whether an intervention has an effect on an outcome in a population, the null hypothesis states that there is no effect. If we want to test whether there is a difference in average scores between two groups in a population, the null hypothesis states that there is no difference.</p>
<p>An alternative hypothesis may simply state that there is a difference or relationship in the population, or it may specify the expected direction (e.g., Population A has a significantly ‘larger’ or ‘smaller’ average value than Population B; Variable A is ‘positively’ or ‘negatively’ related to Variable B). Therefore, alternative hypotheses are defined by:</p>
<p><span class="math display">\[ H_A: \mu_A \neq \mu_B \]</span></p>
<p><span class="math display">\[ H_A: \mu_A &lt; \mu_B \]</span></p>
<p><span class="math display">\[ H_A: \mu_A &gt; \mu_B \]</span></p>
<p><strong>Alpha</strong></p>
<p>The <strong>alpha</strong> level of a hypothesis test, denoted by <span class="math inline">\(\alpha\)</span>, represents the probability of obtaining observed results due to chance if the null hypothesis is true. In other words, <span class="math inline">\(\alpha\)</span> is the probability of rejecting the null hypothesis (and therefore claiming that there is a significant difference or relationship) when in fact we should have failed to reject it because there is insufficient evidence to support the alternative hypothesis.</p>
<p><span class="math inline">\(\alpha\)</span> is often set at .05 but is sometimes set at a more rigorous .01, depending upon the context and tolerance for error. An <span class="math inline">\(\alpha\)</span> of .05 corresponds to a 95% CI (1 - .05), and .01 to a 99% CI (1 - .01). With non-directional alternative hypotheses, we must divide <span class="math inline">\(\alpha\)</span> by 2 (i.e., we could observe a significant result in either tail of the distribution), while one-tailed tests position the rejection region entirely within one tail based on what is being hypothesized.</p>
<p>At the .05 level, we would conclude that a finding is statistically significant if the chance of observing a value at least as extreme as the one observed is less than 1 in 20 if the null hypothesis is true. Note that we observed this behavior with our simulated distribution of sample means. While we could observe more extreme values by chance with repeated attempts, in less than 1 in every 20 times would we expect a 95% CI that does not capture <span class="math inline">\(\mu\)</span>. Moreover, in less than 1 in every 100 times should we expect a sample with a 99% CI that does not capture <span class="math inline">\(\mu\)</span>.</p>
<p><strong>Beta</strong></p>
<p>Another key value is <strong>Beta</strong>, denoted by <span class="math inline">\(\beta\)</span>, which relates to the power of the analysis. Simply put, power reflects our ability to find a difference or relationship if there is one. Power is calculated by 1 - <span class="math inline">\(\beta\)</span>. At this point, it should be intuitive that larger samples increase our chances of observing significant results. As we observed in the T-Test example, CIs for small samples (<span class="math inline">\(n\)</span> &lt; 30) are quite wide relative to those for large samples; therefore, the power of the analysis to detect significance is limited given how extremely different values of <span class="math inline">\(x\)</span> must be to observe non-overlapping CIs.</p>
<p><strong>Type I &amp; II Errors</strong></p>
<p>A <strong>Type I Error</strong> is a false positive, wherein we conclude that there is a significant difference or relationship when there is not. A <strong>Type II Error</strong> is a false negative, wherein we fail to capture a significant finding. <span class="math inline">\(\alpha\)</span> represents our chance of making a Type I Error, while <span class="math inline">\(\beta\)</span> represents our chance of making a Type II Error. I once had a professor explain that committing a Type I error is a shame, while committing a Type II error is a pity, and I’ve found this to be a helpful way to remember what each type of error represents.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-54"></span>
<img src="graphics/hypothesis_testing_errors.png" alt="Type I and II Errors" width="447" />
<p class="caption">
Figure 6.13: Type I and II Errors
</p>
</div>
<p><strong>P-Values</strong></p>
<p>In statistical tests, the <strong>p-value</strong> is referenced to determine whether the null hypothesis can be rejected. The p-value represents the probability of obtaining a result at least as extreme as the one observed if the null hypothesis is true. As a general rule, if <span class="math inline">\(p\)</span> &lt; .05, we can confidently reject the null hypothesis and conclude that the observed difference or relationship was unlikely a chance observation.</p>
<p>While statistical significance helps us understand the probability of observing results by chance when there is actually no difference or effect in the population, it does not tell us anything about the size of the difference or effect. Analysis should never be reduced to inspecting p-values; in fact, p-values have been the subject of much controversy among researchers and practitioners in recent years. Later chapters will cover how to interpret results of statistical tests to surface the story and determine if there is anything ‘practically’ significant among statistically significant findings.</p>
<p><strong>Bonferroni Correction</strong></p>
<p>One caveat when leveraging a p-value to determine statistical significance is that when multiple testing is performed – that is, multiple tests using the same sample data – the probability of a Type I error increases by a factor equivalent to the number of tests performed. It’s important to note that there is not agreement among statisticians about how (or even whether) the p-value threshold for statistical significance needs to be adjusted to account for this increased risk. Nevertheless, we will cover this conservative approach for mitigating this risk.</p>
<p>Thus far, we have only discussed statistical significance in the context of a <strong>per analysis error rate</strong> – that is, the probability of committing a Type I error for a single statistical test. However, when two or more tests are being conducted on the same sample, the <strong>familywise error rate</strong> is an important factor in determining statistical significance. The familywise error rate reflects the fact that as we conduct more and more analyses on the same sample, the probability of a Type I error across the set (or family) of analyses increases. The familywise error rate can be calculated by:</p>
<p><span class="math display">\[ \alpha_{FW} = 1 - (1 - \alpha_{PC})^C,  \]</span></p>
<p>where <span class="math inline">\(c\)</span> is equal to the number of comparisons (or statistical tests) performed, and <span class="math inline">\(\alpha_{PC}\)</span> is equal to the specified per analysis error rate (usually .05). For example, if <span class="math inline">\(\alpha\)</span> = .05 per analysis, the probability of a Type I error with three tests on the same data increases from 5% to 14.3%: <span class="math inline">\(1 - (1 - .05)^3 = .143\)</span>.</p>
<p>The most common method of adjusting the familywise error rate down to the specified per analysis error rate is the <strong>Bonferroni Correction</strong>. To implement this correction, we can simply divide <span class="math inline">\(\alpha\)</span> by the number of analyses performed on the dataset – such as <span class="math inline">\(\alpha / 3 = .017\)</span> in the case of three analyses with <span class="math inline">\(\alpha = .05\)</span>. This means that for each statistical test, we must achieve <span class="math inline">\(p &lt; .017\)</span> to report a statistically significant result. An alternative which allows us to achieve the same number of statistically significant results is to multiply the unadjusted per analysis p-values for each statistical test by the number of tests. For example, if we run three statistical tests and receive <span class="math inline">\(p = .014\)</span>, <span class="math inline">\(p = .047\)</span>, and <span class="math inline">\(p = .125\)</span>, we would achieve one significant result with the first method (<span class="math inline">\(p &lt; .017\)</span>) as well as with the alternative since the first statistical test satisfies the per analysis error rate (<span class="math inline">\(p &lt; .05\)</span>): <span class="math inline">\(p = .014 * 3 = .042\)</span>.</p>
<p>Perneger (1998) is one of many who oppose the use of the Bonferroni Correction, suggesting that these “adjustments are, at best, unnecessary and, at worst, deleterious to sound statistical inference.” The Bonferroni Correction is controversial among researchers because while applying the correction reduces the chance of a Type I error, it also increases the chance of a Type II error. Because this correction makes it more difficult to detect significant results, it is rare to find such a correction reported in published research, though research often involves multiple testing on the same sample. Perneger suggests that simply describing the statistical tests that were performed, and why, is sufficient for dealing with potential problems introduced by multiple testing.</p>
</div>
</div>
<div id="parametric-vs.nonparametric-tests" class="section level2">
<h2><span class="header-section-number">6.3</span> Parametric vs. Nonparametric Tests</h2>
<p><strong>Parametric statistics</strong> assume the population is normally distributed. <strong>Nonparametric statistics</strong> do not assume anything about the population parameters or distribution and are, therefore, often referred to as distribution free tests. Assuming the normality assumption holds, parametric tests generally have more power than their nonparametric counterparts. This means that with a nonparametric test, we are less likely to reject the null hypothesis when it is false if the data come from normally distributed populations.</p>
<p>Since the mean is the most common measure of central tendency, parametric tests usually focus on comparing the mean or variance of data. You may recall that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are sufficient to characterize a population distribution when data are situated symmetrically around the mean. However, the mean can be sensitive to outliers. If outliers are present in the data, the median may be a better way of representing the central tendency of data; in this case, nonparametric tests may be more appropriate.</p>
<p>In addition to normally distributed data in the population, and ensuring outliers are not materially influencing the mean, parametric tests also assume <strong>homogeneity of variance</strong> and <strong>independence</strong>. Homogeneity of variance assumes the variances across multiple groups are equal, though parametric tests are generally robust to violations of equal variances when the sample sizes are large. The assumption of independence requires observations to be randomly sampled from the population and independent of one another; that is, the value of one observation does not influence or depend on the value of another.</p>
<p>Spearman’s correlation coefficient, which we used to evaluate the relationship between job level and education in <a href="#corr-lvl-ed"><strong>??</strong></a>, is a nonparametric test since ordinal data are usually not normally distributed in the population. There is a nonparametric equivalent for each parametric test, and these will be reviewed in detail in <a href="aod.html#aod">8</a>.</p>
<p>It’s important to remember that the normal distribution properties under the CLT relate to the sampling distribution of means – not to the distribution of the population or to the data for one individual sample. The CLT is important for estimating population parameters, but it does not transform a population distribution from nonnormal to normal. If we know the population distribution is nonnormal (e.g., ordinal, nominal, or skewed data), nonparametric tests should be leveraged.</p>
</div>
<div id="exercises" class="section level2">
<h2><span class="header-section-number">6.4</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li><p>Parameters are descriptions or characteristics of a sample, while statistics are descriptions or characteristics of a population.
<br />A. True
<br />B. False</p></li>
<li><p>Which of the following is an example of a null hypothesis, where “µ” reflects the mean of a population?
<br />A. µA = µB
<br />B. µA ≠ µB
<br />C. µA &lt; µB
<br />D. µA &gt; µB
<br />E. None of the above; all are examples of alternative hypotheses.</p></li>
<li><p>Which of the following describes a Type I Error?
<br />A. Failing to reject the null hypothesis when it is false
<br />B. Rejecting the null hypothesis when it is true
<br />C. Reporting something as significant when nothing of significance is present (a shame)
<br />D. Failing to detect something of significance (a pity)
<br />E. Both B and C
<br />F. Both A and D</p></li>
<li><p>100 randomly selected employees in the Marketing department of an organization participated in a survey on career pathing for marketing professionals. What is the sample and what is the population sampled in this case?
<br />A. Sample: 100 employees who completed the survey, Population: All employees in the organization
<br />B. Sample: 100 employees who completed the survey, Population: Marketing employees
<br />C. Sample: All Marketing professionals, Population: All employees in the organization
<br />D. Sample: All employees in the organization, Population: Employees across all companies globally</p></li>
<li><p>The primary purpose of inferential statistics is to make inferences about a population based on sample data. Inferential statistics allows these inferences to be made with defined levels of confidence that what is observed in a sample is also characteristic of the larger population.
<br />A. True
<br />B. False</p></li>
<li><p>The median tends to be a better measure of central tendency since the mean is more sensitive to extreme values (outliers)?
<br />A. True
<br />B. False</p></li>
<li><p>The standard deviation represents the ‘average’ amount by which individual values for a variable deviate (or vary) from the mean. A large standard deviation indicates there is considerable spread in the data, whereas a small standard deviation indicates the mean is fairly representative of the data.
<br />A. True
<br />B. False</p></li>
<li><p>A positively skewed distribution has its largest allocation to the left and a negative distribution to the right.
<br />A. True
<br />B. False</p></li>
<li><p>Large covariance coefficients always indicate strong bivariate associations.
<br />A. True
<br />B. False</p></li>
<li><p>A T-Test should be used when <span class="math inline">\(\sigma\)</span> is unknown and/or <span class="math inline">\(n\)</span> &lt; 30.
<br />A. True
<br />B. False</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="measurement-sampling.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-preparation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/r/04_statistical_fundamentals.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
