<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14 Unsupervised Learning | The Fundamentals of People Analytics: With Applications in R</title>
  <meta name="description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="14 Unsupervised Learning | The Fundamentals of People Analytics: With Applications in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="github-repo" content="crstarbuck/peopleanalytics-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14 Unsupervised Learning | The Fundamentals of People Analytics: With Applications in R" />
  
  <meta name="twitter:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  

<meta name="author" content="Craig Starbuck" />


<meta name="date" content="2022-09-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="pred-mod.html"/>
<link rel="next" href="data-viz.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<link href="libs/tabwid-1.0.0/scrool.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Fundamentals of People Analytics: With Applications in R</a></li>

<li class="divider"></li>
<li><a href="dedication.html#dedication" id="toc-dedication">Dedication</a></li>
<li><a href="foreword.html#foreword" id="toc-foreword">Foreword</a></li>
<li><a href="preface.html#preface" id="toc-preface">Preface</a></li>
<li><a href="getting-started.html#getting-started" id="toc-getting-started"><span class="toc-section-number">1</span> Getting Started</a>
<ul>
<li><a href="getting-started.html#guiding-principles" id="toc-guiding-principles"><span class="toc-section-number">1.1</span> Guiding Principles</a>
<ul>
<li><a href="getting-started.html#pro-employee-thinking" id="toc-pro-employee-thinking"><span class="toc-section-number">1.1.1</span> Pro-Employee Thinking</a></li>
<li><a href="getting-started.html#quality" id="toc-quality"><span class="toc-section-number">1.1.2</span> Quality</a></li>
<li><a href="getting-started.html#prioritization" id="toc-prioritization"><span class="toc-section-number">1.1.3</span> Prioritization</a></li>
</ul></li>
<li><a href="getting-started.html#tooling" id="toc-tooling"><span class="toc-section-number">1.2</span> Tooling</a></li>
<li><a href="getting-started.html#data-sets" id="toc-data-sets"><span class="toc-section-number">1.3</span> Data Sets</a>
<ul>
<li><a href="getting-started.html#employees" id="toc-employees"><span class="toc-section-number">1.3.1</span> Employees</a></li>
<li><a href="getting-started.html#turnover-trends" id="toc-turnover-trends"><span class="toc-section-number">1.3.2</span> Turnover Trends</a></li>
<li><a href="getting-started.html#survey-responses" id="toc-survey-responses"><span class="toc-section-number">1.3.3</span> Survey Responses</a></li>
</ul></li>
<li><a href="getting-started.html#d-framework" id="toc-d-framework"><span class="toc-section-number">1.4</span> 4D Framework</a></li>
</ul></li>
<li><a href="r-intro.html#r-intro" id="toc-r-intro"><span class="toc-section-number">2</span> Introduction to R</a>
<ul>
<li><a href="r-intro.html#getting-started-1" id="toc-getting-started-1"><span class="toc-section-number">2.1</span> Getting Started</a>
<ul>
<li><a href="r-intro.html#installing-r" id="toc-installing-r"><span class="toc-section-number">2.1.1</span> Installing R</a></li>
<li><a href="r-intro.html#installing-r-studio" id="toc-installing-r-studio"><span class="toc-section-number">2.1.2</span> Installing R Studio</a></li>
<li><a href="r-intro.html#installing-packages" id="toc-installing-packages"><span class="toc-section-number">2.1.3</span> Installing Packages</a></li>
<li><a href="r-intro.html#case-sensitivity" id="toc-case-sensitivity"><span class="toc-section-number">2.1.4</span> Case Sensitivity</a></li>
<li><a href="r-intro.html#help" id="toc-help"><span class="toc-section-number">2.1.5</span> Help</a></li>
<li><a href="r-intro.html#objects" id="toc-objects"><span class="toc-section-number">2.1.6</span> Objects</a></li>
<li><a href="r-intro.html#comments" id="toc-comments"><span class="toc-section-number">2.1.7</span> Comments</a></li>
<li><a href="r-intro.html#testing-early-and-often" id="toc-testing-early-and-often"><span class="toc-section-number">2.1.8</span> Testing Early and Often</a></li>
</ul></li>
<li><a href="r-intro.html#vectors" id="toc-vectors"><span class="toc-section-number">2.2</span> Vectors</a></li>
<li><a href="r-intro.html#matrices" id="toc-matrices"><span class="toc-section-number">2.3</span> Matrices</a></li>
<li><a href="r-intro.html#factors" id="toc-factors"><span class="toc-section-number">2.4</span> Factors</a></li>
<li><a href="r-intro.html#data-frames" id="toc-data-frames"><span class="toc-section-number">2.5</span> Data Frames</a></li>
<li><a href="r-intro.html#lists" id="toc-lists"><span class="toc-section-number">2.6</span> Lists</a></li>
<li><a href="r-intro.html#loops" id="toc-loops"><span class="toc-section-number">2.7</span> Loops</a></li>
<li><a href="r-intro.html#user-defined-functions-udfs" id="toc-user-defined-functions-udfs"><span class="toc-section-number">2.8</span> User-Defined Functions (UDFs)</a></li>
<li><a href="r-intro.html#graphics" id="toc-graphics"><span class="toc-section-number">2.9</span> Graphics</a></li>
<li><a href="r-intro.html#review-questions" id="toc-review-questions"><span class="toc-section-number">2.10</span> Review Questions</a></li>
</ul></li>
<li><a href="sql-intro.html#sql-intro" id="toc-sql-intro"><span class="toc-section-number">3</span> Introduction to SQL</a>
<ul>
<li><a href="sql-intro.html#basics" id="toc-basics"><span class="toc-section-number">3.1</span> Basics</a></li>
<li><a href="sql-intro.html#aggregate-functions" id="toc-aggregate-functions"><span class="toc-section-number">3.2</span> Aggregate Functions</a></li>
<li><a href="sql-intro.html#joins" id="toc-joins"><span class="toc-section-number">3.3</span> Joins</a></li>
<li><a href="sql-intro.html#subqueries" id="toc-subqueries"><span class="toc-section-number">3.4</span> Subqueries</a></li>
<li><a href="sql-intro.html#virtual-tables" id="toc-virtual-tables"><span class="toc-section-number">3.5</span> Virtual Tables</a></li>
<li><a href="sql-intro.html#window-functions" id="toc-window-functions"><span class="toc-section-number">3.6</span> Window Functions</a></li>
<li><a href="sql-intro.html#common-table-expressions-ctes" id="toc-common-table-expressions-ctes"><span class="toc-section-number">3.7</span> Common Table Expressions (CTEs)</a></li>
<li><a href="sql-intro.html#review-questions-1" id="toc-review-questions-1"><span class="toc-section-number">3.8</span> Review Questions</a></li>
</ul></li>
<li><a href="measure-sampl.html#measure-sampl" id="toc-measure-sampl"><span class="toc-section-number">4</span> Measurement &amp; Sampling</a>
<ul>
<li><a href="measure-sampl.html#variable-types" id="toc-variable-types"><span class="toc-section-number">4.1</span> Variable Types</a>
<ul>
<li><a href="measure-sampl.html#independent-variables-iv" id="toc-independent-variables-iv"><span class="toc-section-number">4.1.1</span> Independent Variables (IV)</a></li>
<li><a href="measure-sampl.html#dependent-variables-dv" id="toc-dependent-variables-dv"><span class="toc-section-number">4.1.2</span> Dependent Variables (DV)</a></li>
<li><a href="measure-sampl.html#control-variables-cv" id="toc-control-variables-cv"><span class="toc-section-number">4.1.3</span> Control Variables (CV)</a></li>
<li><a href="measure-sampl.html#moderating-variables" id="toc-moderating-variables"><span class="toc-section-number">4.1.4</span> Moderating Variables</a></li>
<li><a href="measure-sampl.html#mediating-variables" id="toc-mediating-variables"><span class="toc-section-number">4.1.5</span> Mediating Variables</a></li>
<li><a href="measure-sampl.html#endogenous-vs.-exogenous-variables" id="toc-endogenous-vs.-exogenous-variables"><span class="toc-section-number">4.1.6</span> Endogenous vs. Exogenous Variables</a></li>
</ul></li>
<li><a href="measure-sampl.html#measurement-scales" id="toc-measurement-scales"><span class="toc-section-number">4.2</span> Measurement Scales</a>
<ul>
<li><a href="measure-sampl.html#discrete-variables" id="toc-discrete-variables"><span class="toc-section-number">4.2.1</span> Discrete Variables</a></li>
<li><a href="measure-sampl.html#continuous-variables" id="toc-continuous-variables"><span class="toc-section-number">4.2.2</span> Continuous Variables</a></li>
</ul></li>
<li><a href="measure-sampl.html#sampling" id="toc-sampling"><span class="toc-section-number">4.3</span> Sampling</a>
<ul>
<li><a href="measure-sampl.html#sampling-nonsampling-error" id="toc-sampling-nonsampling-error"><span class="toc-section-number">4.3.1</span> Sampling &amp; Nonsampling Error</a></li>
</ul></li>
<li><a href="measure-sampl.html#review-questions-2" id="toc-review-questions-2"><span class="toc-section-number">4.4</span> Review Questions</a></li>
</ul></li>
<li><a href="research.html#research" id="toc-research"><span class="toc-section-number">5</span> Research Fundamentals</a>
<ul>
<li><a href="research.html#research-questions" id="toc-research-questions"><span class="toc-section-number">5.1</span> Research Questions</a></li>
<li><a href="research.html#research-hypotheses" id="toc-research-hypotheses"><span class="toc-section-number">5.2</span> Research Hypotheses</a></li>
<li><a href="research.html#internal-vs.-external-validity" id="toc-internal-vs.-external-validity"><span class="toc-section-number">5.3</span> Internal vs. External Validity</a></li>
<li><a href="research.html#research-methods" id="toc-research-methods"><span class="toc-section-number">5.4</span> Research Methods</a></li>
<li><a href="research.html#research-designs" id="toc-research-designs"><span class="toc-section-number">5.5</span> Research Designs</a></li>
<li><a href="research.html#review-questions-3" id="toc-review-questions-3"><span class="toc-section-number">5.6</span> Review Questions</a></li>
</ul></li>
<li><a href="data-prep.html#data-prep" id="toc-data-prep"><span class="toc-section-number">6</span> Data Preparation</a>
<ul>
<li><a href="data-prep.html#data-extraction" id="toc-data-extraction"><span class="toc-section-number">6.1</span> Data Extraction</a>
<ul>
<li><a href="data-prep.html#data-architecture" id="toc-data-architecture"><span class="toc-section-number">6.1.1</span> Data Architecture</a></li>
<li><a href="data-prep.html#database-normalization" id="toc-database-normalization"><span class="toc-section-number">6.1.2</span> Database Normalization</a></li>
<li><a href="data-prep.html#modern-data-infrastructure" id="toc-modern-data-infrastructure"><span class="toc-section-number">6.1.3</span> Modern Data Infrastructure</a></li>
</ul></li>
<li><a href="data-prep.html#data-screening-cleaning" id="toc-data-screening-cleaning"><span class="toc-section-number">6.2</span> Data Screening &amp; Cleaning</a>
<ul>
<li><a href="data-prep.html#missingness" id="toc-missingness"><span class="toc-section-number">6.2.1</span> Missingness</a></li>
<li><a href="data-prep.html#outliers" id="toc-outliers"><span class="toc-section-number">6.2.2</span> Outliers</a></li>
<li><a href="data-prep.html#low-variability" id="toc-low-variability"><span class="toc-section-number">6.2.3</span> Low Variability</a></li>
<li><a href="data-prep.html#inconsistent-categories" id="toc-inconsistent-categories"><span class="toc-section-number">6.2.4</span> Inconsistent Categories</a></li>
<li><a href="data-prep.html#data-binning" id="toc-data-binning"><span class="toc-section-number">6.2.5</span> Data Binning</a></li>
</ul></li>
<li><a href="data-prep.html#one-hot-encoding" id="toc-one-hot-encoding"><span class="toc-section-number">6.3</span> One-Hot Encoding</a></li>
<li><a href="data-prep.html#feature-engineering" id="toc-feature-engineering"><span class="toc-section-number">6.4</span> Feature Engineering</a></li>
<li><a href="data-prep.html#review-questions-4" id="toc-review-questions-4"><span class="toc-section-number">6.5</span> Review Questions</a></li>
</ul></li>
<li><a href="desc-stats.html#desc-stats" id="toc-desc-stats"><span class="toc-section-number">7</span> Descriptive Statistics</a>
<ul>
<li><a href="desc-stats.html#univariate-analysis" id="toc-univariate-analysis"><span class="toc-section-number">7.1</span> Univariate Analysis</a>
<ul>
<li><a href="desc-stats.html#measures-of-central-tendency" id="toc-measures-of-central-tendency"><span class="toc-section-number">7.1.1</span> Measures of Central Tendency</a></li>
<li><a href="desc-stats.html#measures-of-spread" id="toc-measures-of-spread"><span class="toc-section-number">7.1.2</span> Measures of Spread</a></li>
</ul></li>
<li><a href="desc-stats.html#bivariate-analysis" id="toc-bivariate-analysis"><span class="toc-section-number">7.2</span> Bivariate Analysis</a>
<ul>
<li><a href="desc-stats.html#covariance" id="toc-covariance"><span class="toc-section-number">7.2.1</span> Covariance</a></li>
<li><a href="desc-stats.html#correlation" id="toc-correlation"><span class="toc-section-number">7.2.2</span> Correlation</a></li>
</ul></li>
<li><a href="desc-stats.html#review-questions-5" id="toc-review-questions-5"><span class="toc-section-number">7.3</span> Review Questions</a></li>
</ul></li>
<li><a href="inf-stats.html#inf-stats" id="toc-inf-stats"><span class="toc-section-number">8</span> Statistical Inference</a>
<ul>
<li><a href="inf-stats.html#introduction-to-probability" id="toc-introduction-to-probability"><span class="toc-section-number">8.1</span> Introduction to Probability</a>
<ul>
<li><a href="inf-stats.html#probability-distributions" id="toc-probability-distributions"><span class="toc-section-number">8.1.1</span> Probability Distributions</a></li>
<li><a href="inf-stats.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">8.1.2</span> Conditional Probability</a></li>
</ul></li>
<li><a href="inf-stats.html#central-limit-theorem" id="toc-central-limit-theorem"><span class="toc-section-number">8.2</span> Central Limit Theorem</a></li>
<li><a href="inf-stats.html#confidence-intervals" id="toc-confidence-intervals"><span class="toc-section-number">8.3</span> Confidence Intervals</a>
<ul>
<li><a href="inf-stats.html#hypothesis-testing" id="toc-hypothesis-testing"><span class="toc-section-number">8.3.1</span> Hypothesis Testing</a></li>
<li><a href="inf-stats.html#alpha" id="toc-alpha"><span class="toc-section-number">8.3.2</span> Alpha</a></li>
<li><a href="inf-stats.html#type-i-ii-errors" id="toc-type-i-ii-errors"><span class="toc-section-number">8.3.3</span> Type I &amp; II Errors</a></li>
<li><a href="inf-stats.html#p-values" id="toc-p-values"><span class="toc-section-number">8.3.4</span> <span class="math inline">\(p\)</span>-Values</a></li>
<li><a href="inf-stats.html#bonferroni-correction" id="toc-bonferroni-correction"><span class="toc-section-number">8.3.5</span> Bonferroni Correction</a></li>
<li><a href="inf-stats.html#statistical-power" id="toc-statistical-power"><span class="toc-section-number">8.3.6</span> Statistical Power</a></li>
</ul></li>
<li><a href="inf-stats.html#review-questions-6" id="toc-review-questions-6"><span class="toc-section-number">8.4</span> Review Questions</a></li>
</ul></li>
<li><a href="aod.html#aod" id="toc-aod"><span class="toc-section-number">9</span> Analysis of Differences</a>
<ul>
<li><a href="aod.html#parametric-vs.-nonparametric-tests" id="toc-parametric-vs.-nonparametric-tests"><span class="toc-section-number">9.1</span> Parametric vs. Nonparametric Tests</a></li>
<li><a href="aod.html#differences-in-discrete-data" id="toc-differences-in-discrete-data"><span class="toc-section-number">9.2</span> Differences in Discrete Data</a></li>
<li><a href="aod.html#differences-in-continuous-data" id="toc-differences-in-continuous-data"><span class="toc-section-number">9.3</span> Differences in Continuous Data</a></li>
<li><a href="aod.html#review-questions-7" id="toc-review-questions-7"><span class="toc-section-number">9.4</span> Review Questions</a></li>
</ul></li>
<li><a href="lm.html#lm" id="toc-lm"><span class="toc-section-number">10</span> Linear Regression</a>
<ul>
<li><a href="lm.html#sample-size" id="toc-sample-size"><span class="toc-section-number">10.1</span> Sample Size</a></li>
<li><a href="lm.html#simple-linear-regression" id="toc-simple-linear-regression"><span class="toc-section-number">10.2</span> Simple Linear Regression</a></li>
<li><a href="lm.html#multiple-linear-regression" id="toc-multiple-linear-regression"><span class="toc-section-number">10.3</span> Multiple Linear Regression</a></li>
<li><a href="lm.html#moderation" id="toc-moderation"><span class="toc-section-number">10.4</span> Moderation</a></li>
<li><a href="lm.html#mediation" id="toc-mediation"><span class="toc-section-number">10.5</span> Mediation</a></li>
<li><a href="lm.html#review-questions-8" id="toc-review-questions-8"><span class="toc-section-number">10.6</span> Review Questions</a></li>
</ul></li>
<li><a href="lme.html#lme" id="toc-lme"><span class="toc-section-number">11</span> Linear Model Extensions</a>
<ul>
<li><a href="lme.html#model-comparisons" id="toc-model-comparisons"><span class="toc-section-number">11.1</span> Model Comparisons</a></li>
<li><a href="lme.html#hierarchical-regression" id="toc-hierarchical-regression"><span class="toc-section-number">11.2</span> Hierarchical Regression</a></li>
<li><a href="lme.html#multilevel-models" id="toc-multilevel-models"><span class="toc-section-number">11.3</span> Multilevel Models</a></li>
<li><a href="lme.html#polynomial-regression" id="toc-polynomial-regression"><span class="toc-section-number">11.4</span> Polynomial Regression</a></li>
<li><a href="lme.html#review-questions-9" id="toc-review-questions-9"><span class="toc-section-number">11.5</span> Review Questions</a></li>
</ul></li>
<li><a href="log.html#log" id="toc-log"><span class="toc-section-number">12</span> Logistic Regression</a>
<ul>
<li><a href="log.html#binomial-logistic-regression" id="toc-binomial-logistic-regression"><span class="toc-section-number">12.1</span> Binomial Logistic Regression</a></li>
<li><a href="log.html#multinomial-logistic-regression" id="toc-multinomial-logistic-regression"><span class="toc-section-number">12.2</span> Multinomial Logistic Regression</a></li>
<li><a href="log.html#ordinal-logistic-regression" id="toc-ordinal-logistic-regression"><span class="toc-section-number">12.3</span> Ordinal Logistic Regression</a></li>
<li><a href="log.html#review-questions-10" id="toc-review-questions-10"><span class="toc-section-number">12.4</span> Review Questions</a></li>
</ul></li>
<li><a href="pred-mod.html#pred-mod" id="toc-pred-mod"><span class="toc-section-number">13</span> Predictive Modeling</a>
<ul>
<li><a href="pred-mod.html#cross-validation" id="toc-cross-validation"><span class="toc-section-number">13.1</span> Cross-Validation</a></li>
<li><a href="pred-mod.html#model-performance" id="toc-model-performance"><span class="toc-section-number">13.2</span> Model Performance</a></li>
<li><a href="pred-mod.html#bias-variance-tradeoff" id="toc-bias-variance-tradeoff"><span class="toc-section-number">13.3</span> Bias-Variance Tradeoff</a></li>
<li><a href="pred-mod.html#tree-based-algorithms" id="toc-tree-based-algorithms"><span class="toc-section-number">13.4</span> Tree-Based Algorithms</a></li>
<li><a href="pred-mod.html#predictive-modeling" id="toc-predictive-modeling"><span class="toc-section-number">13.5</span> Predictive Modeling</a></li>
<li><a href="pred-mod.html#review-questions-11" id="toc-review-questions-11"><span class="toc-section-number">13.6</span> Review Questions</a></li>
</ul></li>
<li><a href="unsup-lrn.html#unsup-lrn" id="toc-unsup-lrn"><span class="toc-section-number">14</span> Unsupervised Learning</a>
<ul>
<li><a href="unsup-lrn.html#factor-analysis" id="toc-factor-analysis"><span class="toc-section-number">14.1</span> Factor Analysis</a>
<ul>
<li><a href="unsup-lrn.html#exploratory-factor-analysis-efa" id="toc-exploratory-factor-analysis-efa"><span class="toc-section-number">14.1.1</span> Exploratory Factor Analysis (EFA)</a></li>
<li><a href="unsup-lrn.html#confirmatory-factor-analysis-cfa" id="toc-confirmatory-factor-analysis-cfa"><span class="toc-section-number">14.1.2</span> Confirmatory Factor Analysis (CFA)</a></li>
</ul></li>
<li><a href="unsup-lrn.html#clustering" id="toc-clustering"><span class="toc-section-number">14.2</span> Clustering</a>
<ul>
<li><a href="unsup-lrn.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">14.2.1</span> <span class="math inline">\(K\)</span>-Means Clustering</a></li>
<li><a href="unsup-lrn.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">14.2.2</span> Hierarchical Clustering</a></li>
</ul></li>
<li><a href="unsup-lrn.html#review-questions-12" id="toc-review-questions-12"><span class="toc-section-number">14.3</span> Review Questions</a></li>
</ul></li>
<li><a href="data-viz.html#data-viz" id="toc-data-viz"><span class="toc-section-number">15</span> Data Visualization</a>
<ul>
<li><a href="data-viz.html#best-practices" id="toc-best-practices"><span class="toc-section-number">15.1</span> Best Practices</a>
<ul>
<li><a href="data-viz.html#color-palette" id="toc-color-palette"><span class="toc-section-number">15.1.1</span> Color Palette</a></li>
<li><a href="data-viz.html#chart-borders" id="toc-chart-borders"><span class="toc-section-number">15.1.2</span> Chart Borders</a></li>
<li><a href="data-viz.html#zero-baseline" id="toc-zero-baseline"><span class="toc-section-number">15.1.3</span> Zero Baseline</a></li>
<li><a href="data-viz.html#intuitive-layout" id="toc-intuitive-layout"><span class="toc-section-number">15.1.4</span> Intuitive Layout</a></li>
<li><a href="data-viz.html#preattentive-attributes" id="toc-preattentive-attributes"><span class="toc-section-number">15.1.5</span> Preattentive Attributes</a></li>
</ul></li>
<li><a href="data-viz.html#step-by-step-visual-upgrade" id="toc-step-by-step-visual-upgrade"><span class="toc-section-number">15.2</span> Step-by-Step Visual Upgrade</a>
<ul>
<li><a href="data-viz.html#step-1-build-bar-chart-with-defaults" id="toc-step-1-build-bar-chart-with-defaults"><span class="toc-section-number">15.2.1</span> Step 1: Build Bar Chart with Defaults</a></li>
<li><a href="data-viz.html#step-2-remove-legend" id="toc-step-2-remove-legend"><span class="toc-section-number">15.2.2</span> Step 2: Remove Legend</a></li>
<li><a href="data-viz.html#step-3-assign-colors-strategically" id="toc-step-3-assign-colors-strategically"><span class="toc-section-number">15.2.3</span> Step 3: Assign Colors Strategically</a></li>
<li><a href="data-viz.html#step-4-add-axis-titles-and-margins" id="toc-step-4-add-axis-titles-and-margins"><span class="toc-section-number">15.2.4</span> Step 4: Add Axis Titles and Margins</a></li>
<li><a href="data-viz.html#step-5-add-left-justified-title" id="toc-step-5-add-left-justified-title"><span class="toc-section-number">15.2.5</span> Step 5: Add Left-Justified Title</a></li>
<li><a href="data-viz.html#step-6-remove-background" id="toc-step-6-remove-background"><span class="toc-section-number">15.2.6</span> Step 6: Remove Background</a></li>
<li><a href="data-viz.html#step-7-remove-axis-ticks" id="toc-step-7-remove-axis-ticks"><span class="toc-section-number">15.2.7</span> Step 7: Remove Axis Ticks</a></li>
<li><a href="data-viz.html#step-8-mute-titles" id="toc-step-8-mute-titles"><span class="toc-section-number">15.2.8</span> Step 8: Mute Titles</a></li>
<li><a href="data-viz.html#step-9-flip-axes" id="toc-step-9-flip-axes"><span class="toc-section-number">15.2.9</span> Step 9: Flip Axes</a></li>
<li><a href="data-viz.html#step-10-sort-data" id="toc-step-10-sort-data"><span class="toc-section-number">15.2.10</span> Step 10: Sort Data</a></li>
</ul></li>
<li><a href="data-viz.html#visualization-types" id="toc-visualization-types"><span class="toc-section-number">15.3</span> Visualization Types</a>
<ul>
<li><a href="data-viz.html#tables" id="toc-tables"><span class="toc-section-number">15.3.1</span> Tables</a></li>
<li><a href="data-viz.html#heatmaps" id="toc-heatmaps"><span class="toc-section-number">15.3.2</span> Heatmaps</a></li>
<li><a href="data-viz.html#scatterplots" id="toc-scatterplots"><span class="toc-section-number">15.3.3</span> Scatterplots</a></li>
<li><a href="data-viz.html#line-graphs" id="toc-line-graphs"><span class="toc-section-number">15.3.4</span> Line Graphs</a></li>
<li><a href="data-viz.html#slopegraphs" id="toc-slopegraphs"><span class="toc-section-number">15.3.5</span> Slopegraphs</a></li>
<li><a href="data-viz.html#bar-charts" id="toc-bar-charts"><span class="toc-section-number">15.3.6</span> Bar Charts</a></li>
<li><a href="data-viz.html#combination-charts" id="toc-combination-charts"><span class="toc-section-number">15.3.7</span> Combination Charts</a></li>
<li><a href="data-viz.html#waterfall-charts" id="toc-waterfall-charts"><span class="toc-section-number">15.3.8</span> Waterfall Charts</a></li>
<li><a href="data-viz.html#waffle-charts" id="toc-waffle-charts"><span class="toc-section-number">15.3.9</span> Waffle Charts</a></li>
<li><a href="data-viz.html#sankey-diagrams" id="toc-sankey-diagrams"><span class="toc-section-number">15.3.10</span> Sankey Diagrams</a></li>
<li><a href="data-viz.html#pie-charts" id="toc-pie-charts"><span class="toc-section-number">15.3.11</span> Pie Charts</a></li>
<li><a href="data-viz.html#d-visuals" id="toc-d-visuals"><span class="toc-section-number">15.3.12</span> 3D Visuals</a></li>
</ul></li>
<li><a href="data-viz.html#elegant-data-visualization" id="toc-elegant-data-visualization"><span class="toc-section-number">15.4</span> Elegant Data Visualization</a></li>
<li><a href="data-viz.html#review-questions-13" id="toc-review-questions-13"><span class="toc-section-number">15.5</span> Review Questions</a></li>
</ul></li>
<li><a href="storytelling.html#storytelling" id="toc-storytelling"><span class="toc-section-number">16</span> Data Storytelling</a>
<ul>
<li><a href="storytelling.html#know-your-audience" id="toc-know-your-audience"><span class="toc-section-number">16.1</span> Know Your Audience</a></li>
<li><a href="storytelling.html#production-status" id="toc-production-status"><span class="toc-section-number">16.2</span> Production Status</a></li>
<li><a href="storytelling.html#structural-elements" id="toc-structural-elements"><span class="toc-section-number">16.3</span> Structural Elements</a>
<ul>
<li><a href="storytelling.html#tldr" id="toc-tldr"><span class="toc-section-number">16.3.1</span> TL;DR</a></li>
<li><a href="storytelling.html#purpose" id="toc-purpose"><span class="toc-section-number">16.3.2</span> Purpose</a></li>
<li><a href="storytelling.html#methodology" id="toc-methodology"><span class="toc-section-number">16.3.3</span> Methodology</a></li>
<li><a href="storytelling.html#results" id="toc-results"><span class="toc-section-number">16.3.4</span> Results</a></li>
<li><a href="storytelling.html#limitations" id="toc-limitations"><span class="toc-section-number">16.3.5</span> Limitations</a></li>
<li><a href="storytelling.html#next-steps" id="toc-next-steps"><span class="toc-section-number">16.3.6</span> Next Steps</a></li>
<li><a href="storytelling.html#appendix" id="toc-appendix"><span class="toc-section-number">16.3.7</span> Appendix</a></li>
</ul></li>
<li><a href="storytelling.html#qa" id="toc-qa"><span class="toc-section-number">16.4</span> Q&amp;A</a></li>
<li><a href="storytelling.html#review-questions-14" id="toc-review-questions-14"><span class="toc-section-number">16.5</span> Review Questions</a></li>
</ul></li>
<li><a href="bibli.html#bibli" id="toc-bibli"><span class="toc-section-number">17</span> Bibliography</a></li>
<li><a href="storytelling.html#appendix" id="toc-appendix"><span class="toc-section-number">18</span> Appendix</a>
<ul>
<li><a href="appendix.html#d-framework-1" id="toc-d-framework-1"><span class="toc-section-number">18.1</span> 4D Framework</a>
<ul>
<li><a href="appendix.html#discover" id="toc-discover"><span class="toc-section-number">18.1.1</span> Discover</a></li>
<li><a href="appendix.html#design" id="toc-design"><span class="toc-section-number">18.1.2</span> Design</a></li>
<li><a href="appendix.html#develop" id="toc-develop"><span class="toc-section-number">18.1.3</span> Develop</a></li>
<li><a href="appendix.html#deliver" id="toc-deliver"><span class="toc-section-number">18.1.4</span> Deliver</a></li>
</ul></li>
<li><a href="appendix.html#data-visualization" id="toc-data-visualization"><span class="toc-section-number">18.2</span> Data Visualization</a>
<ul>
<li><a href="appendix.html#step-by-step-visual-upgrade-1" id="toc-step-by-step-visual-upgrade-1"><span class="toc-section-number">18.2.1</span> Step-by-Step Visual Upgrade</a></li>
<li><a href="appendix.html#tables-1" id="toc-tables-1"><span class="toc-section-number">18.2.2</span> Tables</a></li>
<li><a href="appendix.html#heatmaps-1" id="toc-heatmaps-1"><span class="toc-section-number">18.2.3</span> Heatmaps</a></li>
<li><a href="appendix.html#scatterplots-1" id="toc-scatterplots-1"><span class="toc-section-number">18.2.4</span> Scatterplots</a></li>
<li><a href="appendix.html#line-charts" id="toc-line-charts"><span class="toc-section-number">18.2.5</span> Line Charts</a></li>
<li><a href="appendix.html#slopegraphs-1" id="toc-slopegraphs-1"><span class="toc-section-number">18.2.6</span> Slopegraphs</a></li>
<li><a href="appendix.html#bar-charts-1" id="toc-bar-charts-1"><span class="toc-section-number">18.2.7</span> Bar Charts</a></li>
<li><a href="appendix.html#combination-charts-1" id="toc-combination-charts-1"><span class="toc-section-number">18.2.8</span> Combination Charts</a></li>
<li><a href="appendix.html#waterfall-charts-1" id="toc-waterfall-charts-1"><span class="toc-section-number">18.2.9</span> Waterfall Charts</a></li>
<li><a href="appendix.html#waffle-charts-1" id="toc-waffle-charts-1"><span class="toc-section-number">18.2.10</span> Waffle Charts</a></li>
<li><a href="appendix.html#sankey-diagrams-1" id="toc-sankey-diagrams-1"><span class="toc-section-number">18.2.11</span> Sankey Diagrams</a></li>
<li><a href="appendix.html#pie-charts-1" id="toc-pie-charts-1"><span class="toc-section-number">18.2.12</span> Pie Charts</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Fundamentals of People Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="unsup-lrn" class="section level1" number="14">
<h1><span class="header-section-number">14</span> Unsupervised Learning</h1>
<p>The inferential and predictive models covered thus far can be categorized as <strong>supervised learning</strong> models. For each observation <span class="math inline">\(x_i\)</span> in the data, there is an associated response <span class="math inline">\(y_i\)</span> in a supervised learning setting, and the goal is to fit a model that relates <span class="math inline">\(y\)</span> to one or more predictors to understand relationships or predict future values on the basis of the identified associations. However, in an <strong>unsupervised learning</strong> setting, no response <span class="math inline">\(y_i\)</span> is associated with <span class="math inline">\(x_i\)</span>. As a result, we cannot <em>supervise</em> the analysis and are limited to understanding how observations cluster or group together based on patterns across the available <span class="math inline">\(p\)</span> attributes.</p>
<p>People analytics often involves the unique challenge of analyzing high-dimensional data with a large number of <span class="math inline">\(p\)</span> attributes but relatively few <span class="math inline">\(n\)</span> observations – a phenomenon often referred to as the <em>curse of dimensionality</em>. Given the sample size requirements covered in previous chapters, we ideally want <span class="math inline">\(n\)</span> to be an order of magnitude larger than <span class="math inline">\(p\)</span> to support statistical power and increase our chances of detecting meaningful patterns and population effects in sample data. Since people data sets are often wide and short, <strong>dimension reduction</strong> is important for reducing the dimensions to a limited subset that captures the majority of the information and optimizes the <span class="math inline">\(n:p\)</span> ratio.</p>
<p>Consider a case in which a colleague uses verbose rhetoric to convey a simple message that could be effectively communicated with fewer words. The superfluous language is unnecessary and does not provide additional information or value. This is analogous to dimension reduction in that we are interested in identifying a limited set of meaningful attributes and discarding redundant and unimportant information that does not contribute to the analysis objectives.</p>
<p>Dimensionality reduction techniques project data onto a lower-dimensional subspace that retains the majority of the variance in the data points. If we take a picture of a group of colleagues during a team outing, for example, we would lose some 3D information by encoding the information into a 2D image. This 2D representation is a <em>subspace</em> of the 3D coordinates. While we would not know how far one person is from another in the 2D representation, we would see that people in the back appear smaller than people in the front. Therefore, the perspective in the 2D image would still capture <em>some</em> information about distance. The limited information loss in moving from three to two dimensions is likely acceptable given the benefit of capturing the memory of the team in a photograph.</p>
<p>Dimension reduction is particularly important in survey research because longer surveys are costly and may result in lower response rates due to the increased completion time requirements. Survey instrumentation with strong psychometric properties features highly correlated survey items for constructs that are relatively uncorrelated with survey items used to measure other independent constructs. Intuitively, we know that highly correlated variables do not capture unique information, as one is a sufficient proxy to capture the available signal in the larger number of features. As we have covered, models with highly correlated variables can create problems due to multicollinearity, and dimension reduction is an alternative approach to variable selection techniques such as the backward stepwise procedure covered in Chapter <a href="lm.html#lm">10</a>.</p>
<p>This chapter will cover dimension reduction fundamentals as well as technical implementations.</p>
<div id="factor-analysis" class="section level2" number="14.1">
<h2><span class="header-section-number">14.1</span> Factor Analysis</h2>
<p>The development of survey instrumentation, whether a single item or a larger multidimensional scale, begins with a good theory. The theory provides conceptual support for the construct – the particular dimensions that characterize the construct, the antecedent variables which theoretically influence it, and the outcomes it will likely drive. With a strong theoretical framework, the researcher can begin proposing ways of <em>operationalizing</em> the conceptual scheme into a measurement approach.</p>
<p>There are clear measurement approaches for business metrics such as leads generated, new business growth, cNPS, and net revenue but in the social sciences, we often need indicators of latent constructs that are difficult – or impossible – to directly measure. If we want to understand the extent to which employees are engaged in their work, we need a comprehensive measure that captures facets of the theoretical frame. For example, vigor, absorption, and dedication are dimensions of Schaufeli, Bakker, and Salanova’s (2006) conception of work engagement which were operationalized in the Utrecht Work Engagement Scale (UWES).</p>
<p>Quantifying the energy levels one brings to work (vigor), the extent to which one feels time passes quickly while working (absorption), and the level of one’s commitment to seeing tasks through to completion (dedication) is challenging since we cannot leverage transactional data or digital exhaust to directly quantify this as we can with operational business metrics. We need a comprehensive – yet parsimonious – set of survey items that function as indicators of the dimensions of the latent work engagement construct. Constructing a larger aggregate measure from the individual indicators, such as the average or sum of all survey items, enables us to reduce the number of variables and optimize the <span class="math inline">\(n:p\)</span> ratio in supervised learning settings.</p>
<div id="exploratory-factor-analysis-efa" class="section level3" number="14.1.1">
<h3><span class="header-section-number">14.1.1</span> Exploratory Factor Analysis (EFA)</h3>
<p><strong>Exploratory factor analysis (EFA)</strong> is a variable reduction technique by which factors are extracted from data – usually as part of the development process for new survey instruments.</p>
<p>A researcher may work with a panel of experts in a particular domain to develop an inventory of items that tap various aspects of the construct per the theoretical framework that underpins it. Based on how the items cluster together, the empirical data will be reconciled against the theoretical conception to define dimensions of the measure. Within a cluster of highly correlated items for a particular dimension, the researcher needs to decide which items are essential and which are redundant and eligible for removal. Aside from the principal clusters (or factors), remaining items also need to be evaluated for their relevance and support for the underlying theory. If items are believed to be members of the theoretical dimensions but do not cluster together with other similar items, it may be indicative of poorly written items that have different interpretations among survey takers. EFA is the empirical process that supports these objectives.</p>
<p>To illustrate the steps for EFA, we will leverage the <code>survey_responses</code> data.</p>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb410-1"><a href="unsup-lrn.html#cb410-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb410-2"><a href="unsup-lrn.html#cb410-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb410-3"><a href="unsup-lrn.html#cb410-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb410-4"><a href="unsup-lrn.html#cb410-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load survey response data</span></span>
<span id="cb410-5"><a href="unsup-lrn.html#cb410-5" aria-hidden="true" tabindex="-1"></a>survey_dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_book/master/data/survey_responses.csv&quot;</span>)</span>
<span id="cb410-6"><a href="unsup-lrn.html#cb410-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb410-7"><a href="unsup-lrn.html#cb410-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Show dimensions of survey data</span></span>
<span id="cb410-8"><a href="unsup-lrn.html#cb410-8" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(survey_dat)</span></code></pre></div>
<pre><code>## [1] 400  12</code></pre>
<p>EFA is implemented via a three-step procedure:</p>
<ol style="list-style-type: decimal">
<li>Assess the factorability of the data.</li>
<li>Extract the factors.</li>
<li>Rotate and interpret the factors.</li>
</ol>
<p><strong>Step 1: Factorability Assessment</strong></p>
<p>With respect to factorability, there needs to be some correlation among variables in order for a dimension reduction technique to collapse variables into linear combinations that capture a large portion of the variance in the data. The data feature sufficient factorability if we achieve a <strong>Kaiser-Meyer-Olkin (KMO)</strong> statistic of at least <span class="math inline">\(.60\)</span> (Kaiser, 1974) and <strong>Bartlett’s Test of Sphericity</strong> reaches statistical significance (Bartlett, 1954). The KMO statistic estimates the proportion of variance that may be common variance; the lower the proportion, the greater the factorability. Bartlett’s test essentially measures the degree of redundancy in the data, where the null hypothesis states that the variables are orthogonal (uncorrelated); rejecting this null hypothesis indicates that there is sufficient correlation for dimension reduction.</p>
<p>The <code>KMO()</code> and <code>cortest.bartlett()</code> functions from the <code>psych</code> library can be used for the KMO statistic and Bartlett’s test, respectively:</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="unsup-lrn.html#cb412-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb412-2"><a href="unsup-lrn.html#cb412-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb412-3"><a href="unsup-lrn.html#cb412-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb412-4"><a href="unsup-lrn.html#cb412-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Kaiser-Meyer-Olkin (KMO) statistic</span></span>
<span id="cb412-5"><a href="unsup-lrn.html#cb412-5" aria-hidden="true" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">KMO</span>(survey_dat)</span></code></pre></div>
<pre><code>## Kaiser-Meyer-Olkin factor adequacy
## Call: psych::KMO(r = survey_dat)
## Overall MSA =  0.9
## MSA for each item = 
##  belong  effort    incl   eng_1   eng_2   eng_3    happ psafety   ret_1   ret_2 
##    0.94    0.86    0.86    0.86    0.89    0.89    0.92    0.90    0.91    0.89 
##   ret_3  ldrshp 
##    0.90    0.93</code></pre>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="unsup-lrn.html#cb414-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Bartlett&#39;s Test of Sphericity</span></span>
<span id="cb414-2"><a href="unsup-lrn.html#cb414-2" aria-hidden="true" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">cortest.bartlett</span>(<span class="fu">cor</span>(survey_dat), <span class="fu">nrow</span>(survey_dat))</span></code></pre></div>
<pre><code>## $chisq
## [1] 2933.161
## 
## $p.value
## [1] 0
## 
## $df
## [1] 66</code></pre>
<p>Data satisfy the factorability requirements since <span class="math inline">\(KMO = .90\)</span> (<code>Overall MSA</code>) and Bartlett’s test is significant at the <span class="math inline">\(p &lt; .001\)</span> level.</p>
<p><strong>Step 2: Factor Extraction</strong></p>
<p>For the second step, we will visually inspect a <strong>scree plot</strong> and determine how many factors are necessary to explain most of the variance in the data. A scree plot is a line plot that helps visualize the portion of the total variance explained by each factor using <strong>eigenvalues</strong>. While the linear algebraic underpinnings are out of scope for this book, it is important to understand that <strong>eigenvectors</strong> are vectors of a linear transformation which have corresponding eigenvalues <span class="math inline">\(\lambda\)</span> that represent factors by which the vectors are scaled. As a general rule, factors with <span class="math inline">\(\lambda \ge 1\)</span> are extracted when running a factor analysis.</p>
<p>The <code>scree()</code> function from the <code>psych</code> library can be used to generate a scree plot:</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="unsup-lrn.html#cb416-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce scree plot</span></span>
<span id="cb416-2"><a href="unsup-lrn.html#cb416-2" aria-hidden="true" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">scree</span>(survey_dat, <span class="at">pc =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:scree-plot"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/scree-plot-1.png" alt="Scree plot showing eigenvalues by factor relative to the extraction threshold (horizontal line)." width="100%" />
<p class="caption">
Figure 14.1: Scree plot showing eigenvalues by factor relative to the extraction threshold (horizontal line).
</p>
</div>
<p>Based on Figure <a href="unsup-lrn.html#fig:scree-plot">14.1</a>, factors 1 and 2 appear to provide relatively outsized information gain as <span class="math inline">\(\lambda &gt; 1\)</span> for both.</p>
<p>You may notice the <code>pc = FALSE</code> argument in the <code>scree()</code> function call. This relates to <strong>principal components analysis (PCA)</strong>, which is an alternative method of dimension reduction. Principal components are new independent variables that represent linear transformations of scaled (<span class="math inline">\((x - \bar{x}) / s\)</span>) versions of the observed variables. While we will focus on factor analysis and PCA in this section, which are most common in the social sciences, there are additional dimension reduction techniques one could explore (e.g., <strong>parallel analysis</strong>).</p>
<p>While there are similarities between factor analysis and PCA, the mathematics are fundamentally different. PCA approaches dimension reduction by creating one or more index variables (linear combinations of original variables) from a larger set of measured variables; these new index variables are referred to as components. On the other hand, factor analysis can be viewed as a set of regression equations with weighted relationships that represent the measurement of a latent variable. Most variables are latent in social psychology contexts since we cannot directly measure constructs like motivation or belonging, so we instead measure indicators of the latent variables using question sets on surveys.</p>
<p>To illustrate how to extract principal components, we can use base R’s <code>prcomp()</code> function:</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="unsup-lrn.html#cb417-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb417-2"><a href="unsup-lrn.html#cb417-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb417-3"><a href="unsup-lrn.html#cb417-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb417-4"><a href="unsup-lrn.html#cb417-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform PCA</span></span>
<span id="cb417-5"><a href="unsup-lrn.html#cb417-5" aria-hidden="true" tabindex="-1"></a>pca <span class="ot">&lt;-</span> <span class="fu">prcomp</span>(survey_dat, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span>
<span id="cb417-6"><a href="unsup-lrn.html#cb417-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb417-7"><a href="unsup-lrn.html#cb417-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate explained variance for each principal component</span></span>
<span id="cb417-8"><a href="unsup-lrn.html#cb417-8" aria-hidden="true" tabindex="-1"></a>pca_var <span class="ot">=</span> (pca<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(pca<span class="sc">$</span>sdev<span class="sc">^</span><span class="dv">2</span>)) <span class="sc">*</span> <span class="dv">100</span></span>
<span id="cb417-9"><a href="unsup-lrn.html#cb417-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb417-10"><a href="unsup-lrn.html#cb417-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create scree plot</span></span>
<span id="cb417-11"><a href="unsup-lrn.html#cb417-11" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">qplot</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(pca_var), pca_var) <span class="sc">+</span> </span>
<span id="cb417-12"><a href="unsup-lrn.html#cb417-12" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb417-13"><a href="unsup-lrn.html#cb417-13" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(pca_var)) <span class="sc">+</span></span>
<span id="cb417-14"><a href="unsup-lrn.html#cb417-14" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Principal Component&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Variance Explained (%)&quot;</span>) <span class="sc">+</span></span>
<span id="cb417-15"><a href="unsup-lrn.html#cb417-15" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pca-var-plot"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/pca-var-plot-1.png" alt="Plot showing the percent of total variance explained by 12 principal components." width="100%" />
<p class="caption">
Figure 14.2: Plot showing the percent of total variance explained by 12 principal components.
</p>
</div>
<p>Note that while we can theoretically have as many factors as we have variables (<span class="math inline">\(p = 12\)</span>), this defeats the purpose of dimension reduction – whether PCA or factor analysis. The objective of dimension reduction is to obtain a smaller number of factors (or components) that capture the majority of the information in the data.</p>
<p>As shown in Figure <a href="unsup-lrn.html#fig:pca-var-plot">14.2</a>, principal components are sorted in descending order according to the percent of total variance they explain. The first principal component alone explains nearly half of the total variance in the data. We are looking for the <em>elbow</em> to ascertain the inflection point at which explained variance plateaus. It is clear that the slope of the line begins to flatten beyond the third principal component, indicating that components 4-12 provide relatively little information. Put differently, we could extract only the first three components without sacrificing much information and gain the benefit of fewer more meaningful variables.</p>
<p>As an aside, in a supervised learning context we could insert these principal components as predictors in a regression model in lieu of a larger number of original variables. This is known as <strong>principal components regression (PCR)</strong>. However, given the importance of explaining models in a people analytics setting, PCR will not be covered since inserting index variables as predictors in the model compromises interpretability.</p>
<p><strong>Step 3: Factor Rotation &amp; Interpretation</strong></p>
<p>For the third step, we will use an oblimin method to rotate the factor matrix. The oblimin rotation is an oblique – rather than orthogonal – rotation and is selected here since it is best suited when underlying dimensions are assumed to be correlated (Hair et al., 2006).</p>
<p>The <code>fa()</code> (factor analysis) function from the <code>psych</code> package can be used for the implementation in R. Based on the scree on PCA plots, we will specify three factors for this analysis. Note that the oblimin rotation is the default for factor analysis, while a varimax (orthogonal) rotation is the default for PCA. Many other rotations can be implemented based on the nature of data and <span class="math inline">\(n\)</span>-count.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="unsup-lrn.html#cb418-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Principal axis factoring using 3 factors and oblimin rotation</span></span>
<span id="cb418-2"><a href="unsup-lrn.html#cb418-2" aria-hidden="true" tabindex="-1"></a>efa.fit <span class="ot">&lt;-</span> psych<span class="sc">::</span><span class="fu">fa</span>(survey_dat, <span class="at">nfactors =</span> <span class="dv">3</span>, <span class="at">rotate =</span> <span class="st">&#39;oblimin&#39;</span>)</span>
<span id="cb418-3"><a href="unsup-lrn.html#cb418-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb418-4"><a href="unsup-lrn.html#cb418-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display factor loadings</span></span>
<span id="cb418-5"><a href="unsup-lrn.html#cb418-5" aria-hidden="true" tabindex="-1"></a>efa.fit<span class="sc">$</span>loadings</span></code></pre></div>
<pre><code>## 
## Loadings:
##         MR1    MR2    MR3   
## belong   0.283         0.456
## effort  -0.114  0.869       
## incl                   0.747
## eng_1           0.886       
## eng_2    0.172  0.782       
## eng_3           0.799       
## happ     0.558         0.355
## psafety                0.609
## ret_1    0.791              
## ret_2    0.922        -0.111
## ret_3    0.822              
## ldrshp   0.556         0.276
## 
##                  MR1   MR2   MR3
## SS loadings    2.906 2.817 1.363
## Proportion Var 0.242 0.235 0.114
## Cumulative Var 0.242 0.477 0.590</code></pre>
<p>The sum of squared loadings (<code>SS loadings</code>) are the eigenvalues for each factor. It is also helpful to review the percent of total variance explained by each factor (<code>Proportion Var</code>) along with the cumulative percent of total variance (<code>Cumulative Var</code>). We can see that the three factors have <span class="math inline">\(\lambda \ge 1\)</span>, which together explain 59 percent of the total variance in the data.</p>
<p>By reviewing the factor loadings, we gain an understanding of which variables are part of each factor (i.e., highly correlated variables which cluster together). Factor loadings represent the correlation of each item with the respective factor. While there is not a consensus on thresholds, a general rule of thumb is that <em>absolute</em> factor loadings should be at least <span class="math inline">\(.5\)</span>. Items with lower factor loadings should be removed from the measurement model.</p>
<p>For the first factor <code>MR1</code>, the three retention items cluster together with happiness and leadership (after rounding). This indicates that happier employees who have more favorable perceptions of leadership are less likely to leave the organization.</p>
<p>Loadings for the second factor <code>MR2</code> indicate that the three engagement items cluster together with discretionary effort. This makes intuitive sense, as we would expect highly engaged employees to contribute higher levels of effort towards their work.</p>
<p>Loadings for the third factor <code>MR3</code> show that belonging, inclusion, and psychological safety cluster together. In other words, employees who feel a stronger sense of belonging and perceive the environment to be more inclusive tend to experience a more favorable climate with respect to psychological safety.</p>
<p>We can visualize this information using the <code>fa.diagram()</code> function from the <code>psych</code> library:</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="unsup-lrn.html#cb420-1" aria-hidden="true" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">fa.diagram</span>(efa.fit)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fct-ld-diagram"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/fct-ld-diagram-1.png" alt="Diagram showing factor loadings (correlations) for each item with the respective factor." width="100%" />
<p class="caption">
Figure 14.3: Diagram showing factor loadings (correlations) for each item with the respective factor.
</p>
</div>
</div>
<div id="confirmatory-factor-analysis-cfa" class="section level3" number="14.1.2">
<h3><span class="header-section-number">14.1.2</span> Confirmatory Factor Analysis (CFA)</h3>
<p><strong>Confirmatory factor analysis (CFA)</strong> is used to test how well data aligns with a theoretical factor structure.</p>
<p>We expect items associated with a given construct to be highly correlated with one another but relatively uncorrelated with items associated with independent constructs. Consider engagement and retention, which are two independent – yet likely correlated – constructs. If multiple items are needed to measure the theoretical dimensions of both engagement and retention, we would expect the engagement items to be more highly correlated with one another than with the retention items. Theory may suggest that retention likelihood increases as engagement increases, but there are many other factors which also drive one’s decision to leave an organization beyond engagement, so we would not expect changes in engagement levels to <em>always</em> be associated with a commensurate change in retention.</p>
<p>We can illustrate using our <code>survey_responses</code> data, which contains three items for both engagement and retention. Let’s first evaluate pairwise relationships between the items:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ggpairs-eng-ret"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/ggpairs-eng-ret-1.png" alt="Bivariate correlations and data distributions for engagement and retention survey items." width="100%" />
<p class="caption">
Figure 14.4: Bivariate correlations and data distributions for engagement and retention survey items.
</p>
</div>
<p>As expected, <code>eng_1</code>, <code>eng_2</code>, and <code>eng_3</code> have stronger correlations with each other (<span class="math inline">\(r \ge .70\)</span>) than with <code>ret_1</code>, <code>ret_2</code>, or <code>ret_3</code> (<span class="math inline">\(r \le .52\)</span>).</p>
<p>CFA enables us to move beyond inter-item correlations to quantify the extent to which latent variables in our data fit an expected theoretical model. We can leverage the <code>lavaan</code> package in R to perform CFA, which is implemented via a three-step procedure:</p>
<ol style="list-style-type: decimal">
<li>Define the model.</li>
<li>Fit the model.</li>
<li>Interpret the output.</li>
</ol>
<p><strong>Step 1: Model Definition</strong></p>
<p>Step one is defining the model within a string per the syntax required by <code>lavaan</code>:</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="unsup-lrn.html#cb421-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb421-2"><a href="unsup-lrn.html#cb421-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lavaan)</span>
<span id="cb421-3"><a href="unsup-lrn.html#cb421-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb421-4"><a href="unsup-lrn.html#cb421-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Model specification; each line represents a separate latent factor</span></span>
<span id="cb421-5"><a href="unsup-lrn.html#cb421-5" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">&#39;engagement =~ eng_1 + eng_2 + eng_3</span></span>
<span id="cb421-6"><a href="unsup-lrn.html#cb421-6" aria-hidden="true" tabindex="-1"></a><span class="st">                retention =~ ret_1 + ret_2 + ret_3&#39;</span>)</span></code></pre></div>
<p><strong>Step 2: Model Fitting</strong></p>
<p>Step two is fitting the model to the data using the <code>cfa()</code> function from the <code>lavaan</code> package:</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="unsup-lrn.html#cb422-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit the model</span></span>
<span id="cb422-2"><a href="unsup-lrn.html#cb422-2" aria-hidden="true" tabindex="-1"></a>cfa.fit <span class="ot">&lt;-</span> lavaan<span class="sc">::</span><span class="fu">cfa</span>(model, <span class="at">data =</span> survey_dat)</span></code></pre></div>
<p>We can also create what is known as a <strong>path diagram</strong> to assist with understanding the CFA model. A path diagram is a symbolic visualization of the measurement model, with circles depicting latent variables (factors), rectangles representing observed indicators (survey items), and arrows indicating paths (relationships) between variables. The measurement model (CFA) together with the structural (path) model is known as <strong>structural equation modeling (SEM)</strong>; CFA is a subset of the SEM umbrella.</p>
<p>The <code>lavaanPlot()</code> package can be used to create and visualize path diagrams in R:</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="unsup-lrn.html#cb423-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb423-2"><a href="unsup-lrn.html#cb423-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lavaanPlot)</span>
<span id="cb423-3"><a href="unsup-lrn.html#cb423-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb423-4"><a href="unsup-lrn.html#cb423-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize path diagram</span></span>
<span id="cb423-5"><a href="unsup-lrn.html#cb423-5" aria-hidden="true" tabindex="-1"></a>lavaanPlot<span class="sc">::</span><span class="fu">lavaanPlot</span>(<span class="at">model =</span> cfa.fit, <span class="at">coefs =</span> <span class="cn">TRUE</span>, <span class="at">stand =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:path-diagram"></span>
<div id="htmlwidget-b7ffb5ab7faad80aaa10" style="width:100%;height:480px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-b7ffb5ab7faad80aaa10">{"x":{"diagram":" digraph plot { \n graph [ overlap = true, fontsize = 10 ] \n node [ shape = box ] \n node [shape = box] \n eng_1; eng_2; eng_3; ret_1; ret_2; ret_3 \n node [shape = oval] \n engagement; retention \n \n edge [ color = black ] \n  engagement->eng_1 [label = \"0.86\"] engagement->eng_2 [label = \"0.85\"] engagement->eng_3 [label = \"0.83\"] retention->ret_1 [label = \"0.81\"] retention->ret_2 [label = \"0.93\"] retention->ret_3 [label = \"0.82\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 14.5: Path diagram showing survey items as indicators of latent engagement and retention factors.
</p>
</div>
<p>Factor loadings are shown for each indicator of the latent variable in Figure <a href="unsup-lrn.html#fig:path-diagram">14.5</a>. All are well above the <em>absolute</em> threshold of at least <span class="math inline">\(.5\)</span>.</p>
<p><strong>Step 3: Model Interpretation</strong></p>
<p>Step three is interpreting the output of the fitted model:</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="unsup-lrn.html#cb424-1" aria-hidden="true" tabindex="-1"></a>cfa.fit <span class="ot">&lt;-</span> lavaan<span class="sc">::</span><span class="fu">cfa</span>(model, <span class="at">data =</span> survey_dat)</span>
<span id="cb424-2"><a href="unsup-lrn.html#cb424-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb424-3"><a href="unsup-lrn.html#cb424-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize the model</span></span>
<span id="cb424-4"><a href="unsup-lrn.html#cb424-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cfa.fit, <span class="at">fit.measures =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## lavaan 0.6-7 ended normally after 25 iterations
## 
##   Estimator                                         ML
##   Optimization method                           NLMINB
##   Number of free parameters                         13
##                                                       
##   Number of observations                           400
##                                                       
## Model Test User Model:
##                                                       
##   Test statistic                                35.477
##   Degrees of freedom                                 8
##   P-value (Chi-square)                           0.000
## 
## Model Test Baseline Model:
## 
##   Test statistic                              1495.174
##   Degrees of freedom                                15
##   P-value                                        0.000
## 
## User Model versus Baseline Model:
## 
##   Comparative Fit Index (CFI)                    0.981
##   Tucker-Lewis Index (TLI)                       0.965
## 
## Loglikelihood and Information Criteria:
## 
##   Loglikelihood user model (H0)              -2456.223
##   Loglikelihood unrestricted model (H1)      -2438.484
##                                                       
##   Akaike (AIC)                                4938.446
##   Bayesian (BIC)                              4990.335
##   Sample-size adjusted Bayesian (BIC)         4949.085
## 
## Root Mean Square Error of Approximation:
## 
##   RMSEA                                          0.093
##   90 Percent confidence interval - lower         0.063
##   90 Percent confidence interval - upper         0.125
##   P-value RMSEA &lt;= 0.05                          0.011
## 
## Standardized Root Mean Square Residual:
## 
##   SRMR                                           0.040
## 
## Parameter Estimates:
## 
##   Standard errors                             Standard
##   Information                                 Expected
##   Information saturated (h1) model          Structured
## 
## Latent Variables:
##                    Estimate  Std.Err  z-value  P(&gt;|z|)
##   engagement =~                                       
##     eng_1             1.000                           
##     eng_2             0.880    0.044   19.827    0.000
##     eng_3             0.963    0.050   19.355    0.000
##   retention =~                                        
##     ret_1             1.000                           
##     ret_2             0.799    0.039   20.384    0.000
##     ret_3             0.699    0.038   18.562    0.000
## 
## Covariances:
##                    Estimate  Std.Err  z-value  P(&gt;|z|)
##   engagement ~~                                       
##     retention         0.406    0.051    7.932    0.000
## 
## Variances:
##                    Estimate  Std.Err  z-value  P(&gt;|z|)
##    .eng_1             0.235    0.027    8.839    0.000
##    .eng_2             0.188    0.021    9.013    0.000
##    .eng_3             0.265    0.027    9.820    0.000
##    .ret_1             0.482    0.044   10.952    0.000
##    .ret_2             0.095    0.018    5.145    0.000
##    .ret_3             0.215    0.020   10.571    0.000
##     engagement        0.649    0.064   10.188    0.000
##     retention         0.917    0.097    9.458    0.000</code></pre>
<p>The <code>lavaan</code> package provides many fit measures, but we will focus only on the most common for evaluating how well the data fit the measurement model.</p>
<ul>
<li>Model Chi-Square (<span class="math inline">\(\chi^2\)</span>): Tests whether the covariance matrix derived from the model represents the population covariance (Test Statistic under the Model Test User Model section of the <code>lavaan</code> output)</li>
<li>Comparative Fit Index (CFI): Values range from 0 to 1, with <span class="math inline">\(CFI &gt; .95\)</span> indicating good fit</li>
<li>Tucker Lewis Index (TLI): Values range from 0 to 1, with <span class="math inline">\(TLI &gt; .95\)</span> indicating good fit</li>
<li>Root Mean Square Error of Approximation (RMSEA): Values of <span class="math inline">\(.01\)</span>, <span class="math inline">\(.05\)</span>, and <span class="math inline">\(.08\)</span> indicate excellent, good, and mediocre fit, respectively (though some texts suggest <span class="math inline">\(.10\)</span> is an adequate threshold for mediocre fit)</li>
<li>Standardized Root Mean Square Residual (SRMR): Square root of the difference between residuals of the sample covariance matrix and the hypothesized model, with <span class="math inline">\(SRMR &lt; .08\)</span> indicating good fit</li>
</ul>
<p>Given data sets are often small in people analytics, it is important to note that RMSEA often exceeds thresholds with small <span class="math inline">\(df\)</span> and <span class="math inline">\(n\)</span> – even when the model is correctly specified (Kenny, Kaniskan, &amp; McCoach, 2015). Therefore, it is important to index more on fit indices such as <span class="math inline">\(CFI\)</span> and <span class="math inline">\(LTI\)</span> in determining how well the data fit the measurement model.</p>
<p>The <span class="math inline">\(\chi^2\)</span> statistic is sometimes referred to a ‘badness of fit’ measure since rejecting the null hypothesis (<span class="math inline">\(p &lt; .05\)</span>) indicates a lack of fit. Though <span class="math inline">\(\chi^2\)</span> is significant (<span class="math inline">\(p &lt; .001\)</span>), both CFI (<span class="math inline">\(.98\)</span>) and TLI (<span class="math inline">\(.97\)</span>) are above the <span class="math inline">\(.95\)</span> threshold for good fit. In addition, <span class="math inline">\(SRMR = .04\)</span> is beneath the threshold of <span class="math inline">\(.08\)</span> and <span class="math inline">\(RMSEA = .09\)</span> is between the mediocre fit threshold range of <span class="math inline">\(.08\)</span> and <span class="math inline">\(.10\)</span>. Therefore, the indicators (survey items) in these data adequately fit the two latent constructs defined by this measurement model.</p>
<p>For more extensive coverage of SEM, Kline (2005) is an excellent resource.</p>
</div>
</div>
<div id="clustering" class="section level2" number="14.2">
<h2><span class="header-section-number">14.2</span> Clustering</h2>
<p><strong>Clustering</strong> is a ML technique that groups observations into clusters which have similar characteristics but different characteristics relative to the observations in other clusters. Clustering is similar to factor analysis in that it is also unsupervised since there is not a response variable, but it differs in that it does not seek to find a low-dimensional representation of observations that capture a large portion of variance in the data; clustering aims to find homogeneous subgroups among observations.</p>
<p>Clustering is common in marketing in which it is implemented to create customer segments with shared characteristics. By grouping customers based on attributes such as income, household size, occupation, and geography, companies can tailor marketing campaigns to each segment based on what is most likely to appeal to the unique needs of each.</p>
<p>In people analytics, clustering has important applications as well. For example, clustering can be implemented to define personas based on unique talent development needs (e.g., early tech career, newly promoted people leaders) or attrition risk (e.g, rising stars with hot skills, low performers in high churn roles, high performers in specialized roles). Data privacy regulations often prevent assigning a predictive score to individuals; therefore, grouping employees based on relative attrition risk levels can support action planning at the segment level.</p>
<p>This section will cover two popular clustering techniques: <span class="math inline">\(k\)</span><em>-means</em> and <em>hierarchical</em> clustering.</p>
<div id="k-means-clustering" class="section level3" number="14.2.1">
<h3><span class="header-section-number">14.2.1</span> <span class="math inline">\(K\)</span>-Means Clustering</h3>
<p><span class="math inline">\(\textbf K\)</span><strong>-means clustering</strong> is a simple approach to grouping observations into <span class="math inline">\(K\)</span> distinct clusters. <span class="math inline">\(K\)</span>-means clustering is implemented via a four-step process:</p>
<ol style="list-style-type: decimal">
<li>Define <span class="math inline">\(K\)</span>.</li>
<li>Randomly assign observations to one of <span class="math inline">\(K\)</span> clusters.</li>
<li>For each of the <span class="math inline">\(K\)</span> clusters, compute the cluster centroid.</li>
<li>Assign each observation to the cluster with the closet centroid (middle).</li>
</ol>
<p>To assign observations to the cluster with the nearest centroid, a distance metric needs to be selected in order to measure the distance between each observation and cluster centroids. While calculating the distance between observations in two dimensions is simple, distance in higher dimensional space is more challenging. We will focus on the most common distance metric, <strong>Euclidean distance</strong>, though there are many others (e.g., Manhattan, Jaccard, Minkowski, Cosine). The Euclidean distance between two data points is the straight line distance based on the observations’ coordinates using the <strong>Pythagorean theorem</strong>:</p>
<p><span class="math display">\[ a^2 + b^2 = c^2, \]</span></p>
<p>where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are sides of a triangle that intersect to form a right angle, and <span class="math inline">\(c\)</span> is the hypotenuse (the side opposite the right angle).</p>
<p>Let’s implement <span class="math inline">\(K\)</span>-means clustering using numeric variables in the <code>employees</code> data. Since the scale of variables matters when comparing distances between observations and cluster centers, we will first scale the variables to have <span class="math inline">\(\bar{x} = 0\)</span> and <span class="math inline">\(s = 1\)</span> in support of a consistent, apples-to-apples comparison:</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="unsup-lrn.html#cb426-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load employee data</span></span>
<span id="cb426-2"><a href="unsup-lrn.html#cb426-2" aria-hidden="true" tabindex="-1"></a>employees <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_book/master/data/employees.csv&quot;</span>)</span>
<span id="cb426-3"><a href="unsup-lrn.html#cb426-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb426-4"><a href="unsup-lrn.html#cb426-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Filter employee data to numeric variables</span></span>
<span id="cb426-5"><a href="unsup-lrn.html#cb426-5" aria-hidden="true" tabindex="-1"></a>idx <span class="ot">&lt;-</span> <span class="fu">which</span>(<span class="fu">sapply</span>(employees, is.numeric)) <span class="co"># store indices of numeric variables</span></span>
<span id="cb426-6"><a href="unsup-lrn.html#cb426-6" aria-hidden="true" tabindex="-1"></a>employees <span class="ot">&lt;-</span> employees[, idx] <span class="co"># filter df using indices</span></span>
<span id="cb426-7"><a href="unsup-lrn.html#cb426-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb426-8"><a href="unsup-lrn.html#cb426-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Drop unimportant and sparsely populated sales variables</span></span>
<span id="cb426-9"><a href="unsup-lrn.html#cb426-9" aria-hidden="true" tabindex="-1"></a>employees <span class="ot">&lt;-</span> <span class="fu">subset</span>(employees, <span class="at">select =</span> <span class="sc">-</span><span class="fu">c</span>(employee_id, standard_hrs, ytd_leads, ytd_sales))</span>
<span id="cb426-10"><a href="unsup-lrn.html#cb426-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb426-11"><a href="unsup-lrn.html#cb426-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Center and scale data</span></span>
<span id="cb426-12"><a href="unsup-lrn.html#cb426-12" aria-hidden="true" tabindex="-1"></a>employees_trans <span class="ot">&lt;-</span> <span class="fu">scale</span>(employees, <span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Next, we need to define <span class="math inline">\(K\)</span>. One way to determine the optimal number of clusters is to leverage the <code>fviz_nbclust()</code> function from the <code>factoextra</code> library to visualize the sum of squared differences between observations and cluster centers against the range of clusters:</p>
<div class="sourceCode" id="cb427"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb427-1"><a href="unsup-lrn.html#cb427-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb427-2"><a href="unsup-lrn.html#cb427-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb427-3"><a href="unsup-lrn.html#cb427-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb427-4"><a href="unsup-lrn.html#cb427-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Determine optimal number of clusters</span></span>
<span id="cb427-5"><a href="unsup-lrn.html#cb427-5" aria-hidden="true" tabindex="-1"></a>factoextra<span class="sc">::</span><span class="fu">fviz_nbclust</span>(employees_trans, kmeans, <span class="at">method =</span> <span class="st">&quot;wss&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:kmeans-elbow-plot"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/kmeans-elbow-plot-1.png" alt="Total within sum of squares (WSS) across cluster count." width="100%" />
<p class="caption">
Figure 14.6: Total within sum of squares (WSS) across cluster count.
</p>
</div>
<p>When interpreting Figure <a href="unsup-lrn.html#fig:kmeans-elbow-plot">14.6</a>, we are looking for the <em>elbow</em> which marks the inflection point at which the sum of squares begins to level off. The goal is to achieve the fewest number of clusters, optimizing for subgroups that are distinctly different <em>between</em> and highly similar <em>within</em>. The elbow indicates the optimal number of clusters, as additional clusters beyond the elbow do not offer a meaningful improvement in achieving homogeneous subgroups of the observations.</p>
<p>There is a discernible elbow at three clusters in Figure <a href="unsup-lrn.html#fig:kmeans-elbow-plot">14.6</a>. Intuitively, fewer clusters promotes action taking in people analytics since clusters need to be defined, and this becomes increasingly challenging as the number of clusters increases. With a large number of clusters, it may be difficult to meaningfully tailor career development or retention strategies, for example, to the unique needs of employees assigned to each cluster as the distinction between each subgroup becomes more opaque.</p>
<p>We can now implement <span class="math inline">\(K\)</span>-means clustering, with <span class="math inline">\(K = 3\)</span>, using the <code>kmeans()</code> function in base R:</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="unsup-lrn.html#cb428-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform K-means clustering</span></span>
<span id="cb428-2"><a href="unsup-lrn.html#cb428-2" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(employees_trans, <span class="at">centers =</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="unsup-lrn.html#cb429-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Return n-count of clusters</span></span>
<span id="cb429-2"><a href="unsup-lrn.html#cb429-2" aria-hidden="true" tabindex="-1"></a>km<span class="sc">$</span>size</span></code></pre></div>
<pre><code>## [1] 592 603 275</code></pre>
<p>Of the 1,470 employees in our <code>employees</code> data, the <span class="math inline">\(n\)</span> distribution across the <span class="math inline">\(K = 3\)</span> clusters is 592, 603, and 275.</p>
<p>We can calculate the mean (and other descriptives) for each variable by cluster to better understand cluster distinctions:</p>
<div class="sourceCode" id="cb431"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb431-1"><a href="unsup-lrn.html#cb431-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean of each cluster using original data</span></span>
<span id="cb431-2"><a href="unsup-lrn.html#cb431-2" aria-hidden="true" tabindex="-1"></a><span class="fu">aggregate</span>(employees, <span class="at">by =</span> <span class="fu">list</span>(<span class="at">cluster =</span> km<span class="sc">$</span>cluster), mean)</span></code></pre></div>
<pre><code>##   cluster stock_opt_lvl trainings      age commute_dist   ed_lvl engagement
## 1       1     0.7787162  2.805743 35.31757     8.907095 2.880068   2.685811
## 2       2     0.7860697  2.781095 35.38474     9.646766 2.854063   2.792703
## 3       3     0.8436364  2.825455 43.75636     8.810909 3.112727   2.687273
##    job_lvl hourly_rate daily_comp monthly_comp annual_comp salary_hike_pct
## 1 1.834459    47.09291   376.7432      8162.77    97953.24        15.26520
## 2 1.724710    83.51078   668.0862     14475.20   173702.42        15.24876
## 3 3.301818    67.72364   541.7891     11738.76   140865.16        15.00364
##   perf_rating prior_emplr_cnt  env_sat  job_sat  rel_sat wl_balance  work_exp
## 1    3.148649        2.668919 2.746622 2.859797 2.673986   1.824324  9.315878
## 2    3.159204        2.645108 2.666667 2.665008 2.706468   1.852405  8.684909
## 3    3.152727        2.850909 2.789091 2.585455 2.807273   1.854545 21.196364
##   org_tenure job_tenure last_promo mgr_tenure interview_rating
## 1   5.146959   3.452703   1.467905   3.300676         3.979730
## 2   4.552239   2.827529   1.077944   2.878939         3.952570
## 3  16.527273   8.974545   6.170909   8.621818         4.090545</code></pre>
<p>We can see that relative to the first two clusters, the third cluster has – on average – an older demographic with more education and a higher job level. In addition, employees in the first cluster earn significantly lower compensation, on average, which may be correlated with categorical variables that were initially dropped such as <code>dept</code> or <code>job_title</code>.</p>
<p>We can also add a new column in the <code>employees</code> data frame with the cluster assignment from <span class="math inline">\(K\)</span>-means to facilitate further analysis:</p>
<div class="sourceCode" id="cb433"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb433-1"><a href="unsup-lrn.html#cb433-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Add cluster assignment to df</span></span>
<span id="cb433-2"><a href="unsup-lrn.html#cb433-2" aria-hidden="true" tabindex="-1"></a>employees <span class="ot">&lt;-</span> <span class="fu">cbind</span>(employees, <span class="at">km_cluster =</span> km<span class="sc">$</span>cluster)</span></code></pre></div>
<p>While <span class="math inline">\(K\)</span>-means clustering is a simple and efficient algorithm (even for large datasets), an a priori specification of <span class="math inline">\(K\)</span> is not always ideal. <span class="math inline">\(K\)</span>-means clustering will create <span class="math inline">\(K\)</span> clusters – even if they are nonsensical – so caution must be exercised. Plotting WSS against cluster count as shown in Figure <a href="unsup-lrn.html#fig:kmeans-elbow-plot">14.6</a> can be helpful in defining <span class="math inline">\(K\)</span>, but alternative clustering algorithms exist that do not require <span class="math inline">\(K\)</span> to be manually defined.</p>
</div>
<div id="hierarchical-clustering" class="section level3" number="14.2.2">
<h3><span class="header-section-number">14.2.2</span> Hierarchical Clustering</h3>
<p>Like <span class="math inline">\(K\)</span>-means clustering, <strong>hierarchical clustering</strong> seeks to group observations into clusters which have similar characteristics but different characteristics relative to the observations in other clusters. However, unlike <span class="math inline">\(K\)</span>-means, the number of clusters is not specified prior to implementing the algorithm with hierarchical clustering. The optimal number of clusters is determined using a <strong>dendrogram</strong>, which is a tree diagram visualizing the hierarchical relationships in data.</p>
<p>One key difference between <span class="math inline">\(K\)</span>-means and hierarchical clustering is that hierarchical clustering involves linkage methods to measure cluster similarity. There is not a one-size-fits-all option for linkage, as the performance of a given linkage technique can vary based on the structure of the data. Outlined below are the five most common types of linkage in hierarchical clustering:</p>
<ol style="list-style-type: decimal">
<li>Complete Linkage: distance between two clusters is defined as the maximum distance between any individual data point in cluster <span class="math inline">\(A\)</span> and any individual data point in cluster <span class="math inline">\(B\)</span></li>
<li>Single Linkage: distance between two clusters is defined as the minimum distance between any individual data point in cluster <span class="math inline">\(A\)</span> and any individual data point in cluster <span class="math inline">\(B\)</span></li>
<li>Average Linkage: distance between two clusters is defined as the average distance between data points in cluster <span class="math inline">\(A\)</span> and data points in cluster <span class="math inline">\(B\)</span></li>
<li>Centroid Method: distance between two clusters is defined as the distance between the centroid of cluster <span class="math inline">\(A\)</span> and the centroid of cluster <span class="math inline">\(B\)</span></li>
<li>Ward’s Method: ANOVA-based approach in which the distance between clusters <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is based on how the sum of squared distances increases when the clusters are merged</li>
</ol>
<p>To implement hierarchical clustering, we will leverage the same centered and scaled data used for <span class="math inline">\(K\)</span>-means clustering in the prior section. Note that the <code>km_cluster</code> column was only added to the original <code>employees</code> data; if this column was present in <code>employees_trans</code>, we would need to drop it so that the hierarchical clustering algorithm is not influenced by results of another clustering technique (<span class="math inline">\(K\)</span>-means).</p>
<p>Since we do not know what linkage method will work best for these data, we will also develop a function that enables us to try a range of techniques and select the one that performs best. The <code>agnes()</code> function from the <code>cluster</code> library is used to implement hierarchical clustering:</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="unsup-lrn.html#cb434-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb434-2"><a href="unsup-lrn.html#cb434-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb434-3"><a href="unsup-lrn.html#cb434-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-4"><a href="unsup-lrn.html#cb434-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define linkage methods</span></span>
<span id="cb434-5"><a href="unsup-lrn.html#cb434-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: centroid is not available for forthcoming agnes() function</span></span>
<span id="cb434-6"><a href="unsup-lrn.html#cb434-6" aria-hidden="true" tabindex="-1"></a>methods <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;complete&quot;</span>, <span class="st">&quot;single&quot;</span>, <span class="st">&quot;average&quot;</span>, <span class="st">&quot;ward&quot;</span>)</span>
<span id="cb434-7"><a href="unsup-lrn.html#cb434-7" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(methods) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;complete&quot;</span>, <span class="st">&quot;single&quot;</span>, <span class="st">&quot;average&quot;</span>, <span class="st">&quot;ward&quot;</span>)</span>
<span id="cb434-8"><a href="unsup-lrn.html#cb434-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-9"><a href="unsup-lrn.html#cb434-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create function to compute agglomerative coefficient</span></span>
<span id="cb434-10"><a href="unsup-lrn.html#cb434-10" aria-hidden="true" tabindex="-1"></a>agg_coeff <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {</span>
<span id="cb434-11"><a href="unsup-lrn.html#cb434-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-12"><a href="unsup-lrn.html#cb434-12" aria-hidden="true" tabindex="-1"></a>  cluster<span class="sc">::</span><span class="fu">agnes</span>(employees_trans, <span class="at">method =</span> x)<span class="sc">$</span>ac</span>
<span id="cb434-13"><a href="unsup-lrn.html#cb434-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb434-14"><a href="unsup-lrn.html#cb434-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb434-15"><a href="unsup-lrn.html#cb434-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute agglomerative coefficient for each linkage method</span></span>
<span id="cb434-16"><a href="unsup-lrn.html#cb434-16" aria-hidden="true" tabindex="-1"></a><span class="fu">sapply</span>(methods, agg_coeff)</span></code></pre></div>
<pre><code>##  complete    single   average      ward 
## 0.7990373 0.6248688 0.7582732 0.9571736</code></pre>
<p>Agglomerative coefficients closer to 1 indicate stronger clustering performance. Therefore, Ward’s distance measure performs best on these data, so we will implement hierarchical clustering using this linkage option.</p>
<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb436-1"><a href="unsup-lrn.html#cb436-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform hierarchical clustering using Ward&#39;s linkage method</span></span>
<span id="cb436-2"><a href="unsup-lrn.html#cb436-2" aria-hidden="true" tabindex="-1"></a>hclust <span class="ot">&lt;-</span> cluster<span class="sc">::</span><span class="fu">agnes</span>(employees_trans, <span class="at">method =</span> <span class="st">&quot;ward&quot;</span>)</span></code></pre></div>
<p>To produce a dendrogram, the <code>pltree()</code> function from the <code>cluster</code> library can be used in conjunction with the <code>hclust</code> object holding the clustering results:</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="unsup-lrn.html#cb437-1" aria-hidden="true" tabindex="-1"></a>cluster<span class="sc">::</span><span class="fu">pltree</span>(hclust, <span class="at">main =</span> <span class="st">&quot;Dendrogram&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hclust-dendrogram"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/hclust-dendrogram-1.png" alt="Dendrogram for hierarchical clustering of employees using Ward linkage method." width="100%" />
<p class="caption">
Figure 14.7: Dendrogram for hierarchical clustering of employees using Ward linkage method.
</p>
</div>
<p>At the bottom of the denogram shown in Figure <a href="unsup-lrn.html#fig:hclust-dendrogram">14.7</a>, each leaf of the tree represents an individual observation. Since <span class="math inline">\(n = 1,470\)</span>, the bottom of the tree is too congested to interpret. As we move up the tree, individual observations are fused together based on the degree of similarity as defined by Ward’s linkage method.</p>
<p>To aid in determining the optimal number of clusters, a <strong>gap statistic</strong> can be calculated, which compares the within-cluster variation for different <span class="math inline">\(K\)</span> values to reference values for a random uniform distribution with no clustering. We will use the <code>clusGap()</code> function from the <code>cluster</code> library to calculate the gap statistic, and then visualize using the <code>fviz_gap_stat()</code> function from the <code>factoextra</code> library:</p>
<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb438-1"><a href="unsup-lrn.html#cb438-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate gap statistic across 1-10 clusters</span></span>
<span id="cb438-2"><a href="unsup-lrn.html#cb438-2" aria-hidden="true" tabindex="-1"></a>gap_stat <span class="ot">&lt;-</span> cluster<span class="sc">::</span><span class="fu">clusGap</span>(employees_trans, <span class="at">FUN =</span> hcut, <span class="at">nstart =</span> <span class="dv">25</span>, <span class="at">K.max =</span> <span class="dv">10</span>, <span class="at">B =</span> <span class="dv">50</span>)</span>
<span id="cb438-3"><a href="unsup-lrn.html#cb438-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb438-4"><a href="unsup-lrn.html#cb438-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate plot of gap statistic against cluster count</span></span>
<span id="cb438-5"><a href="unsup-lrn.html#cb438-5" aria-hidden="true" tabindex="-1"></a>factoextra<span class="sc">::</span><span class="fu">fviz_gap_stat</span>(gap_stat)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hclust-gap-stat"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/hclust-gap-stat-1.png" alt="Plot of gap statistic against cluster count for hierarchical clustering." width="100%" />
<p class="caption">
Figure 14.8: Plot of gap statistic against cluster count for hierarchical clustering.
</p>
</div>
<p>We ideally want to select the value of <span class="math inline">\(K\)</span> that maximizes the gap statistic. In practice, however, balancing cluster parsimony with maximization of the gap statistic is not always straightforward. Figure <a href="unsup-lrn.html#fig:hclust-gap-stat">14.8</a> indicates that the gap statistic increase is fairly constant across the range of <span class="math inline">\(K = 1\)</span> to <span class="math inline">\(K = 10\)</span> clusters. In this case, we may look to select a value of <span class="math inline">\(K\)</span> based on an inflection point at which the trajectory of increase in the gap statistic begins to slow. Based on this approach, we may select <span class="math inline">\(K = 7\)</span>.</p>
<p>We can now cut the dendrogram into 7 clusters using the <code>cutree()</code> function, and then append the cluster to each observation in our original <code>employees</code> data:</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="unsup-lrn.html#cb439-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute distance matrix</span></span>
<span id="cb439-2"><a href="unsup-lrn.html#cb439-2" aria-hidden="true" tabindex="-1"></a>d_matrix <span class="ot">&lt;-</span> <span class="fu">dist</span>(employees_trans, <span class="at">method =</span> <span class="st">&quot;euclidean&quot;</span>)</span>
<span id="cb439-3"><a href="unsup-lrn.html#cb439-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb439-4"><a href="unsup-lrn.html#cb439-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform hierarchical clustering using Ward&#39;s method</span></span>
<span id="cb439-5"><a href="unsup-lrn.html#cb439-5" aria-hidden="true" tabindex="-1"></a>hclust_final <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d_matrix, <span class="at">method =</span> <span class="st">&quot;ward.D2&quot;</span> )</span>
<span id="cb439-6"><a href="unsup-lrn.html#cb439-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb439-7"><a href="unsup-lrn.html#cb439-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Cut the dendrogram into 7 clusters</span></span>
<span id="cb439-8"><a href="unsup-lrn.html#cb439-8" aria-hidden="true" tabindex="-1"></a>groups <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hclust_final, <span class="at">k =</span> <span class="dv">7</span>)</span>
<span id="cb439-9"><a href="unsup-lrn.html#cb439-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb439-10"><a href="unsup-lrn.html#cb439-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Append cluster labels to original data</span></span>
<span id="cb439-11"><a href="unsup-lrn.html#cb439-11" aria-hidden="true" tabindex="-1"></a>employees <span class="ot">&lt;-</span> <span class="fu">cbind</span>(employees, <span class="at">hier_cluster =</span> groups)</span></code></pre></div>
</div>
</div>
<div id="review-questions-12" class="section level2" number="14.3">
<h2><span class="header-section-number">14.3</span> Review Questions</h2>
<ol style="list-style-type: decimal">
<li><p>How can high dimensional data create problems in analytics, and how do dimension reduction techniques remediate these issues?</p></li>
<li><p>What is the difference between Exploratory Factor Analysis (EFA) and Confirmatory Factor Analysis (CFA)?</p></li>
<li><p>What is the difference between Exploratory Factor Analysis (EFA) and Principal Components Analysis (PCA)?</p></li>
<li><p>What is Structural Equation Modeling (SEM), and what are some use cases for it in people analytics?</p></li>
<li><p>How can we test whether data satisfy the eligibility criteria for factor analysis?</p></li>
<li><p>How are factor loadings interpreted to ascertain which variables are members of each factor?</p></li>
<li><p>What is a data-informed approach to selecting the optimal value of <span class="math inline">\(K\)</span> in <span class="math inline">\(K\)</span>-means clustering?</p></li>
<li><p>What is Euclidean distance, and what is its function in clustering?</p></li>
<li><p>How is a dendrogram interpreted in the context of hierarchical clustering?</p></li>
<li><p>When optimizing for both cluster parsimony and gap statistic maximization is not feasible, how can the optimal value of <span class="math inline">\(K\)</span> be determined in hierarchical clustering?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pred-mod.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-viz.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["The_Fundamentals_of_People_Analytics.pdf", "The_Fundamentals_of_People_Analytics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
