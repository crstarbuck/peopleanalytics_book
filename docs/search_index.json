[["dedication.html", "The People Analytics Companion An Applied Guide through the People Analytics Lifecycle Dedication", " The People Analytics Companion An Applied Guide through the People Analytics Lifecycle Craig Starbuck 2022-05-13 Dedication "],["preface.html", "Preface", " Preface Twenty years ago, I was the least likely person to write this book. My first statistics course in college was dreadful. On day one, my professor entered our large lecture hall of about 100 students and shared some grim stats: “Based on historical data, half of you won’t make it to the midterm and of those who do, half won’t receive a passing grade in the end.” This was both discouraging and motivating. Stats was a required course for my major so failure wasn’t an option; I had to pass. The course was challenging, and I attended weekly study sessions with classmates and studied a lot independently to learn the material. I saw no applications for statistics to anything I planned to do with my degree, so the course was reduced to memorization of equations; it was not enjoyable. I passed the course with a B, and I was determined to never open another stats book. You may be wondering what changed to motivate authoring a book involving this insufferable subject. The short answer is that I discovered the very important applications to a discipline I truly love, people analytics. The practical applications were altogether absent from my undergraduate statistics course. As I began to think about complex and nuanced challenges in social science contexts, it became clear that I would not only need to reengage with stats; I would need to develop an authentic appreciation for the discipline. Over the past decade, I have taken the journey of ‘relearning’ statistics and developing a deep understanding of how statistical methodologies can be applied to various organizational problem statements. My purpose in writing this book is to help make this content – which may unfortunately be intimidating to many – both accessible and exciting. In addition to my role in people analytics, I have taught a graduate-level business analytics course for Finance and MBA students for many years and have developed several teaching strategies through this experience that I will apply in this book. Beyond these instructional methods, this book makes a unique contribution in curating what I consider to be the most salient topics for people analytics practitioners. There are many texts available for deeper treatments of individual subjects covered in this book but as of this writing, I have found none that organize within a single text both theoretical and applied instruction spanning the whole of the people analytics lifecycle. Thus, this book represents my earnest attempt to provide a concise – yet adequately comprehensive – treatment of the concepts and methods I’ve found to be most important for people analytics. My hope is that this book will ignite within you the same passion for analytics I have discovered over the past decade. Craig Starbuck December 2021 Craig Starbuck, PhD is the CEO and Co-Founder of OrgAcuity, a tech company with a mission to democratize access to people analytics. Craig has built and led people analytics teams at companies such as Robinhood, Mastercard, Equifax, TD Ameritrade, and Scottrade, and he also spent a decade in various data engineering and analytics positions in the banking and health care industries. He is a Member of the Society for Industrial and Organizational Psychology (SIOP) and has a passion for transforming people data into information and insights that help organizations enhance the experience and wellbeing of employees. "],["foreword.html", "Foreword", " Foreword "],["getting-started.html", "1 Getting Started 1.1 Guiding Principles 1.2 Tools 1.3 4D Framework", " 1 Getting Started Nothing in this book will increase the value of an analysis no one needs. Analyses should always have a strong value proposition – a clear expectation of how an analysis will support a General Manager, People Partner, Salesperson, or other member of the organization. Curiosity is not a business reason, and this chapter will cover a set of guiding principles as well as a project management framework that will help ensure analytics are anchored in well-defined problem statements to increase the likelihood of meaningful ROI. In addition, this book will cut through the fluff and teach you how to do stuff that matters. Knowledge of concepts is futile without an understanding of how to apply them to people analytics use cases. Whether you are a people leader, individual contributor, or aspiring analytics practitioner, this book is for you. This book will serve as a guide through the analytics lifecycle, curating the key concepts and applications germane to common questions and hypotheses within people analytics. 1.1 Guiding Principles Among the many principles guiding how analytics teams operate, there are three that I have found to be universally applicable and critical to the success of an analytics capability. 1.1.1 Pro Employee Thinking “With Great Power Comes Great Responsibility.” ‘Pro employee’ thinking is addressed first and for good reason. People analytics has the power to improve the lives of people in meaningful ways. Whether we are shedding light on an area of the business struggling with work-life balance or identifying developmental areas of which a group of leaders may be unaware, people analytics ideally improves employee well-being and effectively, the success of the business. It is important to embrace a ‘pro employee’ philosophy, as newfound knowledge could also have damaging repercussions if shared with the wrong people or if findings are disseminated without proper instruction on how to interpret and act. One way to error on the side of caution when considering whether to disseminate insights is to ask the following: “With this knowledge, could the recipient act in a manner that is inconsistent with our ‘pro employee’ philosophy?” If the answer to this question is not a clear “no”, discuss with your HR, legal, and privacy partners and together, determine how best to proceed. The decision may be to not share the findings with the intended audience at all or to develop a proper communication and training plan to ensure there is consistency in how recipients interpret the insights and act in response. Employment Law and Data Privacy Counsel are our friends, and it is important to build strong relationships with these critical partners. 1.1.2 Quality “Garbage In, Garbage Out.” Never compromise quality for greater velocity. If quality falls to the bottom of the priority list, all other efforts are pointless. It is unlikely that requestors of data and analytics will ever ask us to take longer to prepare the information. The onus is on us as analytics professionals to level set on a reasonable timeline for analyses based on many factors that can materially impact the quality of analyses and insights. A single instance of compromised quality can have lasting damage on the reputation of the analytics function and cause consumers of insights to view all findings as suspect. Be sure quality is consistently a top value and guard your team’s reputation at all costs. If stakeholders lose trust, there will likely be additional data requests for validation; this is wasteful to both you and your user community and detracts from the bigger story that needs to be conveyed. To be clear, by ‘quality’ I am referring to results, which is dependent on data integrity in the source systems, proper data preparation steps, and many other factors. Most of the analyst’s time is spent on data preparation (data collection, cleaning and organizing, building training sets, mining for patterns, refining algorithms, etc.). If tight controls do not exist within the source application to support data integrity, data preparation efforts can only go so far in delivering reliable and valid findings. It is often the analysts who identify data integrity issues due to the nature of their work; therefore, close relationships should be formed with source application owners to put into place validation rules to proactively prevent the entry of erroneous data or at the very least, exception/audit reports to identify and address the issues soon after the fact. 1.1.3 Prioritization “If everything is a priority, nothing is a priority.” If there is not a supply-demand gap on the analytics team, the team likely isn’t asking enough questions. A backlog of projects can lend support for business cases requesting incremental funding to accelerate and expand the impact of people analytics. It is crucial to be relentless about prioritizing strategically important projects with ‘measurable’ impact over merely interesting questions that few care to answer. According to the Pareto Principle, 80% of outcomes (or outputs) result from 20% of causes (or inputs). In analytics, it is important to be laser focused on identifying the 20% of inputs that will result in disproportionate value creation for stakeholders. There are some general customer-oriented questions I have found to be helpful for the intake process to optimize the allocation of time and resources: Does this support a company or departmental objective? If not, why should this be prioritized over something else? Who is the executive sponsor? Really important projects will have an executive-level sponsor. What quantitative and/or qualitative data can be provided as a rationale for this request? Is there data to support doing this, or is the problem statement rooted merely in theories and anecdotes? Will this mitigate risk or enable opportunities? What actions can or will be taken as a result of this analysis? What is the scale of impact (# of impacted people)? What is the depth of impact (minimum –&gt; significant)? Is this a dependency or blocker for another important deliverable? What is the impact of not doing (or delaying) this? What is the request date? Is there flexibility in this date and/or scope of the request (e.g., what does MVP look like)? These questions can be weighted and scored as well to support a more automated and algorithmic approach to prioritization. 1.2 Tools This book uses freely available software for statistics, modeling, and data visualization. All code has been written such that it is fully reproducible should you choose to follow along on your machine (and I highly recommend you do). 1.2.1 R While there are many commercial-grade analytics toolsets, R is open-sourced statistical and data visualization software that can be downloaded free of charge. It is incredibly powerful, and there is a package (or at least the ability to easily create one) for every conceivable statistical technique and data visualization. It is also widely used in highly regulated environments. As of this writing, R Markdown – the dynamic document creator in which I am writing this book – allows for coding in 56 different languages! Therefore, the debate around whether to use Python, Julia, or something else is now moot; we need not sacrifice the advantages of other languages by choosing one. Please note that while R basics are covered, this is not a book on how to code. It is assumed that you already understand programming fundamentals. If this is not the case, an introductory programming course is highly recommended; this is one of the best investments you can make for a successful career in analytics. The ability to write code is now table stakes for anyone in an analytics-oriented field, as this is the best way to develop reproducible analyses. Coding is to analytics professionals what typing was for Baby Boomers decades ago; a lack of coding proficiency is a major limiting factor on one’s potential in this field. The goal of the code provided in this book is not to represent the most performant, succinct, or productionalizable approaches. The code herein is intended only to facilitate understanding and demonstrate how concepts can be implemented in people analytics settings. Programming expertise is important for optimizing these approaches for production applications. 1.2.2 Data The primary data set used in this book is employees, which contains information on active and terminated employees. Fields are defined in the data dictionary provided below: employee_id: Unique identifier for each employee active: Flag set to Yes for active employees and No for inactive employees stock_opt_lvl: Stock option level trainings: Number of trainings completed within the past year age: Employee age in years commute_dist: Commute distance in miles ed_lvl: Education level, where 1 = High School, 2 = Associate Degree, 3 = Bachelor’s Degree, 4 = Master’s Degree, and 5 = Doctoral Degree ed_field: Education field associated with most recent degree gender: Gender self-identification marital_sts: Marital status dept: Department of which an employee is a member engagement: Employee engagement score measured on a 4-point Likert scale, where 1 = Highly Disengaged and 4 = Highly Engaged job_lvl: Job level, where 1 = Junior and 5 = Senior job_title: Job title overtime: Flag set to Yes if the employee is nonexempt and works overtime and No if the employee does not work overtime business_travel: Business travel frequency hourly_rate: Hourly rate calculated irrespective of hourly/salaried employees daily_comp: Hourly rate * 8 monthly_comp: Hourly rate * 2080 / 12 annual_comp: Hourly rate * 2080 ytd_leads: Year-to-date (YTD) number of leads generated for employees in Sales Executive and Sales Representative positions ytd_sales: Year-to-date (YTD) sales measured in USD for employees in Sales Executive and Sales Representative positions standard_hrs: Expected working hours over a two-week payroll cycle salary_hike_pct: The percent increase in salary for the employee’s most recent compensation adjustment (whether due to a standard merit increase, off-cycle adjustment, or promotion) perf_rating: Most recent performance rating, where 1 = Needs Improvement, 2 = Core Contributor, 3 = Noteworthy, and 4 = Exceptional prior_emplr_cnt: Number of prior employers env_sat: Environment satisfaction score measured on a 4-point Likert scale, where 1 = Highly Dissatisfied and 4 = Highly Satisfied job_sat: Job satisfaction score measured on a 4-point Likert scale, where 1 = Highly Dissatisfied and 4 = Highly Satisfied rel_sat: Colleague relationship satisfaction score measured on a 4-point Likert scale, where 1 = Highly Dissatisfied and 4 = Highly Satisfied wl_balance: Work-life balance score measured on a 4-point Likert scale, where 1 = Poor Balance and 4 = Excellent Balance work_exp: Total years of work experience org_tenure: Years at current company job_tenure: Years in current job last_promo: Years since last promotion mgr_tenure: Years under current manager 1.3 4D Framework In practical analytics settings, we generally operate with respect to five primary constraints: timeliness, client expectation, accuracy, reliability, and cost (Bartlett, 2013). Adherence to a lightweight framework over hastily rushing into an analysis full of assumptions generally lends to better outcomes that respect these constraints. A framework ensures (a) the problem statement is understood and well-defined; (b) relevant literature and prior research are reviewed; (c) the measurement strategy is sound; (d) the analysis approach is suitable for the hypotheses being tested; and (e) results and conclusions are valid and communicated in a way that resonates with the target audience. This chapter will outline a recommended framework as well as other important considerations that should be reviewed early in the project. It is important to develop a clear understanding of the key elements of research. Scientific research is the systematic, controlled, empirical, and critical investigation of natural phenomena guided by theory and hypotheses about the presumed relations among such phenomena (Kerlinger &amp; Lee, 2000). In other words, research is an organized and systematic way of finding answers to questions. If you are in the business of analytics, I encourage you to think of yourself as a research scientist – regardless of whether you are wearing a lab coat or have plans to publish. As we will discover when exploring the laws of probability in a later chapter, there is a 1 in 20 chance of finding a significant result when none exists. Therefore, it is important to remain disciplined and methodical to protect against backward research wherein the researcher mines data for interesting relationships or differences and then develops hypotheses which they know the data support. There have been many examples of bad research over the years, which often presents in the form of p-hacking or data dredging: the act of finding data to confirm what the researcher wants to prove. This can occur by running an exhaustive number of experiments to find one that supports the hypothesis, or by using only a subset of data which features the expected patterning. Academics at elite research institutions are often under immense pressure to publish in top-tier journals which have a track record of accepting new ground-breaking research over replication studies or unsupported hypotheses, and incentives have unfortunately influenced some to compromise integrity. As my PhD advisor told me many years ago, an unsupported hypothesis – while initially disappointing given the exhaustive literature review that precedes its development – is a meaningful empirical contribution given theory suggests the opposite should be true. If you participated in a science fair as a child, you are likely already familiar with the scientific method. The scientific method is the standard scheme of organized and systematic inquiry, and this duly applies to people analytics practitioners in the promotion of robust analyses and recommendations. Figure 1.1: The Scientific Method An important feature of the Scientific Method, as reflected in Figure 1.1, is that the process is akin to an infinite loop in coding; that is, it never ends. New knowledge resulting from hypothesis testing prompts a new set of questions and hypotheses, which initiates a new lifecycle of scientific inquiry. Over the years, I have adapted the scientific method into a curtailed four-step framework to help ensure a rigorous and disciplined approach is applied to the end-to-end analytical process. The four steps are (a) Discover, (b) Design, (c) Develop, and (d) Deliver. Figure 1.2: 4D Framework 1.3.1 Discover You are likely familiar with the following adage: “An ounce of prevention is worth a pound of cure.” Such is the case with respect to planning in an analytics context. During the Discover phase, it is important to remain in the problem zone; seek to understand your clients’ needs through active listening and questions. This is not the time for solutioning or committing to any specific deliverables or timelines. If the client’s needs are ambiguous, proceeding without clarity is unlikely to result in a favorable outcome. It is generally helpful to think about analytics solutions – whether a dashboard with basic metrics and trends or an advanced analysis – like a Product Owner thinks about the initial and subsequent releases of a commercial product. A Minimum Viable Product (MVP) is a version of the solution with the minimum number of features to be useful to early customers who can provide feedback for future enhancements. It is important to clarify that the MVP version of solutions often has both a limited number of users and features, which protects against the tendency to boil the ocean by striving to address every question for every stakeholder. Breaking down large projects into small sets of features that are easier to communicate and adopt provides space for agility and real-time adjustments to the product roadmap per user feedback. There are some initial questions and considerations that will help frame an analytics project and support its success. Client Who is the client? A client can be a person or organization contracting you for consulting services, or an internal stakeholder within your organization who has need. What is important to them? Primary Objective What is the client ultimately hoping to accomplish? Is the request merely to satisfy one’s curiosity, or are there actions that can realistically be taken to materially influence said objective? Problem Statement One of the most important early steps is clearly defining the problem statement. If your understanding of the problem – after translating from the business terms in which it was likely initially expressed – is misaligned with the client’s needs, none of the subsequent steps matter. Guiding Theories What theoretical explanations can the client offer as potential rationalizations for the phenomena of interest? Are there existing theories in the organizational literature that should guide how the problem is tackled (e.g., findings from similar research implemented in other contexts)? Research Questions To respect the nuances of the problem statement, it is important to unpack it and frame as a set of overarching questions to guide the research. Q1: … Q2: … Q3: … Research Hypotheses Once research questions are developed, what do you expect to find based on anecdotal stories or empirical findings? As a next step, these expectations should be expressed in the form of research hypotheses. Please note that these research hypotheses are different from statistical hypotheses (both will be covered in detail in later chapters). H1: … H2: … H3: … To ensure the hypotheses lend to actionable analyses, it is important to consider the following: “What does success look like?” In other words, once the project is complete, against which success measures will the project’s success be determined? Curiosity is not a business reason and hope is not a reasonable strategy. The following questions may prove helpful in the promotion of actionable – over merely interesting – outcomes: What will be done if the hypotheses are empirically supported? What will be done if the hypotheses are not empirically supported? Assumptions At this point, it’s helpful to consider what assumptions may be embedded in this discovery work. Are the questions and hypotheses rooted in what the client has theorized, or are these the product of an ambiguous understanding of the client’s needs? Cadence Is this analysis a one-off, or could there be a need to refresh this analysis on a regular cadence? Are there dates associated with programs or actions this analysis is intended to support? Aggregation Is there a need for individual-level detail supporting the analysis? Aaggregate data should generally be the default unless a compelling justification exists and approval from legal and privacy partners is granted. One important role of analysts is to help keep the audience focused on the bigger picture and findings. Access to individual-level detail can not only introduce unnecessary legal and compliance risk but can also lead to questions and probing that can delay taking needed actions based on the results. Deliverable What is the preferred method of communicating the results of the analysis (e.g., interactive dashboard, static slide deck, document)? It is important to determine this early so that subsequent efforts can be structured to support the preferred deliverable. For example, if an interactive dashboard is preferred, does your Engineering department need to prioritize dependent tasks such as data pipelines, row-level security, BI development, and migrations to production servers? Filters &amp; Dimensions How does your client prefer to segment the workforce? Some common grouping dimensions are business unit, division, team, job family, location, tenure, and management level, but the client may have custom segmentation requirements that will be important to identify and define early in the project. 1.3.2 Design Perhaps the most important initial question to answer in the design phase is: “Does anything already exist that addresses part, or all, of the client’s objectives?” If an existing solution will suffice, or a previous analysis can be easily refreshed with recent data, it may be possible to allocate time and resources elsewhere. If related or complimentary analyses have already been performed, they may accelerate new analyses. The end-user experience is of paramount importance during the Design phase, as solutions should have a consistent look and feel regardless of who developed them. Defining and implementing design guidelines will ensure consistency across analytics projects, as well as within large projects in which multiple analysts are collaborating on various elements of the solution. Data Privacy Are there potential concerns with the study’s objective, planned actions, and/or requested data elements from an employee privacy or legal perspective? A cross-functional data governance committee can help with efficient and consistent decisioning on requests for people data and analytics. In cases where sensitive attributes such as gender, ethnicity, age, sexual orientation, and disability status are requested, it’s always best to exercise a ‘safety first’ mentality and consult with legal and privacy partners to ensure there is comfort with the intended use of the data. The decision on whether or not to include these sensitive data elements is often less about what the audience can view (e.g., People Partners may already have access to the information at the person level in the source system) and more anchored in what they plan to do with the information. Data Sources &amp; Elements Is the required data already accessible in a data warehouse or other analytics environment? If not, does it need to be? What is required to achieve this? What data sources are required? What data elements are required? Data Quality It is important to understand the data generative process and never make assumptions about how anomalies or missing data should be interpreted. After identifying what data sources will be required for a particular analysis, it is important to meet with source system owners and data stewards to deeply understand the business processes by which data are generated in the system(s). Are there data quality concerns that need to be explored and addressed? Variables How will the constructs be measured (e.g., survey instrument, derived attribute, calculated field)? Analysis Method What are the appropriate analysis methods based on the research hypotheses? If modeling is required, is it more important to index on accuracy or interpretability? Dependencies Are other teams required to develop this solution? What is the nature of the work each dependent team will perform? Are there required system configuration changes? Do these teams have capacity to support? Change Management Will this solution impact current processes or solutions? If so, what is the change management plan to facilitate a seamless transition and user experience? Sign-Off Generally, it is best for the client to signoff on the problem statement, analysis approach, and wire frame for the deliverable (if applicable) before providing an ETA and proceeding to the development phase. This ensures alignment on the client’s needs and the perceived utility of the solution in addressing those needs. 1.3.3 Develop While development patterns can vary widely across analytics teams, establishing a set of standards can pay dividends in the form of greater efficiency and reliability over time. Pattern-based development ensures analysts who were not involved in a particular project can access the code and easily and quickly understand each step of the analysis: data extraction –&gt; wrangling –&gt; cleaning –&gt; analysis –&gt; visualization. This is one of the many reasons tools like Excel will not be covered in this book. Software like R and Python allows analysts to organize and annotate steps of the analytical process in a manner that is both logical and reproducible. In case it bears repeating, learning to code is likely the best investment one can make in the pursuit of a career in analytics. Development Patterns Are there development patterns that should guide the development approach to support consistency? Are there existing calculated fields that can/should be leveraged for derived data? Are there best practices that should be employed to optimize performance (e.g., load time for dashboards, executing complex queries during non-peak times)? Are there standard color palettes that should be applied? Productionalizable Code How do models and data science pipelines need to be developed to facilitate a seamless migration from lower to upper environments? For example, initial exploratory data analysis (EDA) may be performed using curated data in flat files for the purpose of identifying meaningful trends, relationships, and differences, but where will this data need to be sourced in production to automate the refresh of models at a regular interval? If the data were provided from multiple source systems, what joins are required to integrate the data? What transformation logic or business rules need to be applied to reproduce the curated data? Unit Testing What test cases will ensure the veracity of data? Who will perform the testing? UAT Testing In the spirit of agility and constant contact with the client to prevent surprises, it is generally a good idea to have the client take the solution for a test run within the UAT environment and then provide sign-off before migrating to production. If the deliverable is a deck or doc with results from a model, UAT may surface clarifying questions that can be addressed before releasing to the broader audience. 1.3.4 Deliver The Deliver phase can take many forms depending on the solution being released. If the solution is designed for a large user base, a series of recorded trainings may be in order so that there is a helpful reference for those unable to attend the live sessions or new joiners in the future. It is important to monitor success measures, which could be insights aligned to research hypotheses, dashboard utilization metrics, progress following data-informed interventions, or any number of others defined within the Discover phase. "],["r-intro.html", "2 Introduction to R 2.1 Getting Started 2.2 Vectors 2.3 Matrices 2.4 Factors 2.5 Data Frames 2.6 Lists 2.7 Loops 2.8 User-Defined Functions (UDFs) 2.9 Graphics 2.10 Review Questions", " 2 Introduction to R This chapter covers how to install R, R Studio, and required packages for replicating examples in this book. This chapter also covers R basics such as objects, data structures, and data types that are fundamental to working in R. In addition, many common functions will be covered in this chapter, and many more will be introduced throughout later chapters. 2.1 Getting Started Run and test your code frequently. Writing a significant number of lines before testing will make debugging your code far more difficult and time-intensive than it needs to be. Installing R R can be compiled and ran on a variety of platforms including UNIX, Windows, and MacOS. R can be downloaded here: https://www.r-project.org/ When installing R, you will need to select a CRAN mirror. The Comprehensive R Archive Network (CRAN) is a network of servers around the world that store identical, current versions of code and documentation for R. You should select the CRAN mirror nearest you to minimize network load. Installing R Studio R Studio is an Integrated Development Environment (IDE) for R. R Studio provides a console with syntax editing that is helpful for debugging code as well as tools for plotting, history, and workspace management. Both open source and commercial editions are available, but the open-source option is sufficient for replicating everything in this book. R Studio can be downloaded here: https://www.rstudio.com/products/rstudio/download/#download Installing Packages Libraries from several R packages will be utilized in this book. The line of code below can be executed within R or R Studio to install all at once: install.packages(c(&quot;tidyverse&quot;, &quot;corrplot&quot;, &quot;psych&quot;, &quot;moments&quot;, &quot;ggpubr&quot;, &quot;GGally&quot;, &quot;sqldf&quot;, &quot;caret&quot;, &quot;car&quot;, &quot;reshape2&quot;, &quot;flextable&quot;, &quot;lmtest&quot;, &quot;equatiomatic&quot;), dependencies = TRUE, repos = &quot;http://cran.us.r-project.org&quot;) ## ## There are binary versions available but the source versions are later: ## binary source needs_compilation ## texPreview 1.5 2.0.0 FALSE ## corrplot 0.89 0.92 FALSE ## psych 2.2.3 2.2.5 FALSE ## moments 0.14 0.14.1 FALSE ## caret 6.0-91 6.0-92 TRUE ## car 3.0-12 3.0-13 FALSE ## ## ## The downloaded binary packages are in ## /var/folders/b1/0nnhbsx55hvfb1b3n83x0qrw0000gn/T//RtmpfHXT67/downloaded_packages Case Sensitivity It’s important to note that everything in R is case-sensitive. When working with functions, be sure to match the case when typing the function name. For example, Mean() is not the same as mean(); since mean() is the correct case for the function, capitalized characters will result in an error when executing the function. Help Documentation for functions is available via the ? command. This can be inserted prior to any function to access the function’s documentation. For example, ?mean will display the documentation, including its required and optional arguments, for the mean() function. Figure 2.1: Documentation for Arithmetic Mean Function Objects Objects underpin just about everything we do in R. An object is a container for various types of data. Objects can take many forms, ranging from simple objects holding a single number or character to complex structures that support advanced visualizations. The assignment character &lt;- is used to assign values to an object. Let’s use the assignment operator to assign a number and character to separate objects. Note that non-numeric values must be enveloped in either single ticks '' or double quotes &quot;&quot;: obj_1 &lt;- 1 obj_1 ## [1] 1 obj_2 &lt;- &#39;a&#39; obj_2 ## [1] &quot;a&quot; Several functions are available for returning the type of data in an object: typeof(obj_2) ## [1] &quot;character&quot; class(obj_2) ## [1] &quot;character&quot; Comments The # symbol is used for commenting/annotating code. This is a best practice to aid in quickly and easily deciphering the role of each line or block of code. Without comments, troubleshooting large scripts can be a more challenging task than necessary. # Assign a new number to the object named obj_1 obj_1 &lt;- 2 # Display value in obj_1 obj_1 ## [1] 2 # Assign a new character to the object named obj_2 obj_2 &lt;- &#39;b&#39; # Display value in obj_2 obj_2 ## [1] &quot;b&quot; In-line code comments can also be added where needed to reduce the number of lines in a script: # Assign a new number to the object named obj_1 obj_1 &lt;- 3 obj_1 # Display value in obj_1 ## [1] 3 # Assign a new character to the object named obj_2 obj_2 &lt;- &#39;c&#39; obj_2 # Display value in obj_2 ## [1] &quot;c&quot; 2.2 Vectors A vector is the most basic data object in R. Vectors contain a collection of data elements of the same data type. For example, a vector may contain a series of numbers, set of characters, collection of dates, or logical TRUE or FALSE results. In this example, we introduce the combine function c(), which allows us to fill an object with more than one value: # Create and fill a numeric vector named vect_num vect_num &lt;- c(2,4,6,8,10) vect_num ## [1] 2 4 6 8 10 # Create and fill a character vector named vect_char vect_char &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;) vect_char ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; We can use the as.Date() function to convert character strings containing dates to an actual date data type. By default, anything within single ticks or double quotes is treated as a character, so we must make an explicit type conversion from characters to dates. Remember that R is case-sensitive. Therefore, as.date() is not a valid function; the D must be capitalized. # Create and fill a date vector named vect_dt vect_dt &lt;- c(as.Date(&quot;2021-01-01&quot;), as.Date(&quot;2022-01-01&quot;)) vect_dt ## [1] &quot;2021-01-01&quot; &quot;2022-01-01&quot; We can use the sequence generation function seq() to fill values between a start and end point using a specified interval. For example, we can fill vect_dt with the first day of each month between 2021-01-01 and 2022-01-01 via the seq() function and its by = &quot;months&quot; argument: # Create and fill a date vector named vect_dt vect_dt &lt;- seq(as.Date(&quot;2021-01-01&quot;), as.Date(&quot;2022-01-01&quot;), by = &#39;months&#39;) vect_dt ## [1] &quot;2021-01-01&quot; &quot;2021-02-01&quot; &quot;2021-03-01&quot; &quot;2021-04-01&quot; &quot;2021-05-01&quot; ## [6] &quot;2021-06-01&quot; &quot;2021-07-01&quot; &quot;2021-08-01&quot; &quot;2021-09-01&quot; &quot;2021-10-01&quot; ## [11] &quot;2021-11-01&quot; &quot;2021-12-01&quot; &quot;2022-01-01&quot; We can also use the : operator to fill integers between a starting and ending number: # Create and fill a numeric vector with values between 1 and 10 vect_num &lt;- 1:10 vect_num ## [1] 1 2 3 4 5 6 7 8 9 10 We can access a particular element of a vector using its index. An index represents the position in a set of elements. In R, the first element of a vector has an index of 1, and the final element of a vector has an index equal to the vector’s length. The index is specified using square brackets, such as [5] for the fifth element of a vector. # Return the value in position 5 of vect_num vect_num[5] ## [1] 5 When applied to a vector, the length() function returns the number of elements in the vector, and this can be used to dynamically return the last value of vectors. # Return the last element of vect_num vect_num[length(vect_num)] ## [1] 10 Vectorized Operations Vectorized operations (or vectorization) underpin mathematical operations in R and greatly simplify computation. For example, if we need to perform a mathematical operation to each data element in a numeric vector, we do not need to specify each and every element explicitly. We can simply apply the operation at the vector level, and the operation will be applied to each of the vector’s individual elements. # Create a numeric vector named x and fill with values between 1 and 10 x &lt;- 1:10 # Add 2 to each element of x x_plus2 &lt;- x+2 x_plus2 ## [1] 3 4 5 6 7 8 9 10 11 12 # Multiply each element of x by 2 x_times2 &lt;- x*2 x_times2 ## [1] 2 4 6 8 10 12 14 16 18 20 # Square each element of x x_sq &lt;- x^2 x_sq ## [1] 1 4 9 16 25 36 49 64 81 100 Many built-in arithmetic functions are available and compatible with vectors: # Aggregate sum of x elements sum(x) ## [1] 55 # Count of x elements length(x) ## [1] 10 # Square root of x elements sqrt(x) ## [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427 ## [9] 3.000000 3.162278 # Natural logarithm of x elements log(x) ## [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101 ## [8] 2.0794415 2.1972246 2.3025851 # Exponential of x elements exp(x) ## [1] 2.718282 7.389056 20.085537 54.598150 148.413159 ## [6] 403.428793 1096.633158 2980.957987 8103.083928 22026.465795 We can also perform various operations on multiple vectors. Vectorization will result in an implied ordering about the elements, in that the specified operation will be applied to the first elements of the vectors and then the second, then third, etc. # Create vectors x1 and x2 and fill with integers x1 &lt;- 1:10 x2 &lt;- 11:20 # Store sum of vectors to new x3 vector x3 &lt;- x1 + x2 x3 ## [1] 12 14 16 18 20 22 24 26 28 30 Using vectorization, we can also evaluate whether a specified condition is true or false for each element in a vector: # Evaluate whether each element of x is less than 6, and store results to a logical vector logical_rslts &lt;- x&lt;6 logical_rslts ## [1] TRUE TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE 2.3 Matrices A matrix is like a vector in that it represents a collection of data elements of the same data type; however, the elements of a matrix are arranged into a fixed number of rows and columns. We can create a matrix using the matrix() function. Per ?matrix, the nrow and ncol arguments can be used to organize like data elements into a specified number of rows and columns. # Create and fill matrix with numbers mtrx_num &lt;- matrix(data = 1:10, nrow = 5, ncol = 2) mtrx_num ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 We can leverage shorthand for function calls. As long as the argument values are in the correct order by the documentation, the argument names are not required. Per ?matrix, the first argument is data, followed by nrow and then ncol. Therefore, we can achieve the same result using the following: # Create and fill matrix with numbers mtrx_num &lt;- matrix(1:10, 5, 2) mtrx_num ## [,1] [,2] ## [1,] 1 6 ## [2,] 2 7 ## [3,] 3 8 ## [4,] 4 9 ## [5,] 5 10 Several functions are available to quickly retrieve the number of rows and columns in a rectangular object like a matrix: # Return the number of rows in mtrx_num nrow(mtrx_num) ## [1] 5 # Return the number of columns in mtrx_num ncol(mtrx_num) ## [1] 2 # Return the number of columns and rows in mtrx_num dim(mtrx_num) ## [1] 5 2 The head() and tail() functions return the first and last pieces of data, respectively. For large matrices (or other types of objects), this can be helpful for previewing the data: # Return the first five rows of the matrix head(matrix(1:10000, 1000, 10), 5) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 1001 2001 3001 4001 5001 6001 7001 8001 9001 ## [2,] 2 1002 2002 3002 4002 5002 6002 7002 8002 9002 ## [3,] 3 1003 2003 3003 4003 5003 6003 7003 8003 9003 ## [4,] 4 1004 2004 3004 4004 5004 6004 7004 8004 9004 ## [5,] 5 1005 2005 3005 4005 5005 6005 7005 8005 9005 # Return the last five rows of the matrix tail(matrix(1:10000, 1000, 10), 5) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [996,] 996 1996 2996 3996 4996 5996 6996 7996 8996 9996 ## [997,] 997 1997 2997 3997 4997 5997 6997 7997 8997 9997 ## [998,] 998 1998 2998 3998 4998 5998 6998 7998 8998 9998 ## [999,] 999 1999 2999 3999 4999 5999 6999 7999 8999 9999 ## [1000,] 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Using vectorization, we can easily perform matrix multiplication. # Create 3x3 matrix matrix(1:9, 3, 3) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 # Multiply each matrix value by 2 matrix(1:9, 3, 3) * 2 ## [,1] [,2] [,3] ## [1,] 2 8 14 ## [2,] 4 10 16 ## [3,] 6 12 18 2.4 Factors A factor is a data structure containing a finite number of categorical values. Each categorical value of a factor is known as a level, and the levels can be either ordered or unordered. This data structure is a requirement for several statistical models we will cover in later chapters. We can create a factor using the factor() function: # Create and fill factor with unordered categories education &lt;- factor(c(&quot;undergraduate&quot;, &quot;post-graduate&quot;, &quot;graduate&quot;)) education ## [1] undergraduate post-graduate graduate ## Levels: graduate post-graduate undergraduate Since education has an inherent ordering, we can use the ordered and levels arguments of the factor() function to order the categories: # Create and fill factor with unordered categories education &lt;- factor(education, ordered = TRUE, levels = c(&quot;undergraduate&quot;, &quot;graduate&quot;, &quot;post-graduate&quot;)) education ## [1] undergraduate post-graduate graduate ## Levels: undergraduate &lt; graduate &lt; post-graduate The ordering of factors is critical to a correct interpretation of statistical model output as we will cover later. 2.5 Data Frames A data frame is like a matrix in that it organizes elements within rows and columns but unlike matrices, data frames can store multiple types of data. Data frames are often the most appropriate data structures for the data required in people analytics. A data frame can be created using the data.frame() function: # Create three vectors containing integers (x), characters (y), and dates (z) x &lt;- 1:10 y &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;,&#39;h&#39;,&#39;i&#39;,&#39;j&#39;) z &lt;- seq(as.Date(&quot;2021-01-01&quot;), as.Date(&quot;2021-10-01&quot;), by = &#39;months&#39;) # Create a data frame with 3 columns (vectors x, y, and z) and 10 rows df &lt;- data.frame(x, y, z) df ## x y z ## 1 1 a 2021-01-01 ## 2 2 b 2021-02-01 ## 3 3 c 2021-03-01 ## 4 4 d 2021-04-01 ## 5 5 e 2021-05-01 ## 6 6 f 2021-06-01 ## 7 7 g 2021-07-01 ## 8 8 h 2021-08-01 ## 9 9 i 2021-09-01 ## 10 10 j 2021-10-01 The data type of each vector in a data frame object can be viewed using the str() function: # Describe the structure of df str(df) ## &#39;data.frame&#39;: 10 obs. of 3 variables: ## $ x: int 1 2 3 4 5 6 7 8 9 10 ## $ y: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ... ## $ z: Date, format: &quot;2021-01-01&quot; &quot;2021-02-01&quot; ... Specific columns in the data frame can be referenced using the $ symbol between the data frame and column names: # Return data in column x in df df$x ## [1] 1 2 3 4 5 6 7 8 9 10 Another method that allows for efficient subsetting of rows and/or columns is the subset() function. The example below illustrates how to subset df using conditions on x and y while only displaying z in the output: # Return z values for rows where x is at least 7 OR y is a, b, or c. subset(df, x &gt;= 7 | y == c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;), select = z) ## z ## 1 2021-01-01 ## 2 2021-02-01 ## 3 2021-03-01 ## 7 2021-07-01 ## 8 2021-08-01 ## 9 2021-09-01 ## 10 2021-10-01 2.6 Lists Lists are versatile objects that can contain elements with different types and lengths. The elements of a list can be vectors, matrices, data frames, functions, or even other lists. A list can be created using the list() function: # Store vectors x, y, and z as well as df to a list lst &lt;- list(x, y, z, df) str(lst) ## List of 4 ## $ : int [1:10] 1 2 3 4 5 6 7 8 9 10 ## $ : chr [1:10] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ... ## $ : Date[1:10], format: &quot;2021-01-01&quot; &quot;2021-02-01&quot; ... ## $ :&#39;data.frame&#39;: 10 obs. of 3 variables: ## ..$ x: int [1:10] 1 2 3 4 5 6 7 8 9 10 ## ..$ y: chr [1:10] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ... ## ..$ z: Date[1:10], format: &quot;2021-01-01&quot; &quot;2021-02-01&quot; ... Unlike vectors, accessing elements of a list requires double brackets, such as [[3]] for the third element. # Return data from the third element of lst lst[[3]] ## [1] &quot;2021-01-01&quot; &quot;2021-02-01&quot; &quot;2021-03-01&quot; &quot;2021-04-01&quot; &quot;2021-05-01&quot; ## [6] &quot;2021-06-01&quot; &quot;2021-07-01&quot; &quot;2021-08-01&quot; &quot;2021-09-01&quot; &quot;2021-10-01&quot; 2.7 Loops In many cases, the need arises to perform an operation a variable number of times. Loops are available to avoid the cumbersome task of writing the same statement many times. The two most common types of loops are while and for loops. Let’s use a while loop to square integers between 1 and 5: # Initialize variable i &lt;- 1 # Using a &#39;while&#39; loop, square the values 1 through 5 and print results to the screen # &#39;i&#39; is a variable that takes on a value between 1 and 5 for the respective loop while (i &lt; 6) { print(i^2) i &lt;- i + 1 # increment i by 1 } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 With a while loop, we needed to initialize the variable i as well as increment it by 1 within the loop. With a for loop, we can accomplish the same goal with less code: # Using a &#39;for&#39; loop, square the values 1 through 5 and print results to the screen for (i in 1:5) { print(i^2) } ## [1] 1 ## [1] 4 ## [1] 9 ## [1] 16 ## [1] 25 2.8 User-Defined Functions (UDFs) Though many built-in functions are available, R provides the flexibility to create our own. In fact, many functions used in this book are actually functions of functions. Functions can be an effective alternative to loops. For example, here is a basic function that achieves the same goal as our while and for loop examples (i.e., squaring integers 1 through 5): # Create a function named square.val() with one argument (x) that squares given x values square.val &lt;- function(x) { x^2 } # Pass integers 1 through 5 into the new square.val() function and display results square.val(1:5) ## [1] 1 4 9 16 25 While many projects warrant UDFs and/or loops, we do not actually need either to square a set of integers. As you gain experience writing R code, you will naturally discover ways to write more performant and terse code: # Square integers (1:5)^2 ## [1] 1 4 9 16 25 2.9 Graphics While base R has native plotting capabilities, we will use the more flexibile and sophisticated visualization capabilities within the ggplot2 library in this book. We can load the ggplot2 library using the library() function: # Load library for data viz library(ggplot2) When working with functions beyond what is available in base R, entering :: between the library and function name is a best practice for efficient coding. R Studio will provide a menu of available functions within the specified library upon typing library_name::. The ggplot2 library contains many types of visualizations. For example, we can build a line chart to show how the values of vector x relate to values of vector y in a data frame named data: # Create data frame containing two related vectors x &lt;- 1:10 y &lt;- (1:10)^2 data &lt;- as.data.frame(cbind(x, y)) # Produce line chart ggplot2::ggplot(data, aes(x = x, y = y)) + ggplot2::geom_line() Furthermore, we can use ggplot parameters and themes to adjust the aesthetics of visuals: # Produce line chart ggplot2::ggplot(data, aes(x = x, y = y)) + ggplot2::geom_line(size = .4, colour = &quot;blue&quot;) + # Reduce line thickness and change color to blue ggplot2::theme_bw() + # Remove the default grey background ggplot2::ggtitle(&quot;Plot Title \\n Split between Two Lines&quot;) + # Assign a title ggplot2::xlab(&quot;Variable x Name&quot;) + # Label the x-axis ggplot2::ylab(&quot;Variable y Name&quot;) + # Label the y-axis ggplot2::theme(plot.title = element_text(hjust = 0.5)) # Center plot title 2.10 Review Questions What is the difference between a factor and character vector? What is vectorization? How do data frames differ from matrices? Does executing the Sum() function achieve the same result as executing sum()? Does seq(1,10) return the same output as 1:10? Is the following command sufficient for creating a vector recognized by R as having three dates: dates &lt;- c(&quot;2022-01-01&quot;, &quot;2022-01-02&quot;, &quot;2022-01-03&quot;). How are while and for loops different? If vectors x1 and x2 each hold 100 integers, how can we add each element of one to the respective element of the other using a single line of code? How are slots in a list object referenced? What are some examples in which a user-defined function (UDF) is needed? "],["measure-sampl.html", "3 Measurement &amp; Sampling 3.1 Variable Types 3.2 Measurement Scales 3.3 Sampling 3.4 Review Questions", " 3 Measurement &amp; Sampling This chapter will survey the type and function of study variables, measurement scales, sampling and nonsampling error, as well as probability and nonprobability sampling methods. 3.1 Variable Types The framing of variables in research hypotheses guides the treatment of each in our analyses. In this section, we will discuss the function of independent, dependent, control, moderating, and mediating variables. 3.1.1 Independent Variables (IV) An Independent Variable (IV) is a variable which is assumed to have a direct effect on another variable. IVs are sometimes referred to as predictors, factors, features, antecedents, or explanatory variables. For example, we may wish to examine whether an inclusion training program given to a random sample of leaders has a positive effect on team-level belonging. In a true experimental design, participation in the inclusion training would be the only difference between the treatment (teams whose leaders who participate in the training) and the control (teams whose leaders who do not participate in the training). In this case, inclusion training participation is the IV (the variable we are manipulating). IVs are also present in non-experimental designs. For example, we may survey employees and ask them to rate their leader’s inclusiveness and provide self-reported belonging ratings. In this context, leader inclusiveness (rather than an inclusion training) is the IV. If we find that average team-level belonging scores tend to be higher when leader inclusiveness scores are higher, this may indicate that leader inclusion has some degree of influence on team-level belonging. Of course, there could be alternative explanations for any observed differences in team-level belonging, which is why experimental designs tend to provide stronger evidence for an IV’s effect. 3.1.2 Dependent Variables (DV) A Dependent Variable (DV) is a variable that is dependent on the IV. DVs are also referred to as outcome, response, or criterion variables. In our leader inclusion example, team-level belonging is the DV since this variable is assumed to depend on the level of team leaders’ inclusiveness. It’s important to note that regardless of a study’s results, it is the positioning of the variables in the study’s hypotheses (rooted in theory) that determines the type of variable. If we hypothesize that leader inclusion training has a positive effect on team-level belonging, but the study finds no such effect, the inclusion intervention is still the IV and team-level belonging the DV. 3.1.3 Control Variables (CV) A Control Variable (CV) is a variable that is held constant in research. The unchanging state of a CV allows us to understand the extent to which the IV has a unique and independent effect on the DV. In an experimental context, control variables represent a researcher’s attempt to control for alternative explanations so that the IV’s main effect on the DV can be isolated. For example, in our leader inclusion example, to be confident that the reason for any observed differences in team-level belonging can be attributed to leader inclusion training rather than other factors that theoretically may explain differences. For example, we should ensure that the two groups are similar with respect to characteristics such as gender and ethnicity, since underrepresented groups (URGs) may have different experiences independent of any training programs. In this case, gender and ethnicity are CVs. While we control for the effects of alternative explanations by way of the research design in an experimental context, we will discuss ways to control for these statistically in Chapter 9 for correlational designs. CVs are equally important in experimental and non-experimental contexts. 3.1.4 Moderating Variables A Moderating Variable influences the strength of the effect of an IV on a DV. Moderating variables are often referred to as interactions or interaction terms in models. Moderating variables may augment (strengthen) or attenuate (weaken) the effect one variable has on another. Interactions are widely understood in the context of medications; one drug may independently have a positive effect on one’s health but when combined with another medication, the interaction can behave very differently – and may even be lethal! In our inclusive leadership example, we may find that the strength of the training’s effect on team-level belonging varies based on a leader’s span of control (SoC). It stands to reason that leaders with a lower SoC may find it easier to cultivate a climate of inclusivity and belonging, while leaders with a higher SoC may have more scope/projects and team dynamics to manage and may find it more difficult to consistently apply strategies covered during the training. Interactions between variables are vital to our understanding of nuance and complexity in the dynamics influencing outcomes in organizational settings. Chapter 9 will cover how to test for significant interactions. 3.1.5 Mediating Variables A Mediating Variable explains the how or why of an IV’s effect on a DV. It may be helpful to think of mediating variables as a “go-between” – a part of the causal pathway of an effect. In the case of inclusive leadership training, effects on belonging are likely not the result of the training itself. More likely, the training raised awareness and helped leaders develop strategies to cultivate team inclusivity. One strategy endorsed during the training may be participative decision making, and the implementation of this strategy may explain why the training has a positive effect on team-level belonging. There may be multiple mediators of any observed effect of the intervention on team-level belonging depending on training outcomes. Variables in these more complex relationship paths are sometimes characterized as proximal and distal effects. Proximal effects are those which directly influence an outcome, such as the impact of participative training on team belonging. Distal effects are upstream effects that indirectly influence an outcome, such as inclusion training participation. Mediating variables may fully or partially mediate the relationship between an IV and DV. Full mediation indicates that the mediator fully explains the effect; in other words, without the mediator in the model, there is no relationship between an IV and DV. Partial mediation indicates that the mediator partially explains the effect; that is, there is still a relationship between an IV and DV without the mediator in the model. Partial mediation indicates that there are additional explanations for how or why observed effects exist, such as the implementation of additional team inclusion strategies influencing team belonging. In Chapter 9, we will discuss how to test for both full and partial mediation. Translating research hypotheses into conceptual models of hypothesized relationships is helpful in visually representing the function of each variable in the study. Figure 3.1 illustrates how each type of variable is depicted using our inclusive leadership example: Figure 3.1: Conceptual Model of Hypothesized Relationships among Variables 3.2 Measurement Scales Measurement scales are used to categorize and quantify variables. There are two major categorizations – discrete and continuous – and these, together with the research hypotheses, help determine appropriate types of analyses to perform. 3.2.1 Discrete Variables Discrete variables are also known as categorical or qualitative variables. Categorical variables have a finite or countable number of values associated with them, and these can be further categorized as either nominal or ordinal. Nominal A nominal variable is one with two or more categories for which there is no intrinsic ordering to the categories. Examples of nominal variables include office locations, departments, and teams. A dichotomous variable is a type of nominal variable which has only two unordered categories. Examples of dichotomous variables include people leader vs. individual contributor, active vs. inactive status, and remote worker vs. non-remote worker. Ordinal An ordinal variable is like a nominal variable with one important difference: ordinal variables have ordered categories. Examples of ordinal variables include education levels, job levels, and survey variables measured on Likert-type scales. 3.2.2 Continuous Variables Continuous variables are also known as quantitative variables. Continuous variables can assume any real value in some interval, and these can be further categorized as either interval or ratio variables. Interval Variables measured on an interval scale have a natural order and a quantifiable difference between values but no absolute zero value. Examples include SAT scores, IQ scores, and temperature measured in Fahrenheit or Celsius (but not Kelvin). In these examples, 0 is either not an option (i.e., SAT and IQ) or does not represent the absence of something (e.g., 0 degrees is a temperature, albeit a cold one!). Ratio Variables measured on a ratio scale have the same properties as data measured on an interval scale with one important difference: ratio data have an absolute zero value. Examples include compensation, revenue, and sales; a zero in these contexts is possible and would indicate a true absence of something. 3.3 Sampling The goal of research is to understand a population based on data from a subset of population members. In practice, it is often not feasible to collect data from every member of a population, so we instead calculate sample statistics to estimate population parameters. Another important concept is the sampling frame. While the population represents the entire group of interest, the sampling frame represents the subset of the population to which the researcher has access. In an ideal setting, the population and sampling frame are the same, but they are often different in practice. For example, a professor may be interested in understanding student sentiment about a new school policy but only has access to collect data from students in the courses she teaches. In this case, the entire student body is the population but the students she has access to (those in the courses she teaches) represent the sampling frame. The sample is the subset of the sampling frame that ultimately participates in the research (e.g,. those who complete a survey or participate in a focus group). 3.3.1 Sampling &amp; Nonsampling Error Sampling and nonsampling errors are general categorizations of biases and error in research (Albright &amp; Winston, 2016). 3.3.1.1 Sampling Error Sampling error is the inevitable result of basing inferences on a random sample rather than the entire population. The two main contributors to sampling error are the size of the sample and variation in the underlying population. The risk of sampling error decreases as the sample size approaches the population size; however, it is usually not feasible to gain information from the entire population, so sampling error is generally a concern. Selection Bias Selection bias is the bias introduced by a non-random method of selecting data for analysis, which can systematically skew results in a particular direction. Selection bias may result in observed relationships or differences that are not due to true relationships or differences in the underlying populations, but to the way in which participants or data were selected for the research. A type of selection bias that is important to consider in people analytics is survival bias. In people analytics, survival bias is the logical error of focusing only on people who made it past some selection process while overlooking those who did not. For example, to gain an accurate understanding of the number of employees who survive to each tenure milestone (e.g., 1 year, 2 years, 3 years, etc.), we need to study both active and inactive people to avoid biased results. We may find a significant drop in the percent of active employees who survive from 3 to 4 years, for example, but without information on inactive employees we do not know if this is a function of hiring (i.e., relatively few hired more than 3 years ago) or due to a spike in attrition beyond 3 years of tenure. The work of a mathematician named Abraham Wald during World War II is a classic example of survival bias. Wald was a member of the Statistical Research Group (SRG) at Columbia University that examined damage to returning aircraft (Wald, 1943). Rather than focusing on the areas with damage, Wald recommended a different way of looking at the data, suggesting that the reason certain parts of planes were not covered in bullet holes was because planes that were shot in these areas did not return. In other words, locations with bullet holes represented locations that could sustain damage and still return home. This insight led to armor being reinforced in areas with no bullet holes. Figure 3.2: Hypothetical Data for Damaged Portions of Returning WWII Planes. Image Courtesy of Wikimedia Commons. Missing data can sometimes be more valuable than the data we have, and it is critical to promote a representative data generative process to prevent biased selection and results. It is important to understand the centrality of randomness in sampling. Randomization protects against subjective biases, self-validates the data, and is the key ingredient that defines the representative means of extracting information (Kahneman, 2011). Sample data that are not representative of the population of interest can lend to anomalies – mere coincidences. While non-random data can be leveraged for directionally accurate insights, randomness is required to make inferences about a broader population with a reasonable degree of confidence. Let’s consider an example from Kahneman (2011) in which six babies are born in sequence at a hospital. The gender of these babies is of course random and independent; the gender of one does not influence the gender of another. Consider the three possible sequences of girls (G) and boys (B) below: BBBGGG GGGGGG BGBBGB Are these sequences equally likely? Though it may initially be counter-intuitive, since the events are independent and the outcomes (B and G) are (approximately) equally likely, any possible sequence of births is as likely as any other. Sample size (colloquially referred to as the \\(n\\)-count) is also important as this can have a material influence on the representativeness of sample data – and consequently, the veracity of results and conclusions based on them. As we will explore in Chapter 6, as the sample size increases, so too does our confidence that estimates based on sample data reflect population parameters. To illustrate the effects of sample sizes, let’s consider a hypothetical study in which the promotion rate in an organization is found to be lowest in divisions that are primarily software engineers, low diversity, small, and geographically dispersed. Which of these characteristics might offer an explanation? Let’s consider that this study also found that the divisions with highest promotion rates have identical characteristics: software engineers, low diversity, small, and geographically dispersed. Small samples yield extreme results more often than large samples. Small samples neither cause nor prevent outcomes; they merely allow the incidence of the outcome to be much higher (or much lower) than it is in the larger population (Kahneman, 2011). Non-Probability Sampling Non-probability sampling can help us gain insight into the possible. However, we cannot make inferences based on data collected through non-probability sampling methods since the sample is unlikely to be representative of the population. Convenience (Accidental) Sampling Convenience sampling is the most common type of nonprobabilistic sampling. This sampling method involves taking samples that are conveniently located around a specific location (physical or virtual). For example, if we were to study employee sentiment about new benefit plans by polling employees walking through the lobby of a particular office building one morning, this would represent convenience sampling. Aside from the risk of employees sharing socially desirable responses in such a setting and invalidating the results, a major shortcoming of this approach is that we are only capturing the sentiment of those who happen to walk into one particular building during one limited window of time. This would not capture the voice of employees who are working remotely, working in another office location, on PTO, taking a sick day, attending an offsite conference or meeting, or stuck in traffic and running late. Quota Sampling Quota sampling is a nonprobabilistic sampling method in which researchers for a sample that is representative of a larger population. With quota sampling, researchers assign quotas to a group of people to create subgroups of individuals that reflect the characteristics of the population. This is nonprobabilistic since researchers choose the sample rather than randomly selecting it. For example, if the characteristics of the employee population are known, the researcher polling employees in the office lobby about benefit plans could collect some additional information (e.g., department, job, tenure) to achieve a commensurate proportion of each in the sample. If 30% of employees in the larger workforce are in the Engineering department, the researcher could assign a quota – such as 3 in every 10 participants – to choose a sample in which 30% of employees come from the Engineering department. Purposive (Judgmental) Sampling The main goal of purposive sampling is to construct a sample by focusing on characteristics of a population that are of interest to the researcher. Purposive sampling is often used in qualitative or mixed methods research contexts in which a smaller sample is sufficient. Since it is a nonprobabilistic sampling method, purposive sampling is highly prone to researcher bias. For example, the People Team may be interested in understanding what is top-of-mind for employees in order to design a survey with relevant items. The team may choose people to participate in focus groups to surface qualitative themes – not for the purpose of generalizing findings but to guide survey item selection efforts. Probability Sampling Probability sampling can help us gain insight into the probable. The main difference between non-probability and probability sampling is that non-probability sampling does not involve random selection and probability sampling does. Probability sampling is intended to facilitate inferences since data collected through random selection methods is more likely to be representative of the population and protects against over-indexing on participants with similar qualities in the sample. Simple Random Sampling Simple random sampling is a method in which each member of the population has the same probability of being selected for a sample. An example of simple random sampling is randomly selecting a specified number or percent of employees from the workforce to participate in a survey without regard for tenure, department, level, or other characteristics. We can use the sample() function in R to randomly select from a vector of elements: # Load library for data wrangling library(dplyr) # Read employee data employees &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv&quot;) # Set seed for reproducible results set.seed(1234) # Sample 10 employees randomly n = 10 sample(employees$employee_id, n, replace = F) ## [1] 2308 2018 2125 2004 1623 1905 1645 1934 1400 1900 The replace = F argument in the sample() function indicates that we want to sample without replacement (i.e., we do not want an employee to be selected twice in the sample). For example, if we draw multiple names from a hat without replacement, we will not put names back into the hat once they are drawn; each has a chance of being selected only once. While sampling with replacement would not make sense for applications such as pulse survey participation, as we would not want a given employee to take a survey multiple times, replace = T can be applied if the application requires it (e.g., a lottery in which employees can win more than once). Stratified Random Sampling Stratified random sampling is a sampling method in which the population is first divided into strata. Then, a simple random sample is taken from each stratum – a homogeneous subset of the population with similar characteristics with regard to the variable of interest. The combined results constitute the sample. To ensure samples do not comprise a larger proportion of employees from a particular department, education level, tenure band, generational cohort, or other variable deemed useful in explaining differences in response scores, researchers can randomly select members for each stratum based on the proportion in the respective stratum in the larger population. For example, if 30% of all employees are in the Engineering department, the researcher could randomly select a calculated number of employees from the Engineering department such that 30% of employees in the sample come from this department. Let’s demonstrate stratified random sampling in R by sampling \\(\\frac{1}{2}\\) of employees within each department. First, we can review counts of employees for each department using the group_by() and summarise() functions from the dplyr library. Note that dplyr functions are chained together using a %&gt;% symbol: # Return employee counts by department employees %&gt;% group_by(dept) %&gt;% summarise(n = n()) ## # A tibble: 3 × 2 ## dept n ## &lt;chr&gt; &lt;int&gt; ## 1 Human Resources 63 ## 2 Research &amp; Development 961 ## 3 Sales 446 Next, we will randomly select \\(\\frac{1}{2}\\) of employees within each department using the group_by() and sample_frac() functions from the dplyr library. We will store the selected employees’ records in a data frame and then query it to validate that the counts are roughly \\(\\frac{1}{2}\\) the total count observed for each department. # Obtain and store stratified random sample strat_sample &lt;- employees %&gt;% group_by(dept) %&gt;% sample_frac(size = .5) # Return sample counts by department strat_sample %&gt;% group_by(dept) %&gt;% summarise(n = n()) ## # A tibble: 3 × 2 ## dept n ## &lt;chr&gt; &lt;int&gt; ## 1 Human Resources 32 ## 2 Research &amp; Development 480 ## 3 Sales 223 Cluster Sampling Cluster sampling is a sampling method often used in market research in which the population is first divided into clusters. Then, a simple random sample of clusters is taken. All the members of the selected clusters together constitute the sample. Unlike stratified random sampling, it is the clusters that are selected at random – not the individuals. It is hoped that each cluster by itself is representative of the population (i.e., each cluster is heterogeneous). For example, employees could be partitioned into clusters based only on their geographic region. Since there is not further partitioning on other variables, each cluster is expected to be heterogeneous on the basis of variables other than geographic region – that is, unless geography is related to other variables (e.g., call center employees are all located in a company’s Pacific Northwest region). By selecting a random set of clusters, the combination of employees across the selected clusters is expected to be representative of the population. Let’s demonstrate how to implement cluster sampling in R: # Randomly assign each employee to 1 of 10 clusters (groups) employees$cluster &lt;- sample(1:10, size = nrow(employees), replace = T) # Randomly select 5 clusters clusters &lt;- sample(unique(employees$cluster), size = 5, replace = F) # Store cluster sample clust_sample &lt;- employees %&gt;% filter(cluster %in% clusters) # Display dimensions of the cluster sample object dim(clust_sample) ## [1] 748 36 Systematic Sampling Systematic sampling involves selecting sample members from a population according to a random starting point and a fixed, periodic interval known as a sampling interval. The sampling interval is computed by taking the population size and dividing it by the desired sample size. The resulting number is the interval at which population members are selected for the sample. For example, if there are 10,000 employees and our desired sample size is 500, the sampling interval is 20. Therefore, we would select every 20th employee for our sample. It is important that the sequence does not represent a standardized pattern that would bias the data; this process needs to be random. For example, if the employee id generated by the HRIS increases with time, we would expect employees with longer tenure to have lower employee ids while new joiners would have higher employee ids. Ordering employees by employee id prior to selection could bias the sample on the basis of variables related to tenure (e.g., aggressive periods of Engineering hiring). Let’s walk through the step-by-step process for implementing the systematic sampling procedure in R: # Specify desired sample size n = 100 # Determine population size N = nrow(employees) # Compute sampling interval, rounding up to nearest whole number via the ceiling() function si = ceiling(N/n) # Randomly select value between 1 and the sampling interval for starting value strt = sample(1:si, 1) # Increment starting value by the sampling interval until the desired sample size is achieved, and hydrate index vector with the selected indices index = seq(strt, strt + si * (n - 1), si) # Store systematic sample syst_sample &lt;- employees[index, ] 3.3.1.2 Nonsampling Error There are many types of nonsampling error that can invalidate results beyond the sampling procedure, and we will focus on several that are particularly germane to people analytics: nonresponse bias, nontruthful responses, measurement error, and voluntary response bias. Nonresponse Bias Surveys are a staple in the set of data sources germane to people analytics. While survey data provide unique attitudinal and perceptive signals that can be highly predictive of future behavior and events, surveys tend to be far more susceptible to nonsampling error than other data sources. As discussed in the context of sampling error, we usually do not have access to information on entire populations of interest, so we must consider the possibility that those for whom we are missing data may have common qualities, perceptions, or opinions that differ from those for whom we do have data. This is known as nonresponse bias. For example, if we administer an employee experience survey to the entire organization and receive a 60% response rate, the reality is that we do not know how the 40% of nonrespondents would have responded. It is possible that nonrespondents represent highly disengaged employees, in which case their responses may have materially influenced results and conclusions in an unfavorable direction. It is also possible that the nonrespondents were busy, away on vacation, cynical to the confidentiality language in the communications, or any number of other reasons which may or may not have resulted in significantly different feedback from that of respondents. Nonresponse bias is not limited to surveys. For example, self-reported demographics such as gender and ethnicity may not be disclosed by all employees in the HR information system (HRIS). This can bias segmentations based on these categorical dimensions. While there are strategies to address this, such as visual ID or applying models trained to infer missing values (which may be necessary to fulfill EEOC reporting requirements), there may still be error in the imputed values. Nontruthful Responses While high response rates may reduce nonresponse bias, this isn’t always something to celebrate. Organizations that incentivize participation in surveys often do so at the risk of people responding in socially desirable ways and providing nontruthful responses to achieve some defined target. For example, if an employee has an unhealthy relationship with his manager but does not trust that managers will not have access to individual-level responses, the employee may decide to indicate on the survey that everything is highly favorable to help the team win the month of casual days leadership promised. This can of course skew and invalidate results. While survey participation should be strongly encouraged since higher response rates can mitigate the risk of certain types of bias and increase confidence that the survey is representative of the collective organization’s sentiments, incentivizing participation can be dangerous. Measurement Error The measurement method can affect observed data either by changing the underlying construct of interest, or by distorting the measurement process without impacting the construct itself (Spector, 2006). Common method variance (CMV), also known as monomethod bias, relates to a widely held belief that relationships between variables measured using the same method are inflated. The idea that the measurement method itself introduces a degree of variance in measures has been cited in organizational papers for decades, and it is raised almost exclusively when cross-sectional, self-reported surveys are utilized. Though controversial, there is generally a consensus that where it is possible to do so, it is preferrable to leverage multiple measurement methods. My doctoral dissertation research explored how implicit voice theories, which are deep-seated beliefs about the risks involved in speaking up to those higher in the organizational hierarchy (e.g., negative career consequences), influenced the extent to which individual contributors actually speak up in prosocial ways to their leaders (Starbuck, 2016). Since the individual contributors are best placed to provide information on the implicit beliefs they maintain, data for the IV were collected using cross-sectional self-reports. At the same time, I surveyed their immediate supervisors and asked them to rate each of their direct reports using a leader-directed voice scale; these supervisor-reports of leader-directed voice were used as the DV in this study. To investigate CMV, which was a tangential interest to the primary research objective, I also included the leader-directed voice scale (using self-reported language) on the survey administered to individual contributors. For the 1,032 employees from whom I collected data in an investment firm context (individual contributors: \\(n\\) = 696; supervisors: \\(n\\) = 336), I was surprised to find only a weak correlation between self-reported voice and supervisor-reported voice (\\(r\\) = .26, \\(p\\) &lt; .01). On average, self-reported voice was higher with less variation (\\(\\bar{x}\\) = 5.91, \\(s\\) = 1.15) relative to supervisor-reported voice (\\(\\bar{x}\\) = 5.69, \\(s\\) = 1.34). Interestingly, there was support for almost none of the hypothesized relationships when supervisor-reported voice was positioned as the DV, though most were supported when self-reported voice was substituted as the DV in post-hoc analyses. Given the prevalence of monomethod self-reports in the social sciences, the influence of CMV is an important consideration. Measurement error relates to errors stemming from confusing questions, survey fatigue, and low-quality scales used to measure multidimensional psychological constructs. The field of psychometrics is a vast scientific discipline concerned with the development of assessment tools, measurement instruments, and formalized models to understand latent psychological constructs such as engagement, belonging, purpose, and wellbeing using observable indicators. The reduction of measurement error is a principal concern to psychometricians. For a deeper treatment of the survey scale development process, see DeVellis (2012). While an exhaustive treatment of psychometrics is beyond the scope of this book, reliability and validity are two broad sets of methods designed to increase the robustness of psychological instrumentation which will be reviewed in this section. Even slight adjustments to validated instrumentation – such as changing the number of scale anchors (e.g., increasing from a 5-point to 7-point Likert scale), tweaking item language, or modifying which items are included in a composite scale – generally warrant reliability and validity analyses. Figure 3.3 illustrates differences between reliability and validity. Figure 3.3: Reliability and Validity A weight scale may be a helpful example for understanding differences between these concepts. A weight scale is designed to provide an accurate measurement of one’s weight, and we expect measurements to be consistently accurate over time. If a 150 lb. person steps onto a weight scale and receives a reading of 180 lbs., the scale is not valid as the person actually weighs 30 lbs. less than the reading. If the person steps onto the scale a second time moments later and receives a reading of 200 pounds, the scale is not reliable either (inconsistent measurements). As researchers, it is critical to measure what we intend to measure (validity) and do it with consistency (reliability). Survey items with poor psychometric properties can lend to invalid conclusions due to measurement error. Reliability describes the quality of measurement (i.e., the consistency or repeatability of measures). Types of reliability include: Inter-Rater or Inter-Observer Reliability: the degree to which different raters/observers give consistent estimates of the same phenomenon. Test-Retest Reliability: the consistency of a measure from one time to another. Parallel-Forms Reliability: the consistency of the results of two tests constructed in the same way from the same content domain. Internal Consistency Reliability: the consistency of results across items within a test. Validity describes how well a concept was translated into a functioning and operating reality (operationalization). There are four main types of validity: (a) face validity, (b) content validity, (c) construct validity, and (d) criterion-related validity: Face validity Face validity is an assessment of how valid a measure appears on the surface. In other words, face validity represents whether the measurement approach “on its face” is a good translation of the construct. This is the least scientific method of validity and should never be accepted on its own merits. Content Validity Content validity is a somewhat subjective assessment of whether a measure covers the full content domain. Content validity relies on people’s perceptions to measure constructs that would otherwise be difficult to measure. For example, a panel of experts may gather to discuss the various dimensions of a theoretical construct. The psychometrician may then use this information to develop survey items that tap these dimensions to achieve a comprehensive measure of the construct. Construct Validity In social science, constructs are often measured using a collection of related indicators that together, tap the various dimensions of the theoretical idea. Constructs may manifest in a set of behaviors, which provide evidence for their existence. Construct validity represents the degree to which a collection of indicators and behaviors – the operationalization of the concept – truly represents theoretical constructs. Psychological safety, a belief that a context is safe for interpersonal risk-taking (Edmondson, 1999), has no direct measure. However, there are indicators and behaviors that are helpful in understanding the extent to which an environment is psychologically safe. We may ask employees whether they are able to bring up problems to decision makers or whether it is safe to take risks on their team. Based on the theoretical conception of psychological safety, these would be helpful (though not exhaustive) indicators of the construct in an organizational setting. There are two types of construct validity: Convergent validity: the degree to which the operationalization is similar to (converges on) other operationalizations to which it theoretically should be similar. Discriminant validity: the degree to which the operationalization is not similar to (diverges from) other operationalizations to which it theoretically should not be similar. A nomological network is central to providing evidence for a measure’s construct validity. The nomological network is an idea developed by Cronbach and Meehl (1955) to represent the constructs of interest, their observable manifestations, as well as the interrelationships among them. The term “nomological” is derived from the Greek word meaning “lawful”; therefore, the nomological network aims to make clear what a construct means so that laws (nomologicals) can be applied. Simply put, the nomological network attempts to link the conceptual and theoretical realm to the observable one to provide a practical methodology for assessing a measure’s construct validity. If psychological safety theory suggests the construct should be positively related to leader openness and negatively associated with employee withholding (silence), we can use validated measures of openness and withholding to test for these theoretical relationships with psychological safety and substantiate construct validity. Criterion-Related Validity Criterion-related validity, sometimes referred to as instrumental validity, describes how well scores from one measure are adequate estimates of performance on an outcome measure (or criterion). There are two types of criterion-related validity: Predictive validity: the operationalization’s ability to predict something it should theoretically be able to predict. Concurrent validity: the operationalization’s ability to distinguish between groups that it should theoretically be able to distinguish between. If psychological safety should positively influence employee voice, there would be support for predictive validity if we find that employees who report more favorable perceptions of psychological safety are more willing to speak up. If we administer a new scale to measure psychological safety and at the same time (concurrently) administer an existing, validated measure of the same construct, highly correlated results would lend support for the new measure’s concurrent validity. 3.4 Review Questions What are the differences between parameters and statistics? How does a sampling frame differ from a sample? How does cluster sampling differ from stratified random sampling? What is the primary benefit of probabilistic sampling methods over non-probabilistic sampling? Is nonresponse bias only applicable in the context of surveys? What type of variable influences the strength of the effect one variable has on another? 100 randomly selected employees in the Marketing department of an organization participated in a survey on career pathing for marketing professionals. What is the sample and what is the population in this case? How does the meaning of zero differ between interval and ratio-scaled variables? Can discrete variables have more than 2 values? What are some examples of nonprobabilistic sampling methods? "],["research.html", "4 Research Fundamentals 4.1 Research Questions 4.2 Research Hypotheses 4.3 Internal vs. External Validity 4.4 Research Methods 4.5 Research Designs 4.6 Review Questions", " 4 Research Fundamentals The importance of appropriate research methods and designs cannot be overstated. Just as probability sampling methods help us achieve data that are representative of the population, research methods and designs help us achieve an accurate understanding of various phenomena and ensure conclusions are justified. 4.1 Research Questions Research questions are fundamental to all research projects. Research questions help focus the study, determine the appropriate methodology, and guide each stage of inquiry, analysis, and reporting. Some examples of research questions germane to people analytics include: \\(Q_1\\): Why has there been an increase in attrition over the past quarter? \\(Q_2\\): How equitable are promotion nominations across the organization? \\(Q_3\\): Are there meaningful differences in the favorability of experiences for remote vs. non-remote employees? \\(Q_4\\): Do new joiners have the training and resources they need to be successful? \\(Q_5\\): What portion of team performance is attributable to leadership effectiveness? 4.2 Research Hypotheses Research hypotheses are testable statements about the expected outcome of a research project or experiment. \\(H_1\\): Manager satisfaction is a significant predictor of voluntary attrition. \\(H_2\\): Promotion nomination rates are not significantly different by gender and ethnicity. \\(H_3\\): Employee experience favorability is not significantly different between remote and non-remote workers. \\(H_4\\): New hire training perceptions are positively associated with onboarding experience favorability. \\(H_5\\): Leadership effectiveness perceptions explain significant variation in team performance. 4.3 Internal vs. External Validity Internal validity refers to the extent to which confounding variables are controlled. In other words, internal validity reflects the robustness of the study. For example, if a study finds a significant relationship between work location and attrition but considers no other factors or explanations, this would not be a robust study. Work location may emerge significant because certain roles for which attrition is higher are more concentrated in one or more geographies. It could also be the case that the company has made acquisitions in new geographies, and the acquired employees have significantly different experiences (and attrition rates) relative to non-acquired employees. Confounding variables are critically important in the context of internal validity. A confounding variable is an extraneous variable whose presence impacts the variables being studied such that results do not reflect the actual relationships. Studies with weak internal validity often result in spurious associations that confound the true relationship between two variables, leading to invalid conclusions and recommendations. External validity refers to the extent to which study conclusions will hold in other contexts (for other people, in other places, at other times). Randomization is fundamental to our ability to generalize and apply findings to other groups or contexts. If we survey employees to understand sentiments about recent changes in business strategy but exclude groups for which there may be different impacts or perceptions, conclusions about the collective sentiment would be suspect at best. 4.4 Research Methods There are three major categories of research methods: (1) quantitative, (2) qualitative, and (3) mixed methods. Quantitative Addresses “what” questions Utilizes numerical data (e.g., surveys, systems) Primarily deductive Used to test hypotheses Involves statistical analyses More objective More generalizable Qualitative Addresses “how” and “why” questions Utilizes text data (e.g., focus groups, interviews, open-ended feedback) Primarily inductive Used to formulate theory or hypotheses Involves organizing data into categories or themes More subjective Less generalizable Mixed Methods Integrates the strengths of both quantitative and qualitative methods within a single study, often leading with qualitative approaches to build theory and hypotheses followed by quantitative methods to test hypotheses 4.5 Research Designs In addition to determining whether a quantitative, qualitative, or mixed methods study is most appropriate, researchers also need to decide on the type of study within each of these three. Research designs are the types of inquiry within quantitative, qualitative, and mixed methods approaches that issue specific direction for the research procedures (Creswell, 2018). There are multiple taxonomies for research designs, and we will simplify to the most common types. Within the quantitative category, there are three types of designs: (a) experimental, (b) quasi-experimental, and (c) correlational. As shown in Figure 4.1, it is important to understand the centrality of randomization in this decision. Figure 4.1: Quantitative Research Designs Experimental Research Experimental research is concerned with casual (internal) validity. Randomized experimental designs provide the most rigor with regard to causal validity. However, in social science research contexts, true experiments often are not possible due to ethical considerations. For example, if we were interested in understanding the causal effect leadership quality has on employee engagement, based on a hypothesis that poor leadership decreases employee engagement, we would need to randomly assign employees to one of two groups that are identical on the basis of all variables that could theoretically explain why employees vary in their levels of engagement. Then, we would need to manipulate the variable of interest (leadership quality) to evaluate if the group of employees subjected to poor leadership (treatment group) reports significantly different levels of engagement relative to the group of employees for whom leadership quality has not been manipulated (control group). In a practical setting, it would of course be unethical to purposefully subject employees to poor leadership with the expectation of reducing engagement – and consequently, retention, productivity, and impact to the organization. Clinical trials are a common setting for true experiments, as isolating the effects of an experimental drug can be a matter of life or death. In a randomized clinical trial, patients are randomly assigned to an experimental group (patients who receive the drug) or control group (patients who receive a placebo). To protect against placebo effects biasing the results, patients do not know if they receive the experimental treatment or the placebo. Done correctly, these experiments have the highest level of internal validity. Another example of an experimental design is A/B testing. A/B testing is often performed in the context of website optimization, in which two or more versions of the site are shown to customers to identify which version impacts key success metrics more positively. In a people analytics context, we may create two versions of a dashboard and randomly assign the permissioned users to each. We could then assess whether utilization rates, average usage time, repeat usage, among other success measures are significantly different between the two groups of users to inform which design is most effective. In experimental research, it is important to consider the influence of the Hawthorne Effect, which refers to the tendency of some individuals to modify their behavior in response to the awareness that they are being observed. This term was coined during experiments at Western Electric’s factory in the Hawthorne suburb of Chicago in the late 1920s and early 1930s. One of many studies conducted to understand how work environments effect productivity was known as the “Illumination Experiment”. During this study, researchers experimented with a number of lighting levels in a warehouse in which workers made electrical relays. The researchers found that any change in the lighting – even when introducing poor lighting – led to favorable changes in output. However, these productivity gains disappeared once the attention faded (Roethlisberg &amp; Dickson, 1939). In a people analytics context, if we inform employees that we are going to monitor their productivity more closely over a period of time, it is likely that at least some employees will attempt to modify their behavior in order to increase productivity levels. After all, higher productivity is generally regarded as an ideal across companies and industries. In this case, manipulation of an IV to study a treatment effect, such as flexible work arrangements, would be impacted by this phenomenon; that is, observed differences in productivity may not be attributable to flexible work arrangements but merely due to employees knowing they were being observed. Quasi-Experimental Research Quasi-experimental research is an experiment in which participants cannot be randomly assigned. In the case of our leadership quality example, a quasi-experiment may examine engagement differences between two groups of employees who rate their leader either favorably (Group A) or unfavorably (Group B). A key limitation of this approach is that the groups may be different in important ways beyond leader perception incongruities. For example, Group A employees may be concentrated within a single department, whereas Group B employees may span all other departments. This would indicate that the difference in leadership – and presumably engagement – is driven by factors unique to the department, making it more challenging to isolate the effects of leadership quality on engagement. Perhaps the department with unfavorable leader perceptions has seen significant attrition, or the department is largely first-time people leaders in need of coaching and support. Another example of quasi-experiments is a pretest-posttest setting in which there are multiple measures. Random assignment could be used in pretest-posttest contexts, in which case this would be characterized as a true experiment, but often this approach is implemented without random assignment. For example, we could test the hypothesized effect of leadership quality on engagement via a pretest-posttest approach. If leaders are selected for a leadership development workshop, we could survey the leaders’ teams and collect data on leader effectiveness perceptions and self-reported engagement prior to (baseline) and after the workshop. It is unlikely that leaders were selected for this workshop by a random process; more likely, there were criteria driving the selection, such as leaders who were identified as critical talent or who achieved a certain performance level. If this study finds that improvements in leadership effectiveness correlate with improvements in engagement, there would be some evidence in favor of improving leadership quality; however, this would not be sufficient evidence for a causal effect. Though quasi-experiments are not as robust as true experiments, they are usually more feasible in a people analytics context. True experiments control for confounding variables by way of the research design (randomization ensures equivalent groups), while these factors must be controlled statistically in quasi-experimental contexts. In Chapter 9, we will discuss how to model relationships among multiple variables in order to study how one variable influences another while holding constant variables that may influence the outcome but are not the primary focus of the research. Non-Experimental Research Unlike experimental and quasi-experimental designs, non-experimental research does not involve the manipulation of an IV. The goal of non-experiments is not to provide evidence for causal effects, but to study measured variables as they naturally occur and disentangle patterns in the data. Given the potential for alternative explanations of any observed differences or relationships, non-experimental research tends to have lower internal validity than experimental and quasi-experimental designs. As we have discussed, it is often not possible or ethical to manipulate IVs or to randomly assign people to groups. In addition, the nature of research questions does not always warrant experiments. In these cases, one or three non-experimental approaches may be considered: (a) cross-sectional, (b) correlational, and (c) observational. Cross-sectional research compares two or more natural groups of people. For example, we may examine differences in engagement between employees in the Engineering department relative to employees in the Product department. In this case, we would neither manipulate one’s department to determine how department influences engagement, nor randomly assign employees to these departments. Department membership exists apart from the research, so these naturally occurring groups can be leveraged for comparisons. There are of course many examples of naturally occurring groups that we would not manipulate, such as gender, ethnicity, generation, education, job family, job level, location, and tenure band. When participant characteristics are used to create groups, the IV is sometimes referred to as experimenter-selected rather than experimenter-manipulated IVs. Correlational research involves studying the statistical relationship between two variables without the manipulation of an IV. The relationship between leadership quality and engagement could be evaluated using correlational research. However, we would be unable to leverage a correlational design to test a hypothesis positing a causal effect of leadership quality on engagement. We would be limited to understanding how leadership quality and engagement covary; that is, to what extent a change in one variable is associated with a change in the other. Engagement may tend to increase as leadership quality increases, but a correlational design does not lend to understanding the direction of causal influence – if such an effect exists. Observational research refers to studies in which the researcher gathers information without research subjects being explicitly involved in the recording of data. Collecting data from the company’s HRIS could be an observational research method. For example, if we access data on terminations to determine the rate of attrition over a specified period, we would not need to interfere by asking past or present employees for this information. We would also do so without manipulating an IV, tagging people to naturally occurring or artificially created groups, or evaluating the association of attrition with another variable. The reality is that such an approach would not be too actionable, however, as this would offer no understanding of what may be influencing attrition or how attrition varies across departments, jobs, locations, or other theoretically-relevant dimensions. 4.6 Review Questions What type of research method and design would be best suited for a study aiming to understand the effect of stay interviews on employee attrition? Why are quasi-experiments less rigorous than true experiments? Why aren’t experimental designs always implemented when an experimenter-manipulated IV is warranted? What is the role of research questions? What is the role of research hypotheses? What is the difference between internal and external validity, and why are these concepts important in research? What is an example of a mixed methods study? What is the key difference between experimental and non-experimental research designs? What are the differences between cross-sectional, correlational, and observational non-experimental designs? How can the Hawthorne Effect impact the integrity of an experiment? "],["desc-stats.html", "5 Descriptive Statistics 5.1 Univariate Analysis 5.2 Bivariate Analysis 5.3 Review Questions", " 5 Descriptive Statistics This chapter reviews essential univariate and bivariate analysis concepts that underpin the more complex statistical methods in subsequent chapters of this book. Univariate and bivariate analyses can be either descriptive or inferential, and this chapter will cover descriptive techniques while Chapter 6 will cover inferential methods. 5.1 Univariate Analysis Univariate analysis is the simplest form of statistical analysis, which explores each variable independently. Descriptive statistics are rudimentary analysis techniques that help describe and summarize a variable’s data in a meaningful way. Descriptive statistics do not allow us to draw any conclusions beyond the available data but are helpful in interpreting the data at hand. There are two categories of descriptive statistics: (a) measures of central tendency describe the central position in a set of data; and (b) measures of spread describe how dispersed the data are. 5.1.1 Measures of Central Tendency Mean Perhaps the most intuitive measure of central tendency is the mean, which is often referred to as the average. The mean of a sample is denoted by \\(\\bar{x}\\) and is defined by: \\[ \\bar{x} = \\frac{\\displaystyle\\sum_{i=1}^{n} x_{i}}{n} \\] The population mean is denoted by \\(\\mu\\) and is defined by: \\[ \\mu = \\frac{\\displaystyle\\sum_{i=1}^{n} x_{i}}{N} \\] The mean of a set of numeric values can be calculated using the mean() function in R: # Fill vector x with integers x &lt;- c(1,1,1,2,2,2,3,3,4,50) # Calculate average of vector x mean(x) ## [1] 6.9 Median The median represents the midpoint in a sorted vector of numbers. For vectors with an even number of values, the median is the average of the middle two numbers; it is simply the middle number for vectors with an odd number of values. When the distribution of data is skewed, or there is an extreme value like we observe in vector x, the median is a better measure of central tendency. The median() function in R can be used to handle the sorting and midpoint selection: # Calculate median of vector x median(x) ## [1] 2 In this example, the median is only 2 while the mean is 6.9 (which is not representative of any of the values in vector \\(x\\)). Large deltas between mean and median values are evidence of outliers. Mode The mode is the most frequent number in a set of values. While mean() and median() are standard functions in R, mode() returns the internal storage mode of the object rather than the statistical mode of the data. We can easily create a function to return the statistical mode(s): # Create function to calculate statistical mode(s) stat.mode &lt;- function(x) { ux &lt;- unique(x) tab &lt;- tabulate(match(x, ux)) ux[tab == max(tab)] } # Return mode(s) of vector x stat.mode(x) ## [1] 1 2 In this case, we have a bimodal distribution since both 1 and 2 occur most frequently. Range The range is the difference between the maximum and minimum values in a set of numbers. The range() function in R returns the minimum and maximum numbers: # Return lowest and highest values of vector x range(x) ## [1] 1 50 We can leverage the max() and min() functions to calculate the difference between these values: # Calculate range of vector x max(x, na.rm = TRUE) - min(x, na.rm = TRUE) ## [1] 49 In people analytics, there are many conventional descriptive metrics – largely counts, percentages, and ratios cut by time series (day, month, quarter, year) and categorical dimensions (department, job, location, tenure band). Here is a sample of common measures: Time to Fill: average days between job requisition posting and offer acceptance Offer Acceptance Rate: percent of offers extended to candidates that are accepted Pass-Through Rate: percent of candidates in a particular stage of the recruiting process who passed through to the next stage Progress to Goal: percent of approved positions that have been filled cNPS/eNPS: candidate and employee NPS (-100 to 100) Headcount: counts and percent of workforce across worker types (employee, intern, contingent) Diversity: counts and percent of workforce across gender, ethnicity, and generational cohorts Positions: count and percent of open, committed, and filled seats Hires: counts and rates Career Moves: counts and rates Turnover: counts and rates (usually terms / average headcount over the period) Workforce Growth: net changes over time, accounting for hires, internal transfers, and exits Span of Control: ratio of people leaders to individual contributors Layers/Tiers: average and median number of layers removed from CEO Engagement: average score or top-box favorability score 5.1.2 Measures of Spread Variance Variance is a measure of variability in the data. Variance is calculated using the average of squared differences – or deviations – from the mean. Variance of a population is defined by: \\[ \\sigma^{2} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i}-\\mu)^{2}}{N} \\] Variance of a sample is defined by: \\[ s^{2} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i}-\\bar{x})^{2}}{n-1} \\] It is important to note that since differences are squared, the variance is always non-negative. In addition, we cannot compare these squared differences to the arithmetic mean since the units are different. For example, if we calculate the variance of annual compensation measured in USD, variance should be expressed as USD squared while the mean exists in the original USD unit of measurement. In R, the sample variance can be calculated using the var() function: # Load library for data wrangling library(dplyr) # Read employee data employees &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv&quot;) # Calculate sample variance for annual compensation var(employees$annual_comp) ## [1] 1788038934 Sample statistics are the default in R. Since the population variance differs from the sample variance by a factor of \\(s^2 (\\frac{n - 1}{n})\\), it is simple to convert output from var() to the population variance: # Store number of observations n = length(employees$annual_comp) # Calculate population variance for annual compensation var(employees$annual_comp) * (n - 1) / n ## [1] 1786822581 Standard Deviation The standard deviation is simply the square root of the variance. The standard deviation of a population is defined by: \\[ \\sigma = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i} - \\mu)^{2}}{N}} \\] The standard deviation of a sample is defined by: \\[ s = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i} - \\bar{x})^{2}}{n - 1}} \\] Since a squared value can be converted back to its original units by taking its square root, the standard deviation expresses variability around the mean in the variable’s original units. In R, the sample standard deviation can be calculated using the sd() function: # Calculate sample standard deviation for annual compensation sd(employees$annual_comp) ## [1] 42285.21 Since the population standard deviation differs from the sample standard deviation by a factor of \\(s \\sqrt \\frac{n - 1}{n}\\), it is simple to convert output from sd() to the population standard deviation: # Calculate population standard deviation for annual compensation sd(employees$annual_comp) * sqrt((n - 1) / n) ## [1] 42270.82 Quartiles Quartiles are a staple of exploratory data analysis (EDA). A quartile is a type of quantile that partitions data into four equally sized parts after ordering the data. Note that each partition is equally sized with respect to the number of data points – not the range of values in each. Quartiles are also related to percentiles. For example, Q1 is the 25th percentile – the value at or below which 25% of values lie. Percentiles are likely more familiar than quartiles, as percentiles show up in the height and weight measurements of babies, performance on standardized tests like the SAT and GRE, among other things. The Interquartile Range (IQR) represents the difference between Q3 and Q1 cut point values (the middle two quartiles). The IQR is sometimes used to detect extreme values in a distribution; values less than \\(Q1 - 1.5 * IQR\\) or greater than \\(Q3 + 1.5 * IQR\\) are generally considered outliers. In R, the quantile() function returns the values that bookend each quartile: # Return quartiles for annual compensation quantile(employees$annual_comp) ## 0% 25% 50% 75% 100% ## 62400 99840 137280 174200 208000 Based on this output, we know that 25% of people in our data earn annual compensation of 99,840 USD or less, 137,280 USD is the median annual compensation, and 75% of people earn annual compensation of 174,200 USD or less. Boxplots are a common way to visualize the distribution of data by categorical and ordinal factors. Boxplots are not usually found in presentations to stakeholders, since they are a bit more technical and often require explanation, but these are very useful to analysts for understanding data distributions during the EDA phase. In R, the ggplot2 library has robust and flexible data visualization capabilities which we will leverage throughout this book. Let’s visualize the spread of annual compensation by education level and gender using the ggplot() function: # Produce boxplots to visualize compensation distribution by education level and gender ggplot2::ggplot(employees, aes(x = as.factor(ed_lvl), y = annual_comp, color = gender)) + ggplot2::labs(x = &quot;Education Level&quot;, y = &quot;Annual Compensation&quot;) + ggplot2::guides(col = guide_legend(&quot;Gender&quot;)) + ggplot2::theme_bw() + ggplot2::geom_boxplot() Boxplots can be interpreted as follows: * Horizontal lines represent median compensation values. * The box in the middle of each distribution represents the IQR. * The end of the line above the IQR represents the threshold for outliers in the upper range: \\(Q3 + 1.5 * IQR\\). * The end of the line below the IQR represents the threshold for outliers in the lower range: \\(Q1 - 1.5 * IQR\\). * Data points represent outliers: \\(x &gt; Q3 + 1.5 * IQR\\) or \\(x &lt; Q1 - 1.5 * IQR\\). We can also return a specific percentile value using the probs argument in the quantile() function. For example, if we want to know the 80th percentile annual compensation value, we can execute the following: # Return 80th percentile annual compensation value quantile(employees$annual_comp, probs = .8) ## 80% ## 180960 In addition, the summary() function returns several common descriptive statistics for an object: # Return common descriptives summary(employees$annual_comp) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 62400 99840 137280 137054 174200 208000 Skewness Skewness is a measure of the horizontal distance between the mode and mean – a representation of symmetric distortion. In most practical settings, data are not normally distributed. That is, the data are skewed either positively (right-tailed distribution) or negatively (left-tailed distribution). The coefficient of skewness is one of many ways in which we can ascertain the degree of skew in the data. The skewness of sample data is defined as: \\[ Sk = \\frac{1}{n} \\frac{\\displaystyle\\sum_{i=1}^{n} (x_i-\\bar{x})^3}{s^3} \\] A positive skewness coefficient indicates positive skew, while a negative coefficient indicates negative skew. The order of descriptive statistics can also be leveraged to ascertain the direction of skew in the data: Positive skewness: mode &lt; median &lt; mean Negative skewness: mode &gt; median &gt; mean Symmetrical distribution: mode = median = mean Figure 5.1 illustrates the placement of these descriptive statistics in each of the three types of distributions: Figure 5.1: Skewness The magnitude of skewness can be determined by measuring the distance between the mode and mean relative to the variable’s scale. Alternatively, we can simply evaluate this using the coefficient of skewness: If skewness is between -0.5 - 0.5, the data are considered symmetrical. If skewness is between -0.5 and -1 or 0.5 and 1, the data are moderately skewed. If skewness is &lt; -1 or &gt; 1, the data are highly skewed. Since there is not a base R function for skewness, we can leverage the moments library to calculate skewness: # Load library library(moments) # Calculate skewness for org tenure, rounded to two significant figures via the round() function round(moments::skewness(employees$org_tenure), 2) ## [1] 2.27 Statistical Moments, after which this library was named, play an important role in specifying the appropriate probability distribution for a set of data. Moments are a set of statistical parameters used to describe the characteristics of a distribution. Skewness is the third statistical moment in the set; hence the sum of cubed differences and cubic polynomial in the denominator of the formula above. The complete set of moments comprises: (1) expected value or mean, (2) variance and standard deviation, (3) skewness, and (4) kurtosis. We can verify that the skewness() function from the moments library returns the expected value (per the aforementioned formula) by validating against a manual calculation: # Store components of skewness calculation n = length(employees$org_tenure) x = employees$org_tenure x_bar = mean(employees$org_tenure) s = sd(employees$org_tenure) # Calculate skewness manually, rounded to two significant figures via the round() function round(1/n * (sum((x - x_bar)^3) / s^3), 2) ## [1] 2.27 A skewness coefficient of 2.27 indicates that organization tenure is positively skewed. We can visualize the data to confirm the expected right-tailed distribution: # Produce histogram to visualize sample distribution ggplot2::ggplot() + ggplot2::aes(employees$org_tenure) + ggplot2::labs(x = &quot;Organization Tenure&quot;, y = &quot;Density&quot;) + ggplot2::geom_histogram(aes(y = ..density..), fill = &quot;#414141&quot;) + ggplot2::geom_density(fill = &quot;#ADD8E6&quot;, alpha = 0.6) + ggplot2::theme_bw() Figure 5.2: Organization Tenure Distribution Kurtosis While skewness provides information on the symmetry of a distribution, kurtosis provides information on the heaviness of a distribution’s tails (“tailedness”). Kurtosis is the fourth statistical moment, defined by: \\[ K = \\frac{1}{n} \\frac{\\displaystyle\\sum_{i=1}^{n} (x_i-\\bar{x})^4}{s^4} \\] Note that the quartic functions characteristic of the fourth statistical moment are the only differences from the skewness formula we reviewed in the prior section (which featured cubic functions). The terms leptokurtic and platykurtic are often used to describe distributions with light and heavy tails, respectively. “Platy-” in platykurtic is the same root as “platypus”, and I’ve found it helpful to recall the characteristics of the flat platypus when characterizing frequency distributions as platkurtic (wide and flat) vs. its antithesis, leptokurtic (tall and skinny). The normal (or Gaussian) distribution is referred to as a mesokurtic distribution in the context of kurtosis. Figure 5.3 illustrates the three kurtosis categorizations: Figure 5.3: Kurtosis Kurtosis is measured relative to a normal distribution. Normal distributions have a kurtosis coefficient of 3. Therefore, the kurtosis coefficient is greater than 3 for leptokurtic distributions and less than 3 for platykurtic distributions. The moments library can also be used to calculate kurtosis in R: # Calculate kurtosis for org tenure, rounded to one significant figure round(moments::kurtosis(employees$org_tenure), 1) ## [1] 13.4 We can verify that the kurtosis() function returns the expected value (per the aforementioned formula) by validating against a manual calculation: # Calculate kurtosis manually, rounded to one significant figure round(1/n * (sum((x - x_bar)^4) / s^4), 1) ## [1] 13.4 Our kurtosis coefficient of 13.4 indicates a leptokurtic distribution which is supported by the visual in Figure 5.2. It is important not to characterize a distribution based on a single isolated metric; we need the complete set of statistical moments to fully understand the distribution of data. 5.2 Bivariate Analysis While univariate analysis explores each variable independently, bivariate analysis explores statistical relationships between two variables. 5.2.1 Covariance While variance provides an understanding of how values for a single variable vary, covariance is an unstandardized measure of how two variables vary together. Values can range from \\(-\\infty\\) to \\(+\\infty\\), and these values can be used to understand the direction of the linear relationship between variables. Positive covariance values indicate that the variables vary in the same direction (e.g., tend to increase or decrease together), while negative covariance values indicate that the variables vary in opposite directions (e.g., when one increases, the other decreases, or vice versa). Covariance of a sample is defined by: \\[ cov_{x,y} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{n-1} \\] It’s important to note that while covariance aids our understanding of the direction of the relationship between two variables, we cannot use it to understand the strength of the association since it is unstandardized. Due to differences in variables’ units of measurement, the strength of the relationship between two variables with large covariance could be weak, while the strength of the relationship between another pair of variables with small covariance could be strong. In R, we can compute the covariance between a pair of numeric variables by passing the two vectors into the cov() function: # Calculate sample covariance between annual compensation and age using complete observations (missing values will cause issues if not addressed) cov(employees$annual_comp, employees$age, use = &quot;complete.obs&quot;) ## [1] 9381.677 In this example, using the default Pearson method, the covariance between annual compensation and age is 9,381.7. The positive value indicates that annual compensation is generally higher for older employees and lower for younger employees. Just as we multiplied the sample variance by \\((n - 1) / n\\) to obtain the population variance, we can apply the same approach to convert the sample covariance returned by cov() to the population covariance: # Calculate population covariance between annual compensation and age cov(employees$annual_comp, employees$age, use = &quot;complete.obs&quot;) * (n - 1) / n ## [1] 9375.295 The examples thus far have only examined associations between two variables at a time. However, rather than looking at isolated pairwise relationships, we can produce a covariance matrix to surface associations among many variables by passing a dataframe or matrix object into the cov() function: # Generate a covariance matrix among select continuous variables cov(subset(employees, select = c(&quot;annual_comp&quot;, &quot;age&quot;, &quot;org_tenure&quot;, &quot;job_tenure&quot;, &quot;prior_emplr_cnt&quot;, &quot;commute_dist&quot;)), use = &quot;complete.obs&quot;) ## annual_comp age org_tenure job_tenure ## annual_comp 1.788039e+09 9381.6772019 -3921.9601469 -3693.1960749 ## age 9.381677e+03 83.4550488 17.9255146 7.0467503 ## org_tenure -3.921960e+03 17.9255146 39.7967987 16.9797312 ## job_tenure -3.693196e+03 7.0467503 16.9797312 13.1271220 ## prior_emplr_cnt 2.340406e+03 6.8377387 -1.8547177 -0.8213802 ## commute_dist 1.067158e+04 -0.1248728 0.7746438 0.5535206 ## prior_emplr_cnt commute_dist ## annual_comp 2340.4057552 10671.5790741 ## age 6.8377387 -0.1248728 ## org_tenure -1.8547177 0.7746438 ## job_tenure -0.8213802 0.5535206 ## prior_emplr_cnt 6.2400490 -0.5923586 ## commute_dist -0.5923586 65.7212510 Using the default Pearson method, the cov() function will return sample variances for each variable down the diagonal, since covariance is not applicable in the context of a variable with itself. We can confirm by producing the variance for age: # Return sample variance for age var(employees$age) ## [1] 83.45505 As expected, the variance for age (\\(s^{2} = 83.5\\)) matches the value found in the age x age cell of the covariance matrix. 5.2.2 Correlation Correlation is a scaled form of covariance. While covariance provides an unstandardized measure of the direction of a relationship between variables, correlation provides a standardized measure that can be used to quantify both the direction and strength of bivariate relationships. Correlation coefficients range from -1 to 1, where -1 indicates a perfectly negative association, 1 indicates a perfectly positive association, and 0 indicates the absence of an association. Pearson’s product-moment correlation coefficient \\(r\\) is defined by: \\[ r_{x,y} = \\frac{\\displaystyle\\sum_{i=1}^{n} (x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sqrt{\\displaystyle\\sum_{i=1}^{n} (x_{i}-\\bar{x})^2 \\displaystyle\\sum_{i=1}^{n} (y_{i}-\\bar{y})^2}} \\] In R, Pearson’s \\(r\\) can be calculated using the cor() function: # Calculate the correlation between annual compensation and age cor(employees$annual_comp, employees$age, use = &quot;complete.obs&quot;) ## [1] 0.02428654 While we already know that the relationship between annual compensation and age is positive based on the positive covariance coefficient, Pearson’s \\(r\\) of .02 indicates that the strength of the positive association is weak (\\(r\\) = 0 represents the absence of a relationship). Though there are no absolute rules for categorizing the strength of relationships, as thresholds often vary by domain, the following is a general rule of thumb for interpreting the strength of bivariate associations: Weak = Absolute value of correlation coefficients between 0 and .3 Moderate = Absolute value of correlation coefficients between .4 and .6 Strong = Absolute value of correlation coefficients between .7 and 1 There are several correlation coefficients, and the measurement scale of \\(x\\) and \\(y\\) determine the appropriate type: Figure 5.4: Proper Applications of Correlation Coefficients Pearson’s \\(r\\) can be used when both variables are measured on continuous scales or when one is continuous and the other is dichotomous (point-biserial correlation). When one or both variables are ordinal, we can leverage Spearman’s \\(\\rho\\) or Kendall’s \\(\\tau\\), which are both standardized nonparametric measures of the association between one or two rank-ordered variables. Let’s look at Spearman’s \\(\\rho\\), which is defined as: \\[ \\rho = 1 - {\\frac {6 \\sum d_i^2}{n(n^2 - 1)}} \\] We can override the default Pearson method in the cor() function to implement a specific form of rank correlation using the method argument: # Calculate the correlation between job level and education level using Spearman&#39;s method cor(employees$job_lvl, employees$ed_lvl, method = &quot;spearman&quot;, use = &quot;complete.obs&quot;) ## [1] 0.1074192 The \\(\\rho\\) coefficient of .11 indicates that the positive association between job level and education level is weak. We could also pass method = &quot;kendall&quot; to this cor() function to implement Kendall’s \\(\\tau\\). The Phi Coefficient (\\(\\phi\\)), sometimes referred to as the mean square contingency coefficient or Matthews correlation in ML, can be used to understand the association between two dichotomous variables. For a 2x2 table for two random variables \\(x\\) and \\(y\\): Figure 5.5: 2x2 Table for Random Variables x and y The \\(\\phi\\) coefficient is defined as: \\[ \\phi = {\\frac {(AD-BC)}{\\sqrt{(A+B)(C+D)(A+C)(B+D)}}} \\] To illustrate, let’s examine whether there is a relationship between gender and performance after transforming performance from its ordinal form to a dichotomous variable (high vs. low performance). We can leverage the psych library to calculate \\(\\phi\\) in R: # Set females to 1 and everything else to 0 employees$gender_code &lt;- ifelse(employees$gender == &#39;Female&#39;, 1, 0) # Set stock options to 1 if level &gt; 0 employees$stock_option_code &lt;- ifelse(employees$stock_opt_lvl &gt; 0, 1, 0) # Create a 2x2 contingency table contingency_tbl &lt;- table(employees$gender_code, employees$stock_option_code) # Calculate the Phi Coefficient between dichotomous variables psych::phi(contingency_tbl) ## [1] -0.01 \\(\\phi\\) is essentially 0, which means stock options are distributed equitably across gender categories (good news!). While there are not differences in the proportion of males and females who receive at least some stock options, examining whether there is equity in the amount of stock grants and refreshes may be a good next step. A correlation matrix can be produced to surface associations among many variables by passing a dataframe or matrix object into the cor() function: # Generate a correlation matrix among select continuous variables cor(subset(employees, select = c(&quot;annual_comp&quot;, &quot;age&quot;, &quot;org_tenure&quot;, &quot;job_tenure&quot;, &quot;prior_emplr_cnt&quot;, &quot;commute_dist&quot;)), use = &quot;complete.obs&quot;) ## annual_comp age org_tenure job_tenure prior_emplr_cnt ## annual_comp 1.00000000 0.02428654 -0.01470248 -0.02410622 0.02215688 ## age 0.02428654 1.00000000 0.31104359 0.21290106 0.29963476 ## org_tenure -0.01470248 0.31104359 1.00000000 0.74288567 -0.11769547 ## job_tenure -0.02410622 0.21290106 0.74288567 1.00000000 -0.09075393 ## prior_emplr_cnt 0.02215688 0.29963476 -0.11769547 -0.09075393 1.00000000 ## commute_dist 0.03113059 -0.00168612 0.01514695 0.01884500 -0.02925080 ## commute_dist ## annual_comp 0.03113059 ## age -0.00168612 ## org_tenure 0.01514695 ## job_tenure 0.01884500 ## prior_emplr_cnt -0.02925080 ## commute_dist 1.00000000 Based on this correlation matrix, most pairwise associations are weak with the exception of the relationship between org_tenure and job_tenure (\\(r = .7\\)). The values down the diagonal are 1 because these represent the correlation between each variable with itself. You may also notice that the information above and below the diagonal is identical and, therefore, redundant. A great R library for visualizing correlation matrices is corrplot. Several arguments can be specified for various visual representations of the relationships among variables: Figure 5.6: Corrplot correlation matrix The GGally library produces a variety of useful information, including correlation coefficients, bivariate scatterplots, and univariate distributions: Figure 5.7: GGpairs bivariate correlations and data distributions. We may find that these bivariate associations look quite different for certain business areas or jobs, assuming departments and jobs were created at different points in the company’s history. There is often a lot of noise in data at the broader company level, so understanding the nature and nuance of associations will be highly important. We will explore this further in the context of linear regression in Chapter 9. It’s important to remember that correlation is not causation. Correlations can be spurious (variables related by chance), and drawing conclusions based on bivariate associations alone – especially in the absence of sound theoretical underpinnings – can be dangerous. Figure 5.8: Correlation comic (Source: www.explainxkcd.com) Here are two examples of nearly perfect correlations between variables for which there is likely no true direct association: Figure 5.9: Correlation between Maine Divorce Rate and Margarine Consumption (r = .99) Figure 5.10: Correlation between Mozzarella Cheese Consumption and Civil Engineering Doctorate Conferrals (r = .96) In addition, covariance and correlation alone are not sufficient for determining whether an observed association in sample data is also present in the population. To understand the likelihood that patterns observed in sample data are also present in the larger population of interest, we need to move beyond descriptive measures. 5.3 Review Questions How does the mean and median compare with respect to sensitivity to extreme values (outliers)? What does the standard deviation tell us about the spread of data, and how does it compare to the variance? How does the order of the mean, median, and mode differ between positively and negatively skewed distributions? Do large covariance coefficients always indicate strong bivariate associations? Why or why not? What information is represented in boxplots? Do do quartiles relate to percentiles? What type of correlation coefficient should be used when evaluating the relationship between a pair of rank-ordered variables? What type of correlation coefficient should be used when evaluating the relationship between a pair of dichotomous variables? How would you characterize the shape of platykurtic, leptokurtic, and mesokurtic distributions? When using the Pearson method, what do the values down the diagonal of a covariance matrix represent? "],["inf-stats.html", "6 Statistical Inference 6.1 Introduction to Probability 6.2 Central Limit Theorem 6.3 Confidence Intervals 6.4 Review Questions", " 6 Statistical Inference The objective of inferential statistics is to make inferences – with some degree of confidence – about a population based on available sample data. Several related concepts are fundamental to this goal and will be covered here. 6.1 Introduction to Probability Randomness and uncertainty exist all around us. In probability theory, random phenomena refer to events or experiments whose outcomes cannot be predicted with certainty (Pishro-Nik, 2014). If you’ve taken a course in probability, there is a good chance you have considered the case of a fair coin flip – one of the most intuitive applications of probability. In the absence of information on how the coin is flipped, we cannot be certain of the outcome. What we can be certain of is that with a large number of coin flips, the proportion of heads will become increasingly close to 50%, or \\(\\frac{1}{2}\\). The Law of Large Numbers (LLN) is an important theorem for building an intuitive understanding of how probability relates to the statistical inference concepts we will cover. In the case of a fair coin flip, it is possible to observe many consecutive heads by chance. This is because small samples can lend to anomalies. However, as the number of flips increases, we will undoubtedly observe an increasing number of tails; we expect a roughly equal number of heads and tails with a large enough number of flips. 6.1.1 Probability Distributions Probability distributions are statistical functions that yield the probability of obtaining possible values for a random variable. Probabilities range from 0 to 1, where the probability of a definite event is 1 and the probability of an impossible event is 0. The empirical probability (or experimental probability) of an event is the fraction of times it occurred relative to the total number of repetitions. Since a probability distribution defines the likelihood of observing all possible outcomes of an event or experiment, the sum of all probabilities for all possible values must equal 1. For example, let’s look at how org tenure is distributed across employees. We can understand the general shape of the distribution using descriptive statistics: # Load library for data wrangling library(dplyr) # Read employee data employees &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv&quot;) # Produce descriptive stats for org tenure summary(employees$org_tenure) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 3.000 5.000 7.032 9.000 72.000 Comparing the higher mean value of 9.2 to the median value of 8 indicates there are larger values skewing the mean upward which can be seen in the Q3 and max values. Beyond descriptives, visuals are often helpful in understanding a variable’s distribution. As shown in Figure 6.1, it is clear that org tenure is positively skewed, and understanding the shape (or spread) of this distribution enables us to identify which values are most likely in order to estimate the likelihood of different results: # Visualize org tenure distribution ggplot2::ggplot() + ggplot2::aes(employees$org_tenure) + ggplot2::labs(x = &quot;Org Tenure&quot;, y = &quot;Density&quot;) + ggplot2::geom_histogram(aes(y = ..density..), fill = &quot;#414141&quot;) + ggplot2::geom_density(fill = &quot;#ADD8E6&quot;, alpha = 0.6) + ggplot2::theme_bw() Figure 6.1: Org Tenure Distribution You can likely image the shape of probability distributions for many common events. If we consider the probability of employees exiting an organization, the outcome is binary. That is, employees either leave or stay; there are no options between these extremes. However, the distribution of performance scores will look quite different. Most organizations have expected – or even forced – distributions in which an average rating is awarded most frequently and low and high performance ratings less frequently. This would start to look more like a bell curve as the number of performance levels increases. Just as we grouped variables into discrete and continuous categories in Chapter 3, this is also how probability distributions are categorized. If you read Chapter 3, you likely already have some a priori expectations about the characteristics of discrete and continuous distributions. The shape of a probability distribution is defined by parameters, which represent its essential properties (e.g., measures of central tendency and spread). These probability distributions underpin the many types of statistical tests covered in this book. Discrete Probability Distributions Discrete probability distributions, also known as Probability Mass Functions (PMF), can be leveraged to model different types of nominal and ordinal variables. Some common discrete distributions include: Bernoulli: probability of success or failure for a single trial with two outcomes Binomial: number of successes and failures in a sequence of independent trials with two outcomes (collection of Bernoulli trials) Multinomial: generalization of the binomial distribution for experiments with more than two outcomes Negative Binomial (Pascal): a version of the binomial distribution for a fixed number of trials (this is positively skewed despite what the name might suggest) Poisson: probability of a given number of events occurring over a specified period Geometric: special case of the negative binomial distribution that repeats trials until a success is observed (rather than a fixed number of times) Several functions are available in R to simulate PMFs. The precise shape of a distribution depends on the parameters, but we will simulate and visualize these common PMFs to illustrate differences in the general shape of each. First, let’s simulate the distributions by drawing 1,000 random values from each with a specified set of parameters: # Set seed for reproducible random distribution set.seed(1234) # Simulate bernoulli distribution bernoulli_dist &lt;- rbinom(1000, 1, prob = .5) # Simulate binomial distribution # Notice the important difference relative to the Bernoulli simulation (100 trials vs. 1) binomial_dist &lt;- rbinom(1000, 100, prob = .5) # Simulate negative binomial distribution nbinomial_dist &lt;- rnbinom(1000, 100, prob = .5) # Simulate multinomial distribution with varying probabilities per level multinomial_dist &lt;- rmultinom(1000, 4, prob = c(.4, .3, .2, .6)) # Simulate poisson distribution poisson_dist &lt;- rpois(1000, 10) # Simulate geometric distribution geometric_dist &lt;- rgeom(1000, prob = .2) Next, we will visualize each distribution: # Create user-defined function (UDF) to simplify probability distribution visualization # Function arguments: (1) data = object containing random distribution values; (2) type = &#39;discrete&#39; or &#39;continuous&#39; probability distribution; and (3) title = name of distribution dist.viz &lt;- function(data, type, title) { if (type == &quot;discrete&quot;){ # Discrete distribution viz &lt;- ggplot2::ggplot() + ggplot2::aes(data) + ggplot2::labs(title = paste(title), x = &quot;x&quot;, y = &quot;count&quot;) + ggplot2::geom_histogram(fill = &quot;#414141&quot;) + ggplot2::theme_bw() + ggplot2::theme(plot.title = element_text(hjust = 0.5)) } else { # Continuous distribution viz &lt;- ggplot2::ggplot() + ggplot2::aes(data) + ggplot2::labs(title = paste(title), x = &quot;x&quot;, y = &quot;density&quot;) + ggplot2::geom_histogram(aes(y = ..density..), fill = &quot;#414141&quot;) + ggplot2::geom_density(fill = &quot;#ADD8E6&quot;, alpha = 0.6) + ggplot2::theme_bw() + ggplot2::theme(plot.title = element_text(hjust = 0.5)) } return(viz) } # Call UDF to build visualizations and store to objects p_bernoulli &lt;- dist.viz(data = bernoulli_dist, type = &quot;discrete&quot;, title = &quot;Bernoulli&quot;) p_binomial &lt;- dist.viz(data = binomial_dist, type = &quot;discrete&quot;, title = &quot;Binomial&quot;) p_nbinomial &lt;- dist.viz(data = nbinomial_dist, type = &quot;discrete&quot;, title = &quot;Negative Binomial&quot;) p_multinomial &lt;- dist.viz(data = multinomial_dist, type = &quot;discrete&quot;, title = &quot;Multinomial&quot;) p_poisson &lt;- dist.viz(data = poisson_dist, type = &quot;discrete&quot;, title = &quot;Poisson&quot;) p_geometric &lt;- dist.viz(data = geometric_dist, type = &quot;discrete&quot;, title = &quot;Geometric&quot;) # Display distribution visualizations ggpubr::ggarrange(p_bernoulli, p_binomial, p_nbinomial, p_multinomial, p_poisson, p_geometric, ncol = 3, nrow = 2) Figure 6.2: Discrete Probability Distributions Continuous Probability Distributions Continuous probability distributions, also known as Probability Density Functions (PDF), can be leveraged to model different types of interval and ratio variables. Some common continuous distributions include: Normal (Gaussian): distribution characterized by a mean and standard deviation for which the mean, median, and mode are equal Uniform: values of a random variable with equal probabilities of occurring Log-Normal: normal distribution of log-transformed values Student’s T: similar to the normal distribution but with thicker tails (approaches normal as \\(n\\) increases) Chi-Square: similar to the t distribution in that the shape approaches normal as \\(n\\) increases F: developed to examine variances from random samples taken from two independent normal populations A number of functions are available in R to simulate PDFs. While the precise shape of a distribution depends on the parameters, we will simulate and visualize these common PDFs to illustrate differences in the general shape of each. Let’s first draw 1,000 random values from each distribution with a specified set of parameters: # Set seed for reproducible random distribution set.seed(1234) # Simulate normal distribution normal_dist &lt;- rnorm(1000, mean = 50, sd = 5) # Simulate log-normal distribution lnormal_dist &lt;- rlnorm(1000, meanlog = 0, sdlog = 1) # Simulate uniform distribution uniform_dist &lt;- runif(1000, min = 1, max = 100) # Simulate student&#39;s t distribution t_dist &lt;- rt(1000, df = 5) # Simulate chi-square distribution chisq_dist &lt;- rchisq(1000, df = 5) # Simulate F distribution f_dist &lt;- rf(1000, df1 = 5, df2 = 200) Next, we will visualize each distribution. Since these continuous distributions are probability density functions, we will superimpose density plots over each: # Call UDF to build visualizations and store to objects # Note that as long as the arguments are in the order specified in the function (see our UDF definition above), the argument names do not need to be specified. To illustrate, we will drop the argument names from these function calls: p_normal &lt;- dist.viz(normal_dist, &quot;continuous&quot;, &quot;Normal&quot;) p_lnormal &lt;- dist.viz(lnormal_dist, &quot;continuous&quot;, &quot;Log-Normal&quot;) p_uniform &lt;- dist.viz(uniform_dist, &quot;continuous&quot;, &quot;Uniform&quot;) p_t &lt;- dist.viz(t_dist, &quot;continuous&quot;, &quot;Student&#39;s T&quot;) p_chisq &lt;- dist.viz(chisq_dist, &quot;continuous&quot;, &quot;Chi-Square&quot;) p_f &lt;- dist.viz(f_dist, &quot;continuous&quot;, &quot;F&quot;) # Display distribution visualizations ggpubr::ggarrange(p_normal, p_lnormal, p_uniform, p_t, p_chisq, p_f, ncol = 3, nrow = 2) Figure 6.3: Continuous Probability Distributions The distribution of data is critically important in statistics. The accuracy of many statistical tests is based on assumptions rooted in underlying data distributions, and violating these assumptions can result in serious errors due to misaligned probability distributions. Though there are many more discrete and continuous probability distributions, we will leverage several of these common types to assess the likelihood of differences, effects, and associations in later chapters of this book. 6.1.2 Conditional Probability Conditional probability reflects the probability conditioned on the occurrence of a previous event or outcome. For example, we may find that the proportion of heads is greater or less than \\(\\frac{1}{2}\\) with a large number of fair coin flips when the coin is consistently heads up when flipped. The outcome is, therefore, conditioned on the fixed – rather than random – positioning of the coin when flipped. Formally, Bayes’ Theorem (alternatively, Bayes’ Rule) states that for any two events A and B wherein the probability of A is not 0 (\\(P(A) \\neq 0\\)): \\[ P(A \\vert B) = \\frac{P(B \\vert A) P(A)}{P(B)}, \\] where: \\(A\\) = an event \\(B\\) = another event \\(P(A|B)\\) = conditional probability that event A occurs, given event B occurs (posterior) \\(P(B|A)\\) = conditional probability that event B occurs, given event A occurs (likelihood) \\(P(B)\\) = normalizing constant, constraining the probability distribution to sum to 1 (evidence) \\(P(A)\\) = probability event A occurs before knowing if event B occurs (prior) Bayes’ Rule allows us to predict the outcome more accurately by conditioning the probability on known factors rather than assuming all events operate under the same conditions. Bayes’ Rule is pervasive in people analytics, as the probability of outcomes can vary widely when conditioned on a person’s age, tenure, education, job, perceptions, relationships, and many other factors. For example, if we consider a company with 100 terminations over a 12-month period and average headcount of 1,000, the probability of attrition not conditioned on any other factor is 10%, or \\(\\frac{1}{10}\\). Aside from trending this probability over time to identify if overall attrition is becoming more or less of a concern, this isn’t too helpful at the company level. However, if we condition the probability of attrition on an event – such as a recent manager exit – and find that the probability of attrition among those whose manager has left in the last six months is 70%, or \\(\\frac{7}{10}\\), this is far more actionable (and concerning). The Monty Hall Problem is an excellent example of how our intuition is often at odds with the laws of conditional probability. In the classic game show, Let’s Make a Deal, Monty Hall asks contestants to choose one of three closed doors. Behind one door is a prize while the other two doors contain nothing. After the contestant selects a door, Monty opens one of the other two doors which does not contain a prize. At this point, there are two closed doors: the door the contestant selected and another for which the content remains unknown. All that is known at this point is that the prize is behind one of the two closed doors. It is at this juncture that Monty introduces a twist by asking if the contestant would like to switch doors. Most assume that the two closed doors have an equal (50/50) chance of containing the prize, because we generally think of probabilities as independent, random events. However, this is incorrect. Contestants who switch from their original selection have a 66% chance (rather than 50%) of winning. This may be counterintuitive, because the brain wants to reduce the problem to a simple coin flip. There is a major difference between the Monty Hall problem and a coin flip; for two outcomes to have the same probability, randomness and independence are required. In the case of the Monty Hall problem, neither assumption is satisfied. When all three doors are closed, each has the same probability of being selected. The probability of choosing the door with a prize is .33. Monty’s knowledge of the door containing the prize does not impact the probability of selecting the winning door. This is because the choice is completely random given we have no information that would increase the probability of a door containing the prize. The process is no longer random when Monty uses his insider knowledge about the prize’s location and opens a door he knows does not contain the prize. The probabilities change. Since Monty will never show the door containing the prize, he is careful to always open a door that has nothing behind it. If he was not constrained by the requirement to not reveal the prize’s location and instead chose to open one of the remaining doors at random, the probabilities would be equal (and he may end up opening the door that contains the prize). Seeing is believing, so let’s prove this with a simulation in R: # Set seed for reproducible simulations set.seed(12345) # Set number of simulations trials = 10000 # Store switch/keep decisions decisions = c(&quot;switch&quot;, &quot;keep&quot;) # Store integer for each door doors = 1:3 # Initialize empty data frame for results results = NULL for (n in 1:trials){ for (decision in decisions){ # Select correct door correct_door &lt;- sample(doors, 1, replace = T) # Contestant chooses a door at random selected_door &lt;- sample(doors, 1, replace = T) # Open door that was neither selected by the contestant nor contains the prize # Choose one door to open if multiple remain without the prize (i.e., the contestant didn&#39;t initially select the door containing the prize) remaining_doors &lt;- which(!doors == correct_door &amp; !doors == selected_door) open_door &lt;- sample(remaining_doors, 1, replace = T) # Contestant makes decision to switch doors or keep with the originally selected door selected_door &lt;- ifelse(decision == &quot;switch&quot;, which(!doors == selected_door &amp; !doors == open_door), selected_door) # Store results in data frame results &lt;- rbind(results, cbind.data.frame( trial = n, decision = decision, result = ifelse(correct_door == selected_door, &quot;win&quot;, &quot;lose&quot;))) } } # Calculate percentage difference in wins for switch vs. keep decisions switch_wins &lt;- nrow(results[results$decision == &quot;switch&quot; &amp; results$result == &quot;win&quot;, ]) / nrow(results) * 100 keep_wins &lt;- nrow(results[results$decision == &quot;keep&quot; &amp; results$result == &quot;win&quot;, ]) / nrow(results) * 100 round((switch_wins - keep_wins) / keep_wins * 100, 0) ## [1] 45 As we can see, wins occur nearly 50% more often when contestants switch doors. This exercise hopefully demonstrates the importance of conditional probability and statistical assumptions like randomness. Also, if ever you find yourself playing Let’s Make a Deal, switch doors. 6.2 Central Limit Theorem The Central Limit Theorem (CLT) is a mainstay of statistics and probability and fundamental to understanding the mechanics of statistical inference. The CLT was initially coined by a French-born mathematician named Abraham De Moivre in the 1700s. While initially unpopular, it was later reintroduced and attracted new interest from theorists and academics (Daw &amp; Pearson, 1972). The CLT states that the average of independent random variables, when increased in number, tend to follow a normal (or Gaussian) distribution. The distribution of sample means approaches a normal distribution regardless of the shape of the population distribution from which the samples are drawn. This is important because the normal distribution has properties that can be used to test the likelihood that an observed value, difference, or relationship in a sample is also present in the population. Figure 6.4: The Empirical Rule Let’s begin with an intuitive example of CLT. Imagine that we have a reliable way to measure how fun a population is on a 100-point scale, where 100 indicates maximum fun (life of the party) and 1 indicates maximum boringness. Consider that a small statistics conference is in progress at a nearby convention center, and there are 40 statisticians in attendance. In a separate room at the same convention center, there is also a group of 40 random people (non-statisticians) who are gathered to discuss some less interesting topic. Our job is to walk into one of the rooms and determine – based on the “fun” factor alone – whether we have entered the statistics conference or the other, less interesting gathering of non-statisticians. Instinctively, we already know the statisticians will be more fun than the other group. However, let’s assume we need the mean fun score and standard deviation of these two groups for this example. The group of statisticians have, on average, a fun score of 85 with a standard deviation of 2, while the group of non-statisticians are a bit less fun with a mean score of 65 and a standard deviation of 4. With a known population mean and standard deviation, the standard error (the standard deviation of the sample means) provides the ability to calculate the probability that the sample (the room of 40 people) belongs to the population of interest (fellow statisticians). Herein lies the beauty of the CLT: roughly 68 percent of sample means will lie within one standard error of the population mean, roughly 95 percent within two standard errors of the population mean, and roughly 99 percent within three standard errors of the population mean. Therefore, any room whose members have an average fun score that is not within two standard errors of the population mean (between 81 and 89 for our statisticians) is statistically unlikely to be the group of statisticians for which we are searching. This is because in less than 5 in 100 cases could we randomly draw a ‘reasonably sized’ sample of statisticians with average funness so extremely different from the population average. Because small samples lend to anomalies, we could – by chance – select a single person who happens to fall in the tails (extremely boring or extremely fun); however, as the sample size increases, it becomes more and more likely that the observed average reflects the average of the larger population. It would be virtually impossible (in less than 1 in 100 cases) to draw a random sample of statisticians from the population with average funness that is not within three standard errors of the population mean (between 79 and 91). Therefore, if we find that the room of people have an average fun score of 75, we will likely have far more fun in the other room! Let’s now see the CLT in action by simulating a random uniform population distribution from which we can draw random samples. Remember, the shape of the population distribution does not matter; we could simulate an Exponential, Gamma, Poisson, Binomial, or other distribution and observe the same behavior. # Set seed for reproducible random distribution set.seed(1234) # Generate uniform population distribution with 1000 values ranging from 1 to 100 rand.unif &lt;- runif(1000, min = 1, max = 100) # Calculate population mean mean(rand.unif) ## [1] 51.22007 # Calculate population variance N = length(rand.unif) var(rand.unif) * (N - 1) / N ## [1] 830.3155 # Produce histogram to visualize population distribution ggplot2::ggplot() + ggplot2::aes(rand.unif) + ggplot2::labs(x = &quot;x&quot;, y = &quot;Density&quot;) + ggplot2::geom_histogram(aes(y = ..density..), fill = &quot;#414141&quot;) + ggplot2::geom_density(fill = &quot;#ADD8E6&quot;, alpha = 0.6) + ggplot2::theme_bw() Figure 6.5: Uniform Population Distribution (N = 1000) As expected, these randomly generated data are uniformly distributed. Next, we will draw 100 random samples of various sizes and plot the average of each. # Define number of samples to draw from population distribution samples &lt;- 10000 # Populate vector with sample sizes sample_n &lt;- c(1:5,10,25,50) # Initialize empty data frame to hold sample means sample_means = NULL # Set seed for reproducible random samples set.seed(456) # For each n, draw random samples for (n in sample_n) { for (draw in 1:samples) { # Store sample means in data frame sample_means &lt;- rbind(sample_means, cbind.data.frame( n = n, x_bar = mean(sample(rand.unif, n, replace = TRUE, prob = NULL)))) } } # Produce histograms to visualize distributions of sample means, grouped by n-count sample_means %&gt;% ggplot2::ggplot() + ggplot2::aes(x = x_bar, fill = n) + ggplot2::labs(x = &quot;x-bar&quot;, y = &quot;Density&quot;) + ggplot2::geom_histogram(aes(y = ..density..), fill = &quot;#414141&quot;) + ggplot2::geom_density(fill = &quot;#ADD8E6&quot;, alpha = 0.6) + ggplot2::theme_bw() + ggplot2::facet_wrap(~n) Figure 6.6: Distribution of 10,000 Sample Means of Varied Size Per the CLT, we can see that as n increases, the sample means become more normally distributed. 6.3 Confidence Intervals A Confidence Interval (CI) is a range of values that likely contains the value of an unknown population parameter. These unknown population parameters are often \\(\\mu\\) or \\(\\sigma\\), though we will also leverage CIs in later chapters for regression coefficients, proportions, rates, and differences. If we draw random samples from a population, we can compute a CI for each sample. Building on the CLT, for a given confidence level (usually 95%, though 99% or 90% are sometimes used), the specified percent of sample intervals is expected to include the estimated population parameter. For example, for a 95% CI we would expect 19 in every 20 (or 95 in every 100) intervals across the samples to include the true population parameter. This is illustrated in Figure 6.7: Figure 6.7: Intervals for 95 Percent Confidence Level It is important to note that CIs should not be applied to the distribution of sample values; CIs relate to population parameters. A common misinterpretation of a CI is that it represents an interval within which a certain percent of sample values exists, and this is inaccurate. Because this misinterpretation is so prevalent, there is a good chance you will be tested on your understanding of CIs when applying to positions involving statistical analyses! A related concept that is fundamental to estimating CIs is the standard error (SE), which is the standard deviation of sample means. While the standard deviation is a measure of variability for a random variable, the variability captured by the SE reflects how well a sample represents the population. Since sample statistics will approach the actual population parameters as the size of the sample increases, the SE and sample size are inversely related; that is, the SE decreases as the sample size increases. The SE is defined by: \\[ SE = \\frac{\\sigma}{\\sqrt{n}} \\] Since the CLT is fundamental to inferential statistics, let’s validate that our simulated distribution of sample means adheres to the properties of normally distributed data per the Empirical Rule: # Store sample means with n = 50 x_bars &lt;- sample_means[sample_means$n == 50, &quot;x_bar&quot;] # Store sample size n &lt;- length(x_bars) # Calculate percent of sample means within +/- 2 SEs length(subset(x_bars, x_bars &lt; mean(x_bars) + 2 * sd(x_bars) &amp; x_bars &gt; mean(x_bars) - 2 * sd(x_bars))) / n * 100 ## [1] 95.35 95% of sample means are within 2 SEs, which is what we expect per the characteristics of the normal distribution. # Calculate percent of sample means within +/- 3 SEs length(subset(x_bars, x_bars &lt; mean(x_bars) + 3 * sd(x_bars) &amp; x_bars &gt; mean(x_bars) - 3 * sd(x_bars))) / n * 100 ## [1] 99.79 Nearly all of the sample means are within 3 SEs, indicating that it would be highly unlikely – nearly impossible even – to observe a sample mean ‘from the same population’ that falls outside this interval. Now, let’s illustrate the relationship between CIs and standard errors using sample data from our uniform population distribution. In our example, both \\(\\mu\\) and \\(\\sigma\\) are known and our sample size \\(n\\) is at least 30; therefore, we can use a Z-Test to calculate the 95% CI. A \\(z\\) score of 1.96 corresponds to the 95% CI for a two-tailed distribution; that is, we are looking for significantly different values in either the larger or smaller direction. The 95% CI represents the range of values we would expect to include \\(\\mu\\) in at least 95 of 100 random samples taken from the population. The CI in this case is defined by: \\[ CI = \\bar{x} \\pm z_{\\alpha/_2} \\frac{\\sigma}{\\sqrt{n}} \\] Let’s randomly take \\(n\\) = 100 from the population, and compute sample statistics to estimate the 95% CI: # Set seed for reproducible random samples set.seed(456) # Sample 100 values from uniform population distribution x &lt;- sample(rand.unif, 100, replace = TRUE, prob = NULL) # Calculate 95% CI ci95_lower_bound &lt;- mean(x) - 1.96 * (sd(x) / sqrt(100)) ci95_upper_bound &lt;- mean(x) + 1.96 * (sd(x) / sqrt(100)) # Print lower bound for 95% CI ci95_lower_bound ## [1] 47.90733 # Print upper bound for 95% CI ci95_upper_bound ## [1] 58.98773 Our known \\(\\mu\\) is 51.2, which is covered by our 95% CI (47.9 - 59.0). Per the CLT, in less than 5% of cases would we expect to draw a random sample from the population that results in a 95% CI which does not include \\(\\mu\\). Note that our CI narrows with larger samples since our confidence that the range includes \\(\\mu\\) increases with more data. Next, let’s look at a 99% CI. We will enter 2.576 for \\(z\\): # Calculate 99% CI ci99_lower_bound &lt;- mean(x) - 2.576 * (sd(x) / sqrt(100)) ci99_upper_bound &lt;- mean(x) + 2.576 * (sd(x) / sqrt(100)) # Print lower bound for 99% CI ci99_lower_bound ## [1] 46.16612 # Print upper bound for 99% CI ci99_upper_bound ## [1] 60.72893 Like the 95% CI, this slightly wider 99% CI (46.2 - 60.7) also includes our \\(\\mu\\) of 51.2. If \\(\\sigma\\) is not known, and/or we have a small sample (\\(n\\) &lt; 30), we need to use a \\(t\\)-test to calculate the CIs. In a people analytics setting, the reality is that population parameters are often unknown. For example, if we knew how engagement scores vary in the employee population, there would be no need to survey a sample of employees and make inferences about said population. As we will see, the \\(t\\)-test underpins many statistical tests and models germane to the people analytics discipline since we are often working with small datasets, so it is important to understand the mechanics. As shown in Figure 6.8, the \\(t\\) distribution is increasingly wider and shorter relative to the normal distribution as the sample size decreases; this is also characteristic of the sampling distribution of means for smaller samples we observed in our CLT example. Specifically, degrees of freedom (df) is used to determine the shape of the probability distribution. Degrees of freedom represents the number of observations in the data that are free to vary when estimating statistical parameters, which is a function of the sample size (\\(n - 1\\)). For example, if we could choose 1 of 5 projects to work on each day between Monday and Friday, we would only be able to choose 4 out of the 5 days; on Friday, only 1 project would remain to be selected, so our degrees of freedom (the number of days in which we have a choice between projects) would be 4. Figure 6.8: t Distribution Shape by Degrees of Freedom When estimating the CI for smaller samples, we need to leverage the wider, more platykurtic \\(t\\) distribution to achieve greater accuracy. Therefore, the CI for a two-tailed test in this case is defined by: \\[ CI = \\bar{x} \\pm t_{\\alpha/_2} \\frac{\\sigma}{\\sqrt{n}} \\] Let’s compare CIs calculated using a \\(t\\)-test to those calculated using the Z-Test. While a fixed \\(z\\) score can be used for each CI level when \\(n\\) &gt; 30, the \\(t\\) statistic varies based on both the CI level and \\(df\\). Though R will determine the correct \\(t\\) statistic for us, let’s reference the table shown in Figure 6.9 to manually lookup the \\(t\\) statistic: Figure 6.9: Critical Values of Student’s t Distribution For illustrative purposes, let’s draw a smaller sample of \\(n\\) = 25 from our uniform population distribution and calculate the 95% CI using the \\(t\\) statistic from the table (\\(df\\) = 24). The \\(t\\) statistic for this CI and \\(df\\) is 2.064: # Set seed for reproducible random samples set.seed(456) # Sample 25 values from uniform population distribution x &lt;- sample(rand.unif, 25, replace = TRUE, prob = NULL) # Calculate 95% CI ci95_lower_bound &lt;- mean(x) - 2.064 * (sd(x) / sqrt(25)) ci95_upper_bound &lt;- mean(x) + 2.064 * (sd(x) / sqrt(25)) # Print lower bound for 95% CI ci95_lower_bound ## [1] 35.24305 # Print upper bound for 95% CI ci95_upper_bound ## [1] 59.60959 As expected, the 95% CI using the \\(t\\) statistic is much wider (35.2 - 59.6), acknowledging the increased uncertainty in estimating population parameters given the limited information in this smaller sample. To increase our confidence to the 99% level, the interval widens even further (30.9 - 63.9): # Calculate 99% CI ci99_lower_bound &lt;- mean(x) - 2.797 * (sd(x) / sqrt(25)) ci99_upper_bound &lt;- mean(x) + 2.797 * (sd(x) / sqrt(25)) # Print lower bound for 99% CI ci99_lower_bound ## [1] 30.91633 # Print upper bound for 99% CI ci99_upper_bound ## [1] 63.93631 6.3.1 Hypothesis Testing Hypothesis testing is how we leverage CIs to test whether a significant difference or relationship exists in the data. Sir Ronald Fisher invented what is known as the null hypothesis, which states that there is no relationship/difference; disprove me if you can! The null hypothesis is defined by: \\[ H_0: \\mu_A = \\mu_B \\] The objective of hypothesis testing is to determine if there is sufficient evidence to reject the null hypothesis in favor of an alternative hypothesis. The null hypothesis always states that there is ‘nothing’ of significance. For example, if we want to test whether an intervention has an effect on an outcome in a population, the null hypothesis states that there is no effect. If we want to test whether there is a difference in average scores between two groups in a population, the null hypothesis states that there is no difference. An alternative hypothesis may simply state that there is a difference or relationship in the population, or it may specify the expected direction (e.g., Population A has a significantly ‘larger’ or ‘smaller’ average value than Population B; Variable A is ‘positively’ or ‘negatively’ related to Variable B). Therefore, alternative hypotheses are defined by: \\[ H_A: \\mu_A \\neq \\mu_B \\] \\[ H_A: \\mu_A &lt; \\mu_B \\] \\[ H_A: \\mu_A &gt; \\mu_B \\] 6.3.2 Alpha The alpha level of a hypothesis test, denoted by \\(\\alpha\\), represents the probability of obtaining observed results due to chance if the null hypothesis is true. In other words, \\(\\alpha\\) is the probability of rejecting the null hypothesis (and therefore claiming that there is a significant difference or relationship) when in fact we should have failed to reject it because there is insufficient evidence to support the alternative hypothesis. \\(\\alpha\\) is often set at .05 but is sometimes set at a more rigorous .01, depending upon the context and tolerance for error. An \\(\\alpha\\) of .05 corresponds to a 95% CI (1 - .05), and .01 to a 99% CI (1 - .01). With non-directional alternative hypotheses, we must divide \\(\\alpha\\) by 2 (i.e., we could observe a significant result in either tail of the distribution), while one-tailed tests position the rejection region entirely within one tail based on what is being hypothesized. At the .05 level, we would conclude that a finding is statistically significant if the chance of observing a value at least as extreme as the one observed is less than 1 in 20 if the null hypothesis is true. Note that we observed this behavior with our simulated distribution of sample means. While we could observe more extreme values by chance with repeated attempts, in less than 1 in every 20 times would we expect a 95% CI that does not capture \\(\\mu\\). Moreover, in less than 1 in every 100 times should we expect a sample with a 99% CI that does not capture \\(\\mu\\). 6.3.3 Beta Another key value is Beta, denoted by \\(\\beta\\), which relates to the power of the analysis. Simply put, power reflects our ability to find a difference or relationship if there is one. Power is calculated by 1 - \\(\\beta\\). At this point, it should be intuitive that larger samples increase our chances of observing significant results. As we observed in the \\(t\\)-test example, CIs for small samples (\\(n\\) &lt; 30) are quite wide relative to those for large samples; therefore, the power of the analysis to detect significance is limited given how extremely different values of \\(x\\) must be to observe non-overlapping CIs. 6.3.4 Type I &amp; II Errors A Type I Error is a false positive, wherein we conclude that there is a significant difference or relationship when there is not. A Type II Error is a false negative, wherein we fail to capture a significant finding. \\(\\alpha\\) represents our chance of making a Type I Error, while \\(\\beta\\) represents our chance of making a Type II Error. I once had a professor explain that committing a Type I error is a shame, while committing a Type II error is a pity, and I’ve found this to be a helpful way to remember what each type of error represents. Figure 6.10: Type I and II Errors 6.3.5 \\(p\\)-Values In statistical tests, the p-value is referenced to determine whether the null hypothesis can be rejected. The p-value represents the probability of obtaining a result at least as extreme as the one observed if the null hypothesis is true. As a general rule, if \\(p\\) &lt; .05, we can confidently reject the null hypothesis and conclude that the observed difference or relationship was unlikely a chance observation. While statistical significance helps us understand the probability of observing results by chance when there is no difference or effect in the population, it does not tell us anything about the size of the difference or effect. Analysis should never be reduced to inspecting p-values; in fact, p-values have been the subject of much controversy among researchers and practitioners in recent years. Later chapters will cover how to interpret results of statistical tests to surface the story and determine if there is anything ‘practically’ significant among statistically significant findings. 6.3.6 Bonferroni Correction One caveat when leveraging a p-value to determine statistical significance is that when multiple testing is performed – that is, multiple tests using the same sample data – the probability of a Type I error increases by a factor equivalent to the number of tests performed. It’s important to note that there is not agreement among statisticians about how (or even whether) the p-value threshold for statistical significance needs to be adjusted to account for this increased risk. Nevertheless, we will cover this conservative approach for mitigating this risk. Thus far, we have only discussed statistical significance in the context of a per analysis error rate – that is, the probability of committing a Type I error for a single statistical test. However, when two or more tests are being conducted on the same sample, the familywise error rate is an important factor in determining statistical significance. The familywise error rate reflects the fact that as we conduct more and more analyses on the same sample, the probability of a Type I error across the set (or family) of analyses increases. The familywise error rate can be calculated by: \\[ \\alpha_{FW} = 1 - (1 - \\alpha_{PC})^C, \\] where \\(c\\) is equal to the number of comparisons (or statistical tests) performed, and \\(\\alpha_{PC}\\) is equal to the specified per analysis error rate (usually .05). For example, if \\(\\alpha\\) = .05 per analysis, the probability of a Type I error with three tests on the same data increases from 5% to 14.3%: \\(1 - (1 - .05)^3 = .143\\). The most common method of adjusting the familywise error rate down to the specified per analysis error rate is the Bonferroni Correction. To implement this correction, we can simply divide \\(\\alpha\\) by the number of analyses performed on the dataset – such as \\(\\alpha / 3 = .017\\) in the case of three analyses with \\(\\alpha = .05\\). This means that for each statistical test, we must achieve \\(p &lt; .017\\) to report a statistically significant result. An alternative which allows us to achieve the same number of statistically significant results is to multiply the unadjusted per analysis p-values for each statistical test by the number of tests. For example, if we run three statistical tests and receive \\(p = .014\\), \\(p = .047\\), and \\(p = .125\\), we would achieve one significant result with the first method (\\(p &lt; .017\\)) as well as with the alternative since the first statistical test satisfies the per analysis error rate (\\(p &lt; .05\\)): \\(p = .014 * 3 = .042\\). Perneger (1998) is one of many who oppose the use of the Bonferroni Correction, suggesting that these “adjustments are, at best, unnecessary and, at worst, deleterious to sound statistical inference.” The Bonferroni Correction is controversial among researchers because while applying the correction reduces the chance of a Type I error, it also increases the chance of a Type II error. Because this correction makes it more difficult to detect significant results, it is rare to find such a correction reported in published research, though research often involves multiple testing on the same sample. Perneger suggests that simply describing the statistical tests that were performed, and why, is sufficient for dealing with potential problems introduced by multiple testing. 6.4 Review Questions What are some examples of a null hypothesis? What is the difference between Type I and Type II errors? What is the primary purpose of inferential statistics, and how does it differ from descriptive statistics? What is the Central Limit Theorem (CLT), and why is it important? Is randomness a requirement for probabilistic methods? Why or why not? What does the Bonferroni Correction seek to achieve? What is a confidence interval (CI)? What are some examples of how the context influences what level of confidence is appropriate for statistical significance testing? Should we always use a 95 CI? When population parameters are unknown, which test would be appropriate for testing the following null hypothesis: \\(\\mu_A = \\mu_B\\)? According to the Empirical Rule, 95% of normally distributed data lie within how many standard deviations of the mean? "],["data-wrang-prep.html", "7 Data Wrangling and Preparation 7.1 Data Management 7.2 SQL 7.3 Data Screening &amp; Cleaning 7.4 One-Hot Encoding 7.5 Feature Engineering 7.6 Review Questions", " 7 Data Wrangling and Preparation To begin a data analysis, we must first access, integrate, and clean the relevant data. These tasks account for the majority of the work analytics professionals do. Figure 7.1: What Data Scientists Really Do 7.1 Data Management Data are extracted directly from the source systems in which they are generated or from downstream repositories such as a data lake, data warehouse, or data mart. A data lake stores myriad types of data – both structured and unstructured – in its native format until needed. Structured data refers to data that fits a predefined data model, such as hire dates formatted as MM/DD/YYYY or zip codes stored as a five-digit string. Unstructured data has no predefined data model, such as audio and video files, free-form performance review comments, emails, or digital exhaust from messaging tools like Slack; it is difficult to structure this type of data within a set of related tables. The main difference between a data lake and data warehouse is the type of data they are designed to store. A data warehouse (DW) is designed to support analytics across large collections of data, such as transactional data (e.g., point-of-sale systems), point-in-time snapshots (e.g., month-end close reports), survey responses, spreadsheets, and more. As shown in Figure 7.2, data in a DW are structured and organized into schemas of related tables to enhance the performance of queries spanning sets of large and diverse data. Figure 7.2 illustrates how worker, position, and recruiting schemas may be related. For example, a candidate submits a job application to a posted requisition, which is connected to an open position Finance approved as part of the company’s workforce plan; when the selected candidate is hired, they become a worker with one or many events (hire, promotion, transfer, termination) and activities (learning, performance appraisals, surveys) during their tenure with the company. Figure 7.2: Related Tables Organized within Schemas Tables in a DW are related using a set of keys. Each table needs a Primary Key (PK), which is a unique identifier for each row in the table. A PK may be a single column or multiple columns; a multi-column PK is often referred to as a composite key. It is generally best to leverage non-recyclable system-generated ids for PKs, as descriptors like names tend to be unreliable. A Foreign Key (FK) is a column whose values correspond to the values of a PK in another table. Referential integrity is the logical dependency of a FK on a PK, and these constraints protect against orphaned FK values in child tables by deleting PK values from an associated parent table. Figure 7.3 shows an Entity Relationship Diagram (ERD) that depicts PK/FK relationships among the Position, Worker, and Requisition tables. Notice that the PK of each related table shown in Figure 7.2 is listed as a FK. Figure 7.3: Entity Relationship Diagram (ERD) A DW may contain many different types of tables, but the two most common are Type 1 and Type 2 tables. These tables are sometimes referred to as slowly changing dimensions (SCD). A Type 1 table is overwritten on a regular cadence (usually daily) and contains no history – only current values. For example, a Type 1 table may contain the latest known attributes for each active and terminated worker such as job, location, and manager. A Type 1 table is convenient when only current information needs to be queried. A Type 2 table is a table in which a new record is inserted when a change occurs for one or more specified dimensions. Jobs, managers, and locations are examples of slowly changing dimensions but unlike the Type 1 table which is truncated and reloaded with the latest information, the Type 2 table houses a start date and end date for each worker and dimension to facilitate reporting and analysis on changes to attribute values over time. Figure 7.4 illustrates the design of a Type 2 SCD for an active worker’s job, manager, and location changes. As the data show, worker 123 was promoted from Data Analyst to Sr. Data Analyst 1.5 years after joining, began reporting to their original manager (456) after a short stint reporting to someone else (789), and has worked remotely throughout their tenure with the company. Note that the combination of end_date = '12/31/9999' and current_record = 'Y' indicate current attributes for active workers. For inactive workers, end_date is set to the worker’s termination date for rows that represent the last known attributes along with current_record = 'Y': Figure 7.4: Type 2 SCD While we could select rows where current_record = 'Y' to construct a view of the last known attributes for each worker in this table, the task would be unnecessarily complex. Using a Type 1 table simplifies the task and output since each dimension value is stored in a separate column and there is only one row per employee. Figure 7.5 shows how the current record for worker 123 would look in a Type 1 SCD: Figure 7.5: Type 1 SCD A data mart is a subset of a DW designed to easily deliver specific information to a certain set of users on a particular subject or for a well-defined use case. For example, in a people analytics context a diversity data mart could be developed to better isolate and secure restricted data such as gender and ethnicity. This data may be used to support diversity descriptives and trends for a limited audience approved by Data Privacy and Employment Law counsel based on justified business needs. 7.2 SQL Structured Query Language (SQL) is the most common language used to extract and wrangle data contained in a relational database. SQL is an essential skill for anyone working in analytics. When working with large datasets, it is best to filter records on the database side to avoid reading superfluous records into an analytics tool such as R only to then filter data to the relevant subset. For example, if we are performing an analysis on employees in the Engineering department, we should ideally filter to this subset on the database side rather than loading into R data on the entire workforce before filtering down to the relevant records. Fewer records can help enhance the performance of R scripts – especially when R is running on a local machine, such as a laptop, rather than on a more powerful server. Basics There are three main clauses in a SQL query: (a) SELECT, (b) FROM, and (c) WHERE. The SELECT and FROM clauses are required, though the optional WHERE clause is frequently needed. SELECT: Specifies the columns to include in the output FROM: Specifies the table(s) in which the relevant data are contained WHERE: Specifies the rows to search Despite the clauses being ordered as shown above (SELECT then FROM then WHERE), the FROM clause is the first to execute since we first need to identify the relevant table(s) before filtering rows and selecting columns. The SELECT clause is the last to execute. Additional clauses are available for grouping and sorting data. GROUP BY: Specifies the columns by which data should be grouped when using aggregate functions HAVING: Specifies conditions for filtering rows based on aggregate functions ORDER BY: Specifies how data should be sorted in the output When implementing aggregate functions in a SELECT clause, such as counting, summing, or averaging a numeric field, all other non-aggregated fields must be included in the GROUP BY clause. Though it is important to execute SQL queries directly on the database to minimize the amount of data read into R, we will use the sqldf library within R to demonstrate the mechanics of a SQL query for easily replicable examples. The sqldf library allows us to write SQL to query a data frame in R in lieu of a database. While the syntax of SQL may vary by database, the core structure of queries is universal. First, let’s load the data sets: # Load data sets employees &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv&quot;) status &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/status.csv&quot;) benefits &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/benefits.csv&quot;) demographics &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/demographics.csv&quot;) job &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/job.csv&quot;) payroll &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/payroll.csv&quot;) performance &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/performance.csv&quot;) prior_employment &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/prior_employment.csv&quot;) survey_response &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/survey_response.csv&quot;) tenure &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/tenure.csv&quot;) # Return row and column counts dim(employees) ## [1] 1470 35 Next, we will apply the sqldf() function to our data frame to extract specific rows and columns. In addition to the SELECT, FROM, and WHERE clauses, we will use the LIMIT clause to limit the number of rows that are displayed given the data frame’s size (\\(n = 1,470\\)). In a practical setting, the LIMIT clause is only used for efficient data profiling and troubleshooting, as we would not want to arbitrarily truncate a data set used for analysis. A best practice in writing SQL is to capitalize the names of clauses and functions and to use separate lines and indentation to make the SQL statements more readable: # Load SQL library library(sqldf) # Store SQL query as a character string using the paste() function sql_string &lt;- paste(&quot;SELECT employee_id FROM employees WHERE dept = &#39;Research &amp; Development&#39; LIMIT 10&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## employee_id ## 1 1002 ## 2 1003 ## 3 1004 ## 4 1005 ## 5 1006 ## 6 1007 ## 7 1008 ## 8 1009 ## 9 1010 ## 10 1011 This query returned a list of employee ids for employees in the Research &amp; Development department. Aggregate Functions Next, let’s take a look at average organization tenure by job for those in the Research &amp; Development department: # Store SQL query as a character string sql_string &lt;- paste(&quot;SELECT job_title, AVG(org_tenure) FROM employees WHERE dept = &#39;Research &amp; Development&#39; GROUP BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title AVG(org_tenure) ## 1 Healthcare Representative 8.618321 ## 2 Laboratory Technician 5.019305 ## 3 Manager 13.673077 ## 4 Manufacturing Director 7.600000 ## 5 Research Director 10.937500 ## 6 Research Scientist 5.113014 ## 7 Vice President 9.500000 There are 7 distinct job titles among employees in the Research &amp; Development department, and the average organization tenure for these ranges from 5 to 13.7 years. Since there could be a small number of employees in certain jobs, in which case average organization tenure may not be as meaningful, we can use the COUNT(*) function to count the number of rows for each group. In this case, COUNT(*) will return the number of employees in each job in the Research &amp; Development department. We will also apply column aliases via AS in the SELECT clause to assign different names to the output fields: # Store SQL query as a character string sql_string &lt;- paste(&quot;SELECT job_title, COUNT(*) AS employee_cnt, AVG(org_tenure) AS avg_org_tenure FROM employees WHERE dept = &#39;Research &amp; Development&#39; GROUP BY job_title ORDER BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title employee_cnt avg_org_tenure ## 1 Healthcare Representative 131 8.618321 ## 2 Laboratory Technician 259 5.019305 ## 3 Manager 52 13.673077 ## 4 Manufacturing Director 145 7.600000 ## 5 Research Director 80 10.937500 ## 6 Research Scientist 292 5.113014 ## 7 Vice President 2 9.500000 The output shows that there are only 2 Vice Presidents in the Research &amp; Development department, while other job titles are much more prevalent. Since relatively few employees are Vice Presidents, let’s use the HAVING clause to only show average organization tenure for Research &amp; Development department jobs with more than 10 employees. We can also use the ROUND() function to truncate average organization tenure to one significant digit: # Store SQL query as a character string sql_string &lt;- paste(&quot;SELECT job_title, COUNT(*) AS employee_cnt, ROUND(AVG(org_tenure), 1) AS avg_org_tenure FROM employees WHERE dept = &#39;Research &amp; Development&#39; GROUP BY job_title HAVING COUNT(*) &gt; 10 ORDER BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title employee_cnt avg_org_tenure ## 1 Healthcare Representative 131 8.6 ## 2 Laboratory Technician 259 5.0 ## 3 Manager 52 13.7 ## 4 Manufacturing Director 145 7.6 ## 5 Research Director 80 10.9 ## 6 Research Scientist 292 5.1 Joins In a practical setting, the required data are rarely contained within a single table. Therefore, we must query multiple tables and join them together. Figure 7.6 illustrates SQL join types using Venn diagrams: Figure 7.6: Types of SQL Joins The structure of SQL queries containing each type of join is represented below: LEFT INCLUSIVE # SELECT [Output Field List] # FROM A # LEFT OUTER JOIN B # ON A.Key = B.Key LEFT EXCLUSIVE # SELECT [Output Field List] # FROM A # LEFT OUTER JOIN B # ON A.Key = B.Key # WHERE B.Key IS NULL FULL OUTER INCLUSIVE # SELECT [Output Field List] # FROM A # FULL OUTER JOIN B # ON A.Key = B.Key FULL OUTER EXCLUSIVE # SELECT [Output Field List] # FROM A # FULL OUTER JOIN B # ON A.Key = B.Key # WHERE A.Key IS NULL OR B.Key IS NULL RIGHT INCLUSIVE # SELECT [Output Field List] # FROM A # RIGHT OUTER JOIN B # ON A.Key = B.Key RIGHT EXCLUSIVE # SELECT [Output Field List] # FROM A # LEFT OUTER JOIN B # ON A.Key = B.Key # WHERE A.Key IS NULL INNER JOIN # SELECT [Output Field List] # FROM A # INNER JOIN B # ON A.Key = B.Key To illustrate how SQL joins work, we will leverage three of the data sets used to produce the consolidated employees data set we have been using: job, tenure, and demographics. For many people analytics projects, employee id is the PK since this identifier for one employee should not be assigned to another employee – past, present, or future. Email or network id may also be a suitable PK. We will use the employee_id column in each of the three data frames to facilitate joins: Next, we will query these data frames to return the average organization tenure and average commute distance for employees in the Research &amp; Development department, grouped by jobs with more than 10 employees. For this, we will leverage an INNER JOIN, which will return records only for employee ids which are present in all three data frames. For example, if a record exists in demographics and tenure for a particular employee id, but there is no corresponding record in job, that employee id would not be included in the output. # Store SQL query as a character string sql_string &lt;- paste(&quot;SELECT job_title, COUNT(*) AS employee_cnt, ROUND(AVG(org_tenure), 1) AS avg_org_tenure, ROUND(AVG(commute_dist), 1) AS avg_commute_dist FROM demographics INNER JOIN tenure ON demographics.employee_id = tenure.employee_id INNER JOIN job ON demographics.employee_id = job.employee_id WHERE dept = &#39;Research &amp; Development&#39; GROUP BY job_title HAVING COUNT(*) &gt; 10 ORDER BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title employee_cnt avg_org_tenure avg_commute_dist ## 1 Healthcare Representative 131 8.6 9.8 ## 2 Laboratory Technician 259 5.0 9.4 ## 3 Manager 52 13.7 7.2 ## 4 Manufacturing Director 145 7.6 9.5 ## 5 Research Director 80 10.9 8.4 ## 6 Research Scientist 292 5.1 9.0 Note that the INNER JOIN in this SQL query was structured such that both tenure and job were joined to demographics via the employee_id column. We could have instead joined job to tenure since we joined tenure to demographics; this would have achieved the same result since all employee ids exist in each of the three data frames. If it were possible for all employee ids to exist in demographics but not in either tenure or job, we could leverage a LEFT JOIN to ensure all records from demographics are included in the output irrespective of whether they have matches in tenure or job: # Store SQL query as a character string sql_string &lt;- paste(&quot;SELECT job_title, COUNT(*) AS employee_cnt, ROUND(AVG(org_tenure), 1) AS avg_org_tenure, ROUND(AVG(commute_dist), 1) AS avg_commute_dist FROM demographics LEFT JOIN tenure ON demographics.employee_id = tenure.employee_id LEFT JOIN job ON demographics.employee_id = job.employee_id WHERE dept = &#39;Research &amp; Development&#39; GROUP BY job_title HAVING COUNT(*) &gt; 10 ORDER BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title employee_cnt avg_org_tenure avg_commute_dist ## 1 Healthcare Representative 131 8.6 9.8 ## 2 Laboratory Technician 259 5.0 9.4 ## 3 Manager 52 13.7 7.2 ## 4 Manufacturing Director 145 7.6 9.5 ## 5 Research Director 80 10.9 8.4 ## 6 Research Scientist 292 5.1 9.0 In this case, if demographics is the base data set which contains all employee ids (i.e., the ‘LEFT’ dataset), it is important for tenure and job to be joined to it. Joining job to tenure may result in information loss if an employee id exists in demographics and job but not in the intermediate tenure data set. When integrating data within R, the tidyverse provides a more efficient and parsimonious method of joining many data sets using various join types. The example below joins nine data sets into a single employees data frame using a left join on the employee_id column: # Load tidyverse library(tidyverse) employees &lt;- list(demographics, status, benefits, job, payroll, performance, prior_employment, survey_response, tenure) %&gt;% reduce(left_join, by = &quot;employee_id&quot;) Subqueries Subqueries are queries nested within other queries. Subqueries are often referred to as inner queries, while the main queries are referred to as outer queries. For example, if we are interested in performing an analysis on employees with more than a year of organization tenure, we can use a subquery to pass a list of employee ids that meet this criterion into the outer query for filtering. In this case, we would not need to include tenure in the join conditions of our main query: # Store SQL query as a character string # Note: Since employee_id exists in multiple data sets, we must name a specific data set when referencing it sql_string &lt;- paste(&quot;SELECT job_title, COUNT(*) AS employee_cnt, ROUND(AVG(commute_dist), 1) AS avg_commute_dist FROM demographics LEFT JOIN job ON demographics.employee_id = job.employee_id WHERE demographics.employee_id IN (SELECT employee_id FROM tenure WHERE org_tenure &gt; 1) AND dept = &#39;Research &amp; Development&#39; GROUP BY job_title HAVING COUNT(*) &gt; 10 ORDER BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title employee_cnt avg_commute_dist ## 1 Healthcare Representative 118 9.7 ## 2 Laboratory Technician 198 9.6 ## 3 Manager 49 6.9 ## 4 Manufacturing Director 133 9.7 ## 5 Research Director 74 8.3 ## 6 Research Scientist 238 9.2 Virtual Tables An alternative to a subquery is creating a virtual table in the FROM clause. When using an INNER JOIN to connect demographics to the virtual table ids, which provides a list of employee ids for those with more than a year of organization tenure, any records in demographics or job that do not relate to employees with at least a year of organization tenure will be dropped. This is true even though a LEFT JOIN is used to join job to demographics since records in demographics will be filtered based on employee_id matches in the virtual table. With this approach, our WHERE clause is limited to the department = 'Research &amp; Development' condition: # Store SQL query as a character string sql_string &lt;- paste(&quot;SELECT job_title, COUNT(*) AS employee_cnt, ROUND(AVG(commute_dist), 1) AS avg_commute_dist FROM demographics LEFT JOIN job ON demographics.employee_id = job.employee_id INNER JOIN (SELECT employee_id FROM tenure WHERE org_tenure &gt; 1) ids ON demographics.employee_id = ids.employee_id WHERE dept = &#39;Research &amp; Development&#39; GROUP BY job_title HAVING COUNT(*) &gt; 10 ORDER BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title employee_cnt avg_commute_dist ## 1 Healthcare Representative 118 9.7 ## 2 Laboratory Technician 198 9.6 ## 3 Manager 49 6.9 ## 4 Manufacturing Director 133 9.7 ## 5 Research Director 74 8.3 ## 6 Research Scientist 238 9.2 As you can see, the output of the query using a virtual table matches the results from the preceding approach that utilized a subquery. Window Functions Window functions are used for performing calculations over a set of rows without collapsing the records. Unlike the aggregate functions we’ve covered, window functions do not collapse rows into a single value; the calculated value is returned for each of the rows over which the calculation is performed. For example, we can assign an organization tenure rank by Research &amp; Development job using the RANK() and OVER() functions in the SELECT clause. The PARTITION BY argument functions like a GROUP BY clause but without collapsing rows, while the ORDER BY argument sorts the records in ascending (ASC) or descending (DESC) order for proper ranking: # Store SQL query as a character string # Limit output to 10 records since query does not collapse records sql_string &lt;- paste(&quot;SELECT demographics.employee_id, job_title, commute_dist, RANK () OVER (PARTITION BY job_title ORDER BY commute_dist DESC) AS commute_dist_rank FROM demographics LEFT JOIN job ON demographics.employee_id = job.employee_id INNER JOIN (SELECT employee_id FROM tenure WHERE org_tenure &gt; 1) ids ON demographics.employee_id = ids.employee_id WHERE dept = &#39;Research &amp; Development&#39; ORDER BY job_title, commute_dist_rank LIMIT 10&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## employee_id job_title commute_dist commute_dist_rank ## 1 2325 Healthcare Representative 29 1 ## 2 1414 Healthcare Representative 28 2 ## 3 1010 Healthcare Representative 27 3 ## 4 1573 Healthcare Representative 27 3 ## 5 2200 Healthcare Representative 26 5 ## 6 1730 Healthcare Representative 25 6 ## 7 1833 Healthcare Representative 25 6 ## 8 1993 Healthcare Representative 25 6 ## 9 2415 Healthcare Representative 25 6 ## 10 1164 Healthcare Representative 24 10 Notice that in the case of commute distance ties, the RANK() function assigns the same rank and then adds the number of ties to that rank to determine the rank for the next highest value of commute distance. We can also treat this query as a virtual table, and then filter on the derived commute_dist_rank field to return the highest commute distance for each job. We can add a DISTINCT() function in the SELECT clause to collapse jobs for which there are more than one employee with the max commute distance, and display the number of ties for each using the COUNT(*) function: # Store SQL query as a character string sql_string &lt;- paste(&quot;SELECT DISTINCT(job_title) AS job_title, COUNT(*) AS employee_count, commute_dist FROM (SELECT demographics.employee_id, job_title, commute_dist, RANK () OVER (PARTITION BY job_title ORDER BY commute_dist DESC) AS commute_dist_rank FROM demographics LEFT JOIN job ON demographics.employee_id = job.employee_id INNER JOIN (SELECT employee_id FROM tenure WHERE org_tenure &gt; 1) ids ON demographics.employee_id = ids.employee_id WHERE dept = &#39;Research &amp; Development&#39; ORDER BY job_title, commute_dist_rank) tbl WHERE tbl.commute_dist_rank = 1 GROUP BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title employee_count commute_dist ## 1 Healthcare Representative 1 29 ## 2 Laboratory Technician 6 29 ## 3 Manager 2 29 ## 4 Manufacturing Director 4 29 ## 5 Research Director 3 28 ## 6 Research Scientist 4 29 ## 7 Vice President 1 8 An alternative approach is to use a common table expression (CTE), which is the result set of a query that exists temporarily and only for use in a larger query. Like the virtual table example, CTEs do not persist data in objects or tables; the data exist only for the duration of the query. # Store SQL query as a character string sql_string &lt;- paste(&quot;WITH max_commute_job AS (SELECT demographics.employee_id, job_title, commute_dist, RANK () OVER (PARTITION BY job_title ORDER BY commute_dist DESC) AS commute_dist_rank FROM demographics LEFT JOIN job ON demographics.employee_id = job.employee_id INNER JOIN (SELECT employee_id FROM tenure WHERE org_tenure &gt; 1) ids ON demographics.employee_id = ids.employee_id WHERE dept = &#39;Research &amp; Development&#39; ORDER BY job_title, commute_dist_rank) SELECT DISTINCT(job_title) AS job_title, COUNT(*) AS employee_count, commute_dist FROM max_commute_job WHERE commute_dist_rank = 1 GROUP BY job_title&quot;) # Execute SQL query sqldf::sqldf(sql_string) ## job_title employee_count commute_dist ## 1 Healthcare Representative 1 29 ## 2 Laboratory Technician 6 29 ## 3 Manager 2 29 ## 4 Manufacturing Director 4 29 ## 5 Research Director 3 28 ## 6 Research Scientist 4 29 ## 7 Vice President 1 8 7.3 Data Screening &amp; Cleaning The initial data review process is sometimes referred to as exploratory data analysis (EDA). EDA involves investigating patterns, completeness, anomalies, and assumptions using summary statistics and graphical representations. A handy function in base R for initial data screening is summary(): # Summarize df summary(employees) ## employee_id age commute_dist ed_lvl ## Min. :1001 Min. :18.00 Min. : 1.000 Min. :1.000 ## 1st Qu.:1368 1st Qu.:30.00 1st Qu.: 2.000 1st Qu.:2.000 ## Median :1736 Median :36.00 Median : 7.000 Median :3.000 ## Mean :1736 Mean :36.92 Mean : 9.193 Mean :2.913 ## 3rd Qu.:2103 3rd Qu.:43.00 3rd Qu.:14.000 3rd Qu.:4.000 ## Max. :2470 Max. :60.00 Max. :29.000 Max. :5.000 ## ed_field gender marital_sts active ## Length:1470 Length:1470 Length:1470 Length:1470 ## Class :character Class :character Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character ## ## ## ## stock_opt_lvl trainings dept job_lvl ## Min. :0.0000 Min. :0.000 Length:1470 Min. :1.000 ## 1st Qu.:0.0000 1st Qu.:2.000 Class :character 1st Qu.:1.000 ## Median :1.0000 Median :3.000 Mode :character Median :2.000 ## Mean :0.7939 Mean :2.799 Mean :2.064 ## 3rd Qu.:1.0000 3rd Qu.:3.000 3rd Qu.:3.000 ## Max. :3.0000 Max. :6.000 Max. :5.000 ## job_title overtime business_travel hourly_rate ## Length:1470 Length:1470 Length:1470 Min. : 30.00 ## Class :character Class :character Class :character 1st Qu.: 48.00 ## Mode :character Mode :character Mode :character Median : 66.00 ## Mean : 65.89 ## 3rd Qu.: 83.75 ## Max. :100.00 ## daily_comp monthly_comp annual_comp standard_hrs salary_hike_pct ## Min. :240.0 Min. : 5200 Min. : 62400 Min. :80 Min. :11.00 ## 1st Qu.:384.0 1st Qu.: 8320 1st Qu.: 99840 1st Qu.:80 1st Qu.:12.00 ## Median :528.0 Median :11440 Median :137280 Median :80 Median :14.00 ## Mean :527.1 Mean :11421 Mean :137054 Mean :80 Mean :15.21 ## 3rd Qu.:670.0 3rd Qu.:14517 3rd Qu.:174200 3rd Qu.:80 3rd Qu.:18.00 ## Max. :800.0 Max. :17333 Max. :208000 Max. :80 Max. :25.00 ## perf_rating prior_emplr_cnt env_sat engagement job_sat ## Min. :3.000 Min. :0.000 Min. :1.000 Min. :1.00 Min. :1.000 ## 1st Qu.:3.000 1st Qu.:1.000 1st Qu.:2.000 1st Qu.:2.00 1st Qu.:2.000 ## Median :3.000 Median :2.000 Median :3.000 Median :3.00 Median :3.000 ## Mean :3.154 Mean :2.693 Mean :2.722 Mean :2.73 Mean :2.729 ## 3rd Qu.:3.000 3rd Qu.:4.000 3rd Qu.:4.000 3rd Qu.:3.00 3rd Qu.:4.000 ## Max. :4.000 Max. :9.000 Max. :4.000 Max. :4.00 Max. :4.000 ## rel_sat wl_balance work_exp org_tenure ## Min. :1.000 Min. :1.000 Min. : 0.00 Min. : 0.000 ## 1st Qu.:2.000 1st Qu.:2.000 1st Qu.: 6.00 1st Qu.: 3.000 ## Median :3.000 Median :2.000 Median :10.00 Median : 5.000 ## Mean :2.712 Mean :1.841 Mean :11.28 Mean : 7.032 ## 3rd Qu.:4.000 3rd Qu.:2.000 3rd Qu.:15.00 3rd Qu.: 9.000 ## Max. :4.000 Max. :2.000 Max. :40.00 Max. :72.000 ## job_tenure last_promo mgr_tenure ## Min. : 0.000 Min. : 0.000 Min. : 0.000 ## 1st Qu.: 2.000 1st Qu.: 0.000 1st Qu.: 2.000 ## Median : 3.000 Median : 1.000 Median : 3.000 ## Mean : 4.229 Mean : 2.188 Mean : 4.123 ## 3rd Qu.: 7.000 3rd Qu.: 3.000 3rd Qu.: 7.000 ## Max. :18.000 Max. :15.000 Max. :17.000 Note that fields with NA's contain missing values. Also, by default employee_id is treated as an integer in R, which is why descriptive statistics appropriate for numeric data are provided. employee_id should actually be treated as a character string since we will not perform any arithmetic operations using these ids. Missingness Before considering whether and how to handle missing data, it is important to distinguish between structural missingness and informative missingness (Kuhn &amp; Johnson, 2013). Structural missingness relates to data that are missing for a logical reason. For example, we would not expect a new joiner with a few days of tenure to have a performance score. Likewise, we would not expect an active employee who is not a rehire to have a termination date. Therefore, it would not make sense to define a value to address missing data in these cases. Informative missingness relates to missing data that is informative regarding an outcome of interest. For example, in a survey context we may find a relationship between missing values on manager effectiveness questions and unfavorability on a psychological safety scale. This may indicate that employees who are fearful of retaliation are uncomfortable providing honest feedback about their managers, while employees who feel it is safe to speak up about issues are more comfortable responding in prosocial ways. In some cases, we have the luxury of simply removing observations with missing values and using the remaining complete cases for analysis. However, since we are often working with wide datasets containing relatively few observations in people analytics, this may not be feasible. As we will cover in later chapters, sample size considerations are fundamental to achieving adequate power in statistical testing, so case removal is only possible with larger datasets. Also, elimination of many impacted records – even with a sufficiently large sample – may bias analyses if there are missingness patterns. Data imputation refers to the methods by which missing data are replaced with substituted values when case removal is not appropriate. The most common data imputation method is replacing missing values with a descriptive statistic such as the mean, median, or mode based on available data. For example, if most employees have an age in the system, the average, median, or most frequent age could be used in place of the cases with a missing age. To be more precise, the average, median, or most frequent age of those with similar characteristics may be used (e.g., similar years of experience, job, level). We would expect there to be less variability in age within a well-defined segment relative to the entire employee population, so this would likely be a more accurate substitute for an individual’s actual age. Let’s evaluate the employees data frame for missing annual_comp values using the logical is.na() function, and return values of variables relevant in determining one’s annual compensation: # Store original annual comp for sample employee orig_comp &lt;- subset(employees, employee_id == &#39;2176&#39;, select = annual_comp) # Force a NA in lieu of annual comp for illustrative purposes employees[employees$employee_id == &#39;2176&#39;, &#39;annual_comp&#39;] &lt;- NA # Return relevant employee characteristics where annual comp is missing subset(employees, is.na(annual_comp), select = c(employee_id, job_title, job_lvl)) ## employee_id job_title job_lvl ## 1176 2176 Manufacturing Director 2 Next, we can impute the average value of annual_comp based on employees with the same values for the relevant variables: # Return average annual comp for employees with similar characteristics, excluding employees with missing comp values imputed_comp &lt;- sapply(subset(employees, job_title == &#39;Manufacturing Director&#39; &amp; job_lvl == 2, select = annual_comp), mean, na.rm = TRUE) # Impute missing comp for relevant segment employees[employees$employee_id == &#39;2176&#39;, &#39;annual_comp&#39;] &lt;- imputed_comp # Display absolute difference between original and imputed comp round(abs(orig_comp - subset(employees, employee_id == &#39;2176&#39;, select = annual_comp)), 0) ## annual_comp ## 1176 1169 While this approach should help in demonstrating the mechanics of imputing a missing value on a case-by-case basis, a more scalable solution is needed for data with a large number of missing values across employees with different values of these variables. There are more sophisticated methods of data imputation that involve models to estimate missing values. Linear regression and K-Nearest Neighbor (KNN) models are commonly used for this and will be covered in later chapters. These modeling techniques leverage a similar approach to the method outlined above in that the target values of cases with similar characteristics to those with missing values are used to aid estimation. Multiple imputation combines the information from multiple data sets imputed using different methods with a goal of minimizing the potential bias introduced by a singular method of imputation. Outliers The treatment of outliers is a controversial topic. Appropriate methods for defining and addressing outliers are domain-specific, and there are many important considerations that should inform whether and how outliers should be treated. As discussed in Chapter 5, a common method of outlier detection is identifying values which fall outside the following interval: \\[I = Q1 - 1.5 * IQR; Q3 + 1.5 * IQR\\] Low Variability Variables with low variability often do not provide sufficient information for identifying patterns in data. For example, if we are interested in using information on stock options to understand why employees vary in their levels of retention risk, but find that the employee stock purchase plan (ESPP) terms are identical for nearly all employees, including a stock option variable in the analysis is unlikely to provide any meaningful signal. When working with survey data, checking for straightlining should be an early data screening step. Straightlining refers to a constant response across all survey items, which may be evidence that the respondent lost motivation or was not attentive and thoughtful when taking the survey. Since straight-line responses may influence results, it is often best to discard these cases – especially when the sample size is adequately large for the planned analyses without them. If the same response is given for both positively and negatively worded versions of a question (e.g., comparing “I plan to be here in a year” to “I do not plan to be here in a year”), which we expect to be inversely related, this gives added support for discarding these responses. Fields with low variability can be easily identified using descriptive statistics from the summary() function. If the Min and Max values are equal, there is no variability in the field’s values. Based on the following descriptives, we should remove standard_hrs from the data: # Return descriptives to understand distribution of standard hours summary(employees$standard_hrs) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 80 80 80 80 80 80 Given that the data dictionary in Chapter 1 indicates performance ratings range from 1 to 4, the following descriptives should raise red flags: # Return descriptives to understand distribution of standard hours summary(employees$perf_rating) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.000 3.000 3.000 3.154 3.000 4.000 Assuming not everyone in the company is a stellar performer (i.e., only Noteworthy and Exceptional ratings), we may be working with a partial data set that will likely bias analyses. This may be due to poor integrity of performance data in the source system or repository from which the data were pulled, or the data may have been limited via a condition in the query. Inconsistent Categories Inconsistent categories impact aggregation and trending by categorical dimensions. It is often necessary to create mappings based on logical rules in order to standardize dimension values across time. In the case of reorgs, a department may be disbanded, renamed, or integrated into one or multiple other departments. Therefore, when working with historical data, records may contain legacy department names that do not align with the current organizational taxonomy. Mapping from former to current departments may require logic based on manager ids, divisions, job profiles, locations, or other variables depending on the nature of reorgs over time. Job architecture projects often introduce the need for mappings as well. Jobs and levels may completely change for all employees with a job architecture revamp, in which case trending along job and level dimensions (e.g., attrition by job or level over multiple years) is only possible with logic that clarifies how legacy jobs and levels map to those in the new career framework. Changes to allowable values in source systems often result in inconsistent categorical data over time. For example, the education field may switch from a free-form text field in which employees can enter any value (e.g., B.S., B.A., BS, BA, Bachelor of Science, Bachelor of Arts, Bachelor’s, Bachelors, Bachelor’s Degree, Bachelor Degree, undergraduate degree, 4-year degree, four-year degree) to a standardized solution in which there is a clean and well-defined set of allowable values from which employees can choose (e.g., Bachelor’s Degree, Master’s Degree, Doctoral Degree). This warrants either a one-time historical cleanup upon implementing the allowable values or downstream logic to tidy up data for analytics. A best practice is to address data quality issues upstream (e.g., in the source system) to avoid duplicative data cleaning procedures across downstream applications. Data Binning Data binning refers to the process by which larger high-level groups of values are defined and constructed. As a general rule, extremely granular categories should be avoided – especially when there is no theoretical basis for such categories facilitating a project’s objectives or deepening insights. Where the \\(n\\)-count is expected to be consistently low for a defined categorical bin, it is usually best to define a larger bin. For example, a variable measuring highest level of educational attainment that contains 9th, 10th, 11th, and 12th grade categories may be converted into higher-level “High School Not Completed” and “High School Completed” bins. For modeling applications, it is important to let the algorithm determine the cutpoints for numeric data in relation to the outcome. For example, if organization tenure is measured in years, arbitrarily defining bin sizes of ‘Less Than 1 Year’, ‘1-5 Years’, and ‘More Than 5 Years’ will likely result in information loss. Any variability within these bins that may be useful in explaining variance in the outcome would be lost with such wide bins. In fact, machine learning (ML) models such as decision trees, which will be covered in later chapters, are great for algorithmically determining cut points for binning numeric data across descriptive, diagnostic, and predictive projects alike. 7.4 One-Hot Encoding One-hot encoding, also known as dummy coding, involves transforming a categorical variable into numeric values on which statistical procedures can be performed. For EDA, this is not required, as counts and percent of total metrics can be calculated on these dimensions for descriptive purposes. However, for statistical and ML modeling applications, categorical variables must be converted into \\(k-1\\) variables, where \\(k\\) is the number of categories, using binary (1/0) coding. Understanding how categorical data are coded is critical to a correct interpretation of output. For example, if a remote work variable exists with “Remote” or “Non-Remote” values, we may code “Remote” values as 1 and “Non-Remote” values as 0. We could then evaluate the statistical relationship of this transformed categorical variable with other numeric variables. If a categorical variable has more than 2 values, we must create a separate 1/0 field for each value and omit one category for use as a reference group. As we will cover in Chapter 9, one of several assumptions in linear regression is that independent variables are not collinear; that is no pair of independent variables is highly correlated. Without an omitted category, each of the one-hot encoded fields will be perfectly correlated with the others. That is, when the field representing category A is 1, the fields for other categories will always be 0. As illustrated in Figure 7.7, by omitting a category there will be cases when all fields have a 0 value (i.e., rows where the value is the omitted category), which will reduce the strength of the bivariate correlations. Figure 7.7: One-Hot Encoding For a categorical variable with only two values, the ifelse() function can be leveraged to assign values: # Return unique values of gender field unique(employees$gender) ## [1] &quot;Female&quot; &quot;Male&quot; # Gender one-hot encoding employees$gender_ohe &lt;- ifelse(employees$gender == &#39;Female&#39;, 1, 0) # Preview records head(subset(employees, select = c(employee_id, gender_ohe))) ## employee_id gender_ohe ## 1 1001 1 ## 2 1002 0 ## 3 1003 0 ## 4 1004 1 ## 5 1005 0 ## 6 1006 0 For variables with more than 2 categories, we can leverage the model.matrix() function for one-hot encoding. Let’s illustrate be encoding locations. # Return counts by department employees %&gt;% count(dept, sort = TRUE) ## dept n ## 1 Research &amp; Development 961 ## 2 Sales 446 ## 3 Human Resources 63 By default, the model.matrix() function will produce a matrix of 1/0 values for \\(k-1\\) categories. The first column in the matrix is an intercept column containing a value of 1 for each row to ensure linear independence, and the default behavior results in the first value of the factor being the omitted group. For more flexibility over which value is omitted, we can drop the intercept using -1 in the first argument passed to model.matrix() and then choose the reference group for the analysis in a subsequent step. # Department one-hot encoding dept_ohe &lt;- model.matrix(~dept-1, data = employees) # Preview data head(dept_ohe) ## deptHuman Resources deptResearch &amp; Development deptSales ## 1 0 0 1 ## 2 0 1 0 ## 3 0 1 0 ## 4 0 1 0 ## 5 0 1 0 ## 6 0 1 0 We will drop the department with the lowest \\(n\\) rather than the more arbitrary method based on the first value of the factor. Since departments are coded as either \\(1\\) or \\(0\\), we can use the colSums() function to sum each column and the which.min() function to identify which has the lowest sum (i.e., smallest department by employee count). # Drop department with lowest sum (lowest n-count) dept_ohe &lt;- dept_ohe[, -which.min(colSums(dept_ohe))] # Preview refined one-hot encoded data head(dept_ohe) ## deptResearch &amp; Development deptSales ## 1 0 1 ## 2 1 0 ## 3 1 0 ## 4 1 0 ## 5 1 0 ## 6 1 0 # Combine employees and matrix containing one-hot encoded departments employees &lt;- cbind(employees, dept_ohe) # Drop original department field employees &lt;- subset(employees, select = -c(dept)) 7.5 Feature Engineering Level one people analytics tends to utilize only the delivered fields from the HRIS (e.g., location, job profile, org tenure), but a good next step is to derive smarter variables from these fields. These can then be used to cut data differently or as inputs in models. Below are some examples of how basic data available in the HRIS can be transformed into new variables that provide different information. This can be easily accomplished using the arithmetic functions we’ve covered. Number of jobs per unit of tenure (larger proportions tend to see greater career pathing) Office/remote worker (binary variable dummy coded as 1/0) Local/remote manager (binary variable dummy coded as 1/0) Hire/Rehire (binary variable dummy coded as 1/0) Hired/acquired (proxy for culture shock effects) Gender isolation (ratio of employee’s gender to number of the same within immediate work group) Generation isolation (comparison of age bracket to most frequent generational bracket within immediate work group) Ethnic isolation (ratio of employee’s ethnicity to number of the same within immediate work group) Difference between employee and manager age Percentage change between last two performance appraisal scores (per competency and/or overall) Team and department quit outbreak indicators (ratio of terms over x months relative to average headcount over x months) Industry experience (binary or length in years) 7.6 Review Questions What are the differences between data lakes, data warehouses, and data marts? What is the difference between a Type 1 and Type 2 table in a DW? What two clauses must always be present in a SQL query? How do aggregate functions differ from window functions in SQL? What is a subquery? What is the difference between an INNER JOIN and LEFT JOIN? Why is it dangeous to address missing values without domain knowledge of how the data are generated? How can missing values be addressed when impacted records cannot be eliminated from a data set? When is one-hot encoding required for categorical variables? Why should variables with low to no variability be dropped? "],["aod.html", "8 Analysis of Differences 8.1 Parametric vs. Nonparametric Tests 8.2 Differences in Discrete Data 8.3 Differences in Continuous Data 8.4 Review Questions", " 8 Analysis of Differences There are many statistical tests that can be used to test for differences within or between two or more groups. This chapter will cover common contexts for differences in people analytics and the tests applicable to each. 8.1 Parametric vs. Nonparametric Tests In the context of data measured on a continuous scale (quantitative), we will cover parametric tests along with their nonparametric counterparts. When the hypothesis relates to average (mean) differences and \\(n\\) is large, parametric tests are preferred as they generally have more statistical power. Nonparametric tests are distribution-free tests that do not require the population’s distribution to be characterized by certain parameters, such as a normal distribution defined by a mean and standard deviation. Nonparametric tests are great for qualitative data since the distribution of non-numeric data cannot be characterized by parameters. Beyond ensuring the data were generated from a random and representative process as discussed in Chapter 3, as well as following the data screening procedures outlined in Chapter 7 (e.g., addressing concerning outliers), parametric tests of differences generally feature three key assumptions: Independence: Observations within each group are independent of each other Homogeneity of Variance: Variances of populations from which samples were drawn are equal Normality: Residuals must be normally distributed (with mean of 0) within each group While homogeneity of variance assumes the variances across multiple groups are equal, parametric tests are generally robust to violations of equal variances when the sample sizes are large. Also, you may recall that \\(\\mu\\) and \\(\\sigma\\) are sufficient to characterize a population distribution when data are situated symmetrically around the mean. However, the mean can be sensitive to outliers so if outliers are present in the data, the median may be a better way of representing the data’s center (i.e., nonparametric tests); just remember that the use of nonparametric tests requires hypotheses to be modified to adjust for median – rather than mean – centers. You may be wondering whether the magical elixir that is the CLT, which we covered in Chapter 6, influences our ability to utilize parametric tests with respect to the assumption of normality. It’s important to remember that the normal distribution properties under the CLT relate to the sampling distribution of means – not to the distribution of the population or to the data for an individual sample. The CLT is important for estimating population parameters, but it does not transform a population distribution from nonnormal to normal. If we know the population distribution is nonnormal (e.g., ordinal, nominal, or skewed data), nonparametric tests should be considered. This is why we used Spearman’s correlation coefficient – a nonparametric test – in Chapter 5 to evaluate the relationship between job level and education; these ordinal data are not normally distributed in the population. # Load employee data employees &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv&quot;) 8.2 Differences in Discrete Data Nonparametric tests are gernerally best when working with data measured on a discrete scale since these data do not come from normally distributed populations. The two most commonly used tests to analyze variables measured on a discrete scale are the nonparametric Chi-square test and Fisher’s exact test. Figure 8.1: Chi-Square and Fisher Exact Test Criteria for Discrete Data Both tests organize data within 2x2 contingency tables which enables us to understand interrelations between variables. Chi-Square Test The Chi-Square Test of Independence evaluates patterns of observations to determine if categories occur more frequently than we would expect by chance. The Chi-square statistic is defined by: \\[ {\\chi}^2 = \\sum\\frac{(O_i - E_i)^2}{E_i}, \\] where \\(O_i\\) is the observed value, and \\(E_i\\) is the expected value. \\(H_0\\) states that each variable is independent of one another (i.e., there is no relationship). In addition to the \\({\\chi}^2\\) test statistic, \\(df\\) for the contingency table, defined by \\(df = (rows - 1) * (columns - 1)\\), is required to determine whether we reject or fail to reject \\(H_0\\). While there is not consensus on the minimum sample size for this test, it is important to note that the \\({\\chi}^2\\) statistic follows a chi-square distribution asymptotically. This means we can only calculate accurate \\(p\\)-values for larger samples, and a general rule of thumb is that the expected value for each cell needs to be at least 5. The challenge with small \\(n\\)-counts is illustrated in Figure 8.2; the chi-square distribution approaches a vertical line as \\(df\\) drops below 5. Figure 8.2: Chi-Square Distributions by Degrees of Freedom (df) We will demonstrate how to perform a chi-square test of independence by evaluating whether exit rates are independent of whether an employee works overtime. Let’s first construct a 2x2 contingency table using the table() function: # Create contingency table cont_tbl &lt;- table(employees$active, employees$overtime) cont_tbl ## ## No Yes ## No 110 127 ## Yes 944 289 A mosaic plot is a great way to visualize the delta between expected and observed frequencies for each cell. This can be produced using the mosaicplot() function from the graphics library: Figure 8.3: Mosaic Plot of Residuals for Overtime x Active Status In Figure 8.3, blue indicates that the observed value is higher than the expected value, while red indicates that the observed value is lower than the expected value. Based on this plot, there appears to be some meaningful patterns and departures from expected values in both the high and low directions. There are more inactive employees than expected in the overtime group, and more active employees than expected in the group with no overtime. These large standardized residuals are indicative of meaningful relationships between the two categorical variables. Let’s run the chi-square test of independence to determine whether these residuals are statistically significant. This test can be performed in R by passing the contingency table into the chisq.test() function: # Perform chi-square test of independence chisq.test(cont_tbl) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: cont_tbl ## X-squared = 87.564, df = 1, p-value &lt; 2.2e-16 Based on the results, exit rates are not independent of overtime (\\({\\chi}^2(1) = 87.56, p &lt; .05\\)). Therefore, there is a statistically significant relationship between an employee working overtime and the rate at which they change from an active to inactive status – confirming what was evident in the mosaic plot. Fisher’s Exact Test When the sample size is small, Fisher’s Exact Test can be used to calculate the exact \\(p\\)-value rather than an approximation, which is the case with many statistical tests such as the chi-square test. \\(H_0\\) for Fisher’s exact test is the same as \\(H_0\\) for the chi-square test of independence: There is no relationship between the two categorical variables (i.e., they are independent). We can perform Fisher’s exact test using the fisher.test() function in R: # Perform Fisher&#39;s exact test fisher.test(cont_tbl) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: cont_tbl ## p-value &lt; 2.2e-16 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 0.1969101 0.3572582 ## sample estimates: ## odds ratio ## 0.2654384 Note the odds ratio shown in this output. The odds ratio represents the ratio of positive to negative cases, which is the ratio of overtime for active and inactive workers in this example. The odds ratio is defined by: \\[OR = \\frac{a*d}{b*c}\\] An odds ratio of 1 indicates no difference in overtime frequency between active and inactive workers. Figure 8.4 illustrates the cells for the odds ratio calculation for the 2x2 contingency table of overtime for active and inactive (termed) workers. Figure 8.4: 2x2 Contingency Table of Overtime for Active and Inactive Workers Since the 95% \\(CI\\) for the odds ratio does not include 1, we reject the null hypothesis and conclude that exit rates are related to working overtime; this is consistent with results from the chi-square test of independence. Since overtime was indicated far more often for inactive workers than for active workers, it is no surprise that the denominator of our ratio is larger than the numerator (i.e., \\(OR\\) &lt; 1). As discussed in Chapter 5, we can produce a \\(\\phi\\) coefficient to understand the strength of the association by passing the contingency table into the phi function from the psych library: # Calculate the Phi Coefficient psych::phi(cont_tbl) ## [1] -0.25 The relationship between active status and overtime is negative, and the strength of the relationship is weak (\\(\\phi\\) = -.25). Another common method of measuring the strength of the association between two categorical variables is Cramer’s V, which ranges from 0 (no association) to 1 (strong association). In the interest of not muddying the waters with an exhaustive set of alernative methods, the implementation won’t be covered. 8.3 Differences in Continuous Data A variety of parametric and nonparametric tests are available for evaluating differences between variables measured on a continuous scale. Figure 8.5 provides a side-by-side of these parametric and corresponding nonparametric tests of differences. Figure 8.5: Parametric and Nonparametric Tests of Differences for Continuous Data Independent Samples \\(\\textbf t\\)-Test When evaluating differences between two independent samples, social psychology researchers generally select from two tests: Student’s \\(t\\)-test and Welch’s \\(t\\)-test. There are other alternatives, such as Yuen’s \\(t\\)-test and a bootstrapped \\(t\\)-test, but these are less commonly reported in scholarly social science journals and will not be covered in this book. The Student’s \\(t\\)-test, which was introduced in Chapter 6, is a parametric test whose assumptions of equal variances seldom hold in people analytics. Welch’s \\(t\\)-test is generally preferred to the Student’s \\(t\\)-test because it has been shown to provide better control of Type 1 error rates when homogeneity of variance is not met, whilst losing little robustness (e.g., Delacre, Lakens, &amp; Leys, 2017). When \\(n\\) is equal between groups, the Student’s \\(t\\)-test is known to be robust to violations of the equal variance assumption, as long as \\(n\\) is sufficiently high to accurately estimate parameters and the underlying distribution is not characterized by high skewness and kurtosis. Let’s explore the mechanics of independent samples \\(t\\)-tests. Figure 8.6 illustrates mean differences (\\(MD\\)) for nine Welch’s \\(t\\)-tests based on random sample data generated from independent normal populations. Remember that statistical power increases with a large \\(n\\), as \\(t\\) distributions approximate a standard normal distribution with larger \\(df\\). In the context of analysis of differences, this translates to an increase in the likelihood of detecting statistical differences in the means of two distributions. Note that for the two cases where both \\(MD\\) and \\(n\\) are relatively small, mean differences are not statistically significant (\\(p &gt; .05\\)). You may also notice that as the \\(t\\)-statistic approaches 0, statistical differences become less likely since a smaller \\(t\\)-statistic indicates a smaller difference between mean values. Figure 8.6: Density plots comparing distributions with a mean of 100 (blue) and 120 (grey) with sample sizes of 100, 1,000, and 10,000 (rows) and standard deviations of 25, 50, and 75 (columns). Dashed vertical lines represent distribution means. Next, we will walk through the steps involved in performing Welch’s \\(t\\)-test. Let’s first visualize the distribution of data for each group using boxplots: Figure 8.7: Annual Compensation Distributions for Managers and Research Scientists While the median – rather than the mean which is being evaluated with Welch’s \\(t\\)-test – is shown in these boxplots, this is a great way to visually inspect whether there are meaningful differences in the distribution of data between groups (in addition to identifying outliers). We could of course use density plots or histograms as an alternative. As we can see, annual compensation for employees with a Manager job title tends to be slightly higher than for those with a Research Scientist job title, and the variance in annual compensation appears to be fairly consistent between the groups. There are several alternatives to a visual inspection of normality, such as the \\({\\chi}^2\\) Goodness-of-Fit test (Snedecor &amp; Cochran, 1980), Kolmogorov-Smirnov (K-S) test (Chakravarti, Laha, &amp; Roy, 1967), or Shapiro-Wilk test (Shapiro &amp; Wilk, 1965). The general idea is consistent for each of these tests: compare observed data to what would be expected if data are sampled from a normally distributed population. The \\({\\chi}^2\\) Goodness-of-Fit test compares the count of data points across the range of values relative to what would be expected in each for a sample with the same dimensions taken from a normal distribution. For example, if data are sampled from a normal population distribution, it follows that roughly half the values should exist below the mean and half above the mean. The K-S test evaluates how the observed cumulative distribution compares to the properties of a normal cumulative distribution. The Shapiro-Wilk test is based on correlations between observed and expected data. We will test for normality using the Shapiro-Wilk test. The null hypothesis for the Shapiro-Wilk test is that the data are normally distributed, so a high \\(p\\)-value indicates that the assumption of normality is satisfied (i.e., failure to reject the null hypothesis of normally distributed data). We can use the with() function together with the shapiro.test() function to run this test in R: # Compute Shapiro-Wilk test of normality for each group with(employees, shapiro.test(annual_comp[job_title == &#39;Manager&#39;])) ## ## Shapiro-Wilk normality test ## ## data: annual_comp[job_title == &quot;Manager&quot;] ## W = 0.93546, p-value = 0.000103 with(employees, shapiro.test(annual_comp[job_title == &#39;Research Scientist&#39;])) ## ## Shapiro-Wilk normality test ## ## data: annual_comp[job_title == &quot;Research Scientist&quot;] ## W = 0.96002, p-value = 3.427e-07 Based on these tests, the distribution of annual compensation for Managers and Research Scientists are significantly different from normally distributed data (\\(p\\) &lt; .05). While we should not proceed with performing Welch’s \\(t\\)-test due to unequal variances, let’s do so merely to illustrate how the test is implemented in R. To perform Welch’s \\(t\\)-test in R, we can simply pass into the t.test() function a numeric vector for each of the two groups. # Create compensation vectors for two jobs comp_mgr &lt;- unlist(subset(employees, job_title == &#39;Manager&#39;, select = annual_comp)) comp_rsci &lt;- unlist(subset(employees, job_title == &#39;Research Scientist&#39;, select = annual_comp)) # Run Welch&#39;s t-test t.test(comp_mgr, comp_rsci) ## ## Welch Two Sample t-test ## ## data: comp_mgr and comp_rsci ## t = 0.22623, df = 159.55, p-value = 0.8213 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -8860.685 11153.244 ## sample estimates: ## mean of x mean of y ## 139900.8 138754.5 If the data adhered to the assumptions of Welch’s \\(t\\)-test, we would conclude that the mean difference between annual compensation for Managers (\\(\\bar{x}\\) = 139,901) and Research Scientists (\\(\\bar{x}\\) = 138,755) is not significant (\\(t(159.55)\\) = .23, \\(p\\) = .82). Note that we can access specific metrics from this output by storing results to an object and then referencing specific elements by name or index: # This assigns each element of results from Welch&#39;s t-test to an indexed position in the object t_rslts &lt;- t.test(comp_mgr, comp_rsci) t_rslts$statistic # t-statistic ## t ## 0.2262262 t_rslts$parameter # df ## df ## 159.5544 t_rslts$p.value # p-value ## [1] 0.8213151 t_rslts$method # type of t-test ## [1] &quot;Welch Two Sample t-test&quot; When object elements are referenced by index, the element name is displayed in the output to clarify what the metric represents: t_rslts[1] # t-statistic ## $statistic ## t ## 0.2262262 t_rslts[2] # df ## $parameter ## df ## 159.5544 t_rslts[3] # p-value ## $p.value ## [1] 0.8213151 t_rslts[9] # type of t-test ## $method ## [1] &quot;Welch Two Sample t-test&quot; Given \\(df\\) = 159.55, you may be wondering how \\(df\\) is calculated for Welch’s \\(t\\)-test given that thus far, we have only discussed the basic \\(df\\) calculation outlined in Chapter 6; namely, \\(df = n - 1\\). Welch’s \\(t\\)-test uses the Welch-Satterthwaite equation for \\(df\\) (Satterthwaite, 1946; Welch, 1947). This equation approximates \\(df\\) for a linear combination of independent sample variances; which means that if samples are not independent, this approximation may not be valid. The Welch-Satterthwaite equation is defined by: \\[ df = \\frac {(\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2})^2} {\\frac{1}{n_1 - 1} (\\frac{s^2_1}{n_1})^2 + \\frac{1}{n_2 - 1} (\\frac{s^2_2}{n_2})^2} \\] Cohen’s \\(\\textbf d\\) is a standardized measure of the difference between two means that helps us understand the size (or practical significance) of observed mean differences. Cohen’s \\(d\\) is defined by: \\[ d = \\frac{\\bar{x}_1 - \\bar{x}_2} {s_p}, \\] where \\(s_p\\) represents the pooled standard deviation defined by: \\[ s_p = \\sqrt\\frac{s^2_1 + s^2_2}{2} \\] Cohen’s \\(d\\) can be produced using the cohen.d() function from the effsize package in R. The following thresholds can be referenced as a general rule of thumb for interpreting effect size: Small = 0.2 Medium = 0.5 Large = 0.8 # Load library for effect size functions library(effsize) # Perform Cohen&#39;s d effsize::cohen.d(comp_mgr, comp_rsci) ## ## Cohen&#39;s d ## ## d estimate: 0.0273669 (negligible) ## 95 percent confidence interval: ## lower upper ## -0.2004390 0.2551728 Not only are the differences statistically insignificant, Cohen’s \\(d\\) = .03 indicates a negligible difference. Therefore, there is nothing of interest based on these statistical and practical significance tests. Mann-Whitney U Test A popular nonparametric (distribution-free) alternative to Welch’s \\(t\\)-test is the Mann-Whitney U Test, also referred to as the Wilcoxon Rank-Sum Test. Rather than comparing the mean between two groups, like the Student’s \\(t\\)-test or Welch’s \\(t\\)-test, the Mann-Whitney U test considers the entire distribution by evaluating the extent to which the ranks are consistent between groups (i.e., similarity in the proportion of records with each value). When distributions are similar, the medians of the two groups are compared. The wilcox.test() function is used to run this test in R. Let’s illustrate by examining whether engagement (an ordinal variable in our dataset) is significantly different between those who have been promoted in the past year and those who have not: # Create dummy-coded promotion variable employees$promo &lt;- ifelse(employees$last_promo == 1, 1, 0) # Create numeric engagement vectors for promo groups no_promo &lt;- unlist(subset(employees, promo == 0, select = engagement)) promo &lt;- unlist(subset(employees, promo == 1, select = engagement)) # Perform the Mann-Whitney U (aka Wilcoxon rank-sum) test wilcox.test(no_promo, promo) ## ## Wilcoxon rank sum test with continuity correction ## ## data: no_promo and promo ## W = 196056, p-value = 0.6707 ## alternative hypothesis: true location shift is not equal to 0 Based on these results, we fail to reject the null hypothesis, which states that there is no difference in engagement between those with and without promotions (\\(W\\) = 196,056, \\(p\\) = .67). Note the reference to continuity correction in the output. Continuity correction is applied when using a continuous distribution to approximate a discrete distribution. The Mann-Whitney U test we performed approximated a continuous distribution for testing differences between our ordinal (discrete) engagement data by applying this continuity correction. Just as Cohen’s \\(d\\) is used to measure the magnitude of difference between a pair of means, Cliff’s delta can be leveraged to evaluate the size of differences between ordinal variables. Cliff’s delta measures how often a value in one distribution is higher than values in another, and this is appropriate in situations in which a nonparametric test of differences is used. This statistic can be produced using the cliff.delta() function from the effsize package in R. # Run Cliff&#39;s Delta effsize::cliff.delta(no_promo, promo) ## ## Cliff&#39;s Delta ## ## delta estimate: -0.0131625 (negligible) ## 95 percent confidence interval: ## lower upper ## -0.07329491 0.04706528 Some (e.g., Vargha &amp; Delaney, 2000) have endeavored to categorize the Cliff’s delta statistic, which ranges from -1 to 1, into effect size buckets. However, such categorizations are far more controversial than thresholds attributed to Cohen’s \\(d\\). Nevertheless, the near-zero delta estimate of -.01 indicates a negligible difference. Paired Samples \\(\\textbf t\\)-Test A Paired Samples \\(\\textbf t\\)-Test is used to compare means between pairs of measurements. This test is known by many other names, such as a dependent samples \\(\\textbf t\\)-test, paired-difference \\(\\textbf t\\)-test, matched pairs \\(\\textbf t\\)-test, and repeated-samples \\(\\textbf t\\)-test. The assumption of normality in the context of a paired samples \\(t\\)-test relates to normally distributed paired differences. This is important, as the \\(p\\)-value for the test statistic will not be valid if this assumption is violated. To illustrate, let’s design an experiment. Let’s assume morale has declined for employees who travel frequently, and several actions have been proposed by a task force to help address this. The task force has decided to pilot a new flexible work benefit over a six-month period to determine if it has a meaningful effect on morale. This new benefit is piloted to a random sample of frequent travelers, and our task is to test whether the outcomes warrant a broader rollout to frequent travelers. Our DV (happiness) will be measured using a composite index derived from individual engagement, environment satisfaction, job satisfaction, and relationship satisfaction scores. Our objective is to determine if there is a significant improvement in this happiness index for the treatment group (those who are part of the flexible work pilot) relative to the pre/post difference for the control group (those not selected for the flexible work pilot). While we could simply look at the pre/post differences for the treatment group, we understand from Chapter 4 that this would be a weak design that may lead to inaccurate conclusions. There could be alternative explanations for any observed increases in happiness that are unrelated to the intervention itself. For example, between time 1 and time 2, travel frequency may have decreased for everyone, which may contribute to overall happiness. By comparing pre/post differences between the treatment and control groups, we gain more confidence in isolating the effect of the flexible work benefit on happiness since alternative explanations should be reflected in any pre/post changes observed for the control group. Difference-in-differences (DiD) estimation is a quasi-experimental approach that originated from econometrics for this same purpose, but it is beyond the scope of this book. Angrist and Pischke (2009) is an excellent resource for learning about these methods. Let’s prepare the data for this experiment. Since employees is a cross-sectional dataset (single point-in-time), we will generate simulated data for repeated measures (i.e., post-intervention scores). # Set seed for reproducible results set.seed(1234) # Derive happiness index from survey variables employees$happiness_ind &lt;- (employees$engagement + employees$env_sat + employees$job_sat + employees$rel_sat) / 4 # Sample size of frequent travelers n = nrow(subset(employees, business_travel == &#39;Travel_Frequently&#39;, select = employee_id)) # Randomly assign half of frequent travelers to treatment and control groups treat_ids &lt;- sample(unlist(subset(employees, business_travel == &#39;Travel_Frequently&#39;, select = employee_id)), floor(n * .5)) ctrl_ids &lt;- unlist(subset(employees, business_travel == &#39;Travel_Frequently&#39; &amp; !employee_id %in% treat_ids, select = employee_id)) # Initialize dfs for pre/post metrics treat_metrics = data.frame(pre_ind = numeric(length(treat_ids)), rand_num = rnorm(length(treat_ids), mean = 15, sd = 5) * .001, post_ind = numeric(length(treat_ids)), diff = numeric(length(treat_ids))) ctrl_metrics = data.frame(pre_ind = numeric(length(ctrl_ids)), rand_num = rnorm(length(ctrl_ids), mean = 0, sd = 1) * .001, post_ind = numeric(length(ctrl_ids)), diff = numeric(length(ctrl_ids))) # Store happiness indices for treatment and control groups treat_metrics$pre_ind &lt;- unlist(subset(employees, employee_id %in% treat_ids, select = happiness_ind)) ctrl_metrics$pre_ind &lt;- unlist(subset(employees, employee_id %in% ctrl_ids, select = happiness_ind)) # Create vectors with artificially inflated post-intervention happiness indices treat_metrics$post_ind &lt;- treat_metrics$pre_ind + treat_metrics$rand_num ctrl_metrics$post_ind &lt;- ctrl_metrics$pre_ind + ctrl_metrics$rand_num # Force an upper bound of 4 to adjusted index scores (variables were measured using a 4-point Likert scale) treat_metrics$post_ind &lt;- if(treat_metrics$post_ind &gt; 4) {4} else {treat_metrics$post_ind} ctrl_metrics$post_ind &lt;- if(ctrl_metrics$post_ind &gt; 4) {4} else {ctrl_metrics$post_ind} It’s important to remember that a paired samples \\(t\\)-test requires that each of the paired measurements be obtained from the same subject. Therefore, if an employee terms between time 1 and time 2, or does not provide the survey responses needed to calculate the happiness index at both time 1 and time 2, the employee should be removed from the data since paired measurements will not be available. The variance is not assumed to be equal for a paired test; therefore, the homogeneity of variance assumption is not applicable in this context. Next, we will evaluate whether paired differences are normally distributed using the Shapiro Wilk test. While individual survey items are measured on an ordinal scale, our derived happiness index is the average of multiple ordinal items and can be considered an approximately continuous variable. There are \\(2^p - p - 1\\) combinations of scores, where \\(p\\) is the number of variables. For our happiness index, there are \\(2^4 - 4 - 1 = 11\\) combinations. # Load library library(ggpubr) # Calculate pre/post differences treat_metrics$diff &lt;- treat_metrics$post_ind - treat_metrics$pre_ind ctrl_metrics$diff &lt;- ctrl_metrics$post_ind - ctrl_metrics$pre_ind # Histogram for distribution of pre/post treatment group differences p_treat &lt;- ggplot2::ggplot() + ggplot2::aes(treat_metrics$diff) + ggplot2::labs(title = &quot;Treatment Group&quot;, x = &quot;Happiness Index Differences&quot;, y = &quot;Frequency&quot;) + ggplot2::geom_histogram(fill = &quot;#414141&quot;) + ggplot2::theme_bw() + ggplot2::theme(plot.title = element_text(hjust = 0.5)) # Histogram for distribution of pre/post control group differences p_ctrl &lt;- ggplot2::ggplot() + ggplot2::aes(ctrl_metrics$diff) + ggplot2::labs(title = &quot;Control Group&quot;, x = &quot;Happiness Index Differences&quot;, y = &quot;Frequency&quot;) + ggplot2::geom_histogram(fill = &quot;#414141&quot;) + ggplot2::theme_bw() + ggplot2::theme(plot.title = element_text(hjust = 0.5)) # Display histograms side-by-side ggpubr::ggarrange(p_treat, p_ctrl, ncol = 2, nrow = 1) Figure 8.8: Pre/Post Differences for Treatment and Control Groups Based on a visual inspection, the distributions of differences appear to be roughly normal. This should not be surprising given random values were sampled from normal distributions to derive artificial post-intervention happiness indices. Let’s test for normality by performing the Shapiro-Wilk test on vectors of differences: # Compute Shapiro-Wilk test of normality shapiro.test(treat_metrics$diff) ## ## Shapiro-Wilk normality test ## ## data: treat_metrics$diff ## W = 0.98936, p-value = 0.3738 shapiro.test(ctrl_metrics$diff) ## ## Shapiro-Wilk normality test ## ## data: ctrl_metrics$diff ## W = 0.99096, p-value = 0.5134 Since \\(p\\) &gt; .05 for both tests, the assumption of normally distributed differences is met. Given the data generative process implemented for this example, differences would become increasingly normal as the sample size increases. We now have the greenlight to perform the paired samples \\(t\\)-test. We can run a paired samples \\(t\\)-test in R by passing paired = TRUE as an argument to the same t.test() function used for the independent samples \\(t\\)-test. Since we are investigating whether the average post-intervention happiness index is significantly greater than the average pre-intervention happiness index, we also need the alternative = &quot;greater&quot; argument since the default two-tailed test only evaluates whether the average indices are significantly different (regardless of whether the pre- or post-intervention index is larger). # Perform one-tailed paired samples t-test for treatment group t.test(treat_metrics$post_ind, treat_metrics$pre_ind, paired = TRUE, alternative = &quot;greater&quot;) ## ## Paired t-test ## ## data: treat_metrics$post_ind and treat_metrics$pre_ind ## t = 35.906, df = 137, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.01490482 Inf ## sample estimates: ## mean of the differences ## 0.01562551 These results indicate that the post-intervention happiness index is significantly larger than the pre-intervention happiness index. This is encouraging with respect to the potential efficacy of the flexible work pilot, but the question about whether the control group experienced a commensurate improvement over the observation period remains unanswered. Let’s run the same paired samples \\(t\\)-test using the control group indices: # Perform one-tailed paired samples t-test for control group t.test(ctrl_metrics$post_ind, ctrl_metrics$pre_ind, paired = TRUE, alternative = &quot;greater&quot;) ## ## Paired t-test ## ## data: ctrl_metrics$post_ind and ctrl_metrics$pre_ind ## t = 0.59995, df = 138, p-value = 0.2748 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -8.719262e-05 Inf ## sample estimates: ## mean of the differences ## 4.953658e-05 Since \\(p\\) &gt; .05, we can conclude that there was not a significant increase in happiness indices for the control group, which provides additional – but not conclusive – support for the effectiveness of the flexible work benefit. Chapter 9 will introduce linear regression, which is a powerful modeling tool for people analytics that helps control for multiple alternative explanations of associations with the DV in order to isolate the unique effects of each IV. We can evaluate the magnitude of mean differences for these paired samples by passing the paired = TRUE argument to the same cohen.d() function used for independent samples: # Perform Cohen&#39;s d effsize::cohen.d(treat_metrics$post_ind, treat_metrics$pre_ind, paired = TRUE) ## ## Cohen&#39;s d ## ## d estimate: 0.03132117 (negligible) ## 95 percent confidence interval: ## lower upper ## 0.02960345 0.03303890 Though pre/post indices are statistically significant for the treatment group, the size of the difference is negligible (\\(d\\) = .03). Wilcoxon Signed-Rank Test The Wilcoxon Signed-Rank Test is the nonparametric alternative to the paired samples \\(t\\)-test. This distribution-free test does not require normally distributed differences. The matched Wilcoxon Signed-Rank test is performed in R using the same wilcox.test() function used to perform the unmatched Wilcoxon Rank-Sum test. Though we can use a paired samples \\(t\\)-test to test differences for our flexible work benefit study since the assumption of normally distributed differences is met, let’s run a Wilcoxon Signed-Rank test for demonstrative purposes: # Perform Wilcoxon Signed-Rank test wilcox.test(treat_metrics$post_ind, treat_metrics$pre_ind, paired = TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: treat_metrics$post_ind and treat_metrics$pre_ind ## V = 9591, p-value &lt; 2.2e-16 ## alternative hypothesis: true location shift is not equal to 0 # Perform Wilcoxon Signed-Rank test wilcox.test(ctrl_metrics$post_ind, ctrl_metrics$pre_ind, paired = TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: ctrl_metrics$post_ind and ctrl_metrics$pre_ind ## V = 5166, p-value = 0.5275 ## alternative hypothesis: true location shift is not equal to 0 Consistent with results from the paired samples \\(t\\)-tests, significantly higher post-intervention happiness indices were observed for the treatment group but not for the control group. We can evaluate the magnitude of differences for these paired samples by passing the paired = TRUE argument to the same cliff.delta() function used for independent samples: # Run Cliff&#39;s Delta effsize::cliff.delta(treat_metrics$post_ind, treat_metrics$pre_ind, paired = TRUE) ## ## Cliff&#39;s Delta ## ## delta estimate: 0.1544844 (small) ## 95 percent confidence interval: ## lower upper ## 0.01652557 0.28667094 With Cliff’s delta, we observe a small difference between pre/post indices for the treatment group (delta estimate = .15). Analysis of Variance (ANOVA) Analysis of Variance (ANOVA) is used to determine whether the means of scale-level DVs are equal across nominal-level variables with three or more independent categories. It is important to understand that \\(H_0\\) in ANOVA not only requires all group means to be equal but their complex contrasts as well. For example, if we have four groups named A, B, C, and D, \\(H_0\\) requires that \\(\\mu_A = \\mu_B = \\mu_C = \\mu_D\\) is true as well as the various complex contrasts such as \\(\\mu_{A,B} = \\mu_{C,D}\\) and \\(\\mu_A = \\mu_{B,C,D}\\) and \\(\\mu_D = \\mu_{B,C}\\). Therefore, a difference between one or more of these contrasts results in a decision to reject \\(H_0\\) in ANOVA. As a result, we may find a significant \\(F\\)-statistic but no significant differences between pairwise means. In addition, it is possible to find a significant pairwise mean difference but a non-significant result from ANOVA. As you may recall from Chapter 6, multiple comparisons reduce the power of statistical tests. Since multiple tests of mean differences are performed with ANOVA, the family-wise error rate is used to adjust for the increased probability of a Type I error across the set of analyses. Since the power of a single pairwise test is greater relative to the power of familywise comparisons, we may find a significant result for the former but not the latter. ANOVA requires IVs to be categorical (nominal or ordinal) and the DV to be continuous (interval or ratio). A one-way ANOVA is used to determine how one categorical IV influences a continuous DV. A two-way ANOVA is used to determine how two categorical IVs influence a continuous DV. A three-way ANOVA is used to evaluate how three categorical IVs influence a continuous DV. An ANOVA that uses two or more categorical IVs is often referred to as a factorial ANOVA. As discussed in Chapter 1, it is important to remain grounded in specific hypotheses, as a significant ANOVA may not actually test what is being hypothesized. ANOVA is not a test, per se, but a \\(F\\)-test underpins it. The mathematical procedure behind the \\(F\\)-test is relatively straightforward: Compute the within-group variance, which is also known as residual variance. Simply put, this tells us how different each member of the group is from the average. Compute the between-group variance. This represents how different the group means are from one another. Produce the \\(F\\)-statistic, which is the ratio of within-group variance to between-group variance. More formally, the \\(F\\)-statistic is defined by: \\[ F = \\frac{MS_{between}}{MS_{within}}, \\] where: \\[ MS_{between} = \\frac{SS_{between}}{df_{between}}, \\] \\[ MS_{within} = \\frac{SS_{within}}{df_{within}}, \\] \\[ SS_{between} = \\displaystyle\\sum_{j=1}^{p} n_j(\\bar{x}_j-\\bar{x})^2, \\] \\[ SS_{within} = \\displaystyle\\sum_{j=1}^{p} \\displaystyle\\sum_{i=1}^{n_j} (x_{ij}-\\bar{x_j})^2 \\] One-Way ANOVA To illustrate how to perform a one-way ANOVA, we will test the hypothesis that mean annual compensation is equal across job satisfaction levels. Each observation in employees represents a unique employee, and a given employee can only have one job satisfaction score and one annual compensation value. The assumption of independence is met since each record exists independent of one another and each job satisfaction group is comprised of different employees. Levene’s test (Levene, 1960) can be used to test the homogeneity of variance assumption – even with non-normal distributions. This can be performed in R using the leveneTest() function from the car package: # Perform Levene&#39;s test for homogeneity of variance car::leveneTest(annual_comp ~ as.factor(job_sat), data = employees) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 3 0.3293 0.8042 ## 1466 The test statistic associated with Levene’s test relates to the null hypothesis that there are no significant differences in variances across the job satisfaction levels. Since \\(p &gt; .05\\), we fail to reject this null hypothesis and can assume equal variances. Next, let’s test the assumption of normality. It is important to note that the assumption of normality does not apply to the distribution of the DV but to the distribution of residuals for each group of the IV. Residuals in the context of ANOVA represent the difference between the actual values of the continuous DV relative to its mean value for each level of the categorical IV (e.g., \\(y - \\bar{y}_A\\), \\(y - \\bar{y}_B\\), \\(y - \\bar{y}_C\\)). In ANOVA, we expect the residuals to be normally distributed around a mean of 0 (the balance point) when the data are normally distributed within each IV category; the more skewed the data, the larger the average distance of each DV value from the mean. # Load data viz library library(ggplot2) library(ggpubr) # Create function to visualize distribution dist.viz &lt;- function(data, x) { viz &lt;- ggplot2::ggplot() + ggplot2::aes(data) + ggplot2::labs(title = paste(&quot;Job Sat = &quot;, x), x = &quot;Annual Compensation&quot;, y = &quot;Frequency&quot;) + ggplot2::geom_histogram(fill = &quot;#414141&quot;) + ggplot2::theme_bw() + ggplot2::theme(plot.title = element_text(hjust = 0.5)) return(viz) } # Produce annual compensation vectors for each job satisfaction level # Unlist() is needed to convert the default object from subset() into a numeric vector group_1 &lt;- unlist(subset(employees, job_sat == 1, select = annual_comp)) group_2 &lt;- unlist(subset(employees, job_sat == 2, select = annual_comp)) group_3 &lt;- unlist(subset(employees, job_sat == 3, select = annual_comp)) group_4 &lt;- unlist(subset(employees, job_sat == 4, select = annual_comp)) # Call UDF to build annual comp histogram for each job satisfaction level viz_1 &lt;- dist.viz(data = group_1, x = 1) viz_2 &lt;- dist.viz(data = group_2, x = 2) viz_3 &lt;- dist.viz(data = group_3, x = 3) viz_4 &lt;- dist.viz(data = group_4, x = 4) # Display distribution visualizations ggpubr::ggarrange(viz_1, viz_2, viz_3, viz_4, ncol = 2, nrow = 2) Figure 8.9: Annual Compensation Distribution by Job Satisfaction Level As we can see, annual compensation data are not normally distributed within job satisfaction groups. Therefore, we would not expect the distribution of residuals to be normally distributed within these groups either. To test whether the assumption of normality is met, we will first produce and review a quantile-quantile (Q-Q) plot. A Q-Q plot compares two probability distributions by plotting their quantiles (data partitioned into equal-sized groups) against each other. After partitioning annual compensation into groups differentiated by job satisfaction level, we can use the ggqqplot() function from the ggpubr library to build a Q-Q plot and evaluate the distribution of residuals. # Generate residuals for each group residuals &lt;- c(group_1 - mean(group_1), group_2 - mean(group_2), group_3 - mean(group_3), group_4 - mean(group_4)) # Create a Q-Q plot of residuals ggpubr::ggqqplot(residuals) Figure 8.10: Q-Q Plot of Annual Compensation Residuals To satisfy the assumption of normality, residuals must lie along the linear line. Based on the Q-Q plot in Figure 8.10, there is a clear departure from normality at both ends of the theoretical range. Let’s test for normality using the Shapiro-Wilk test: # Compute Shapiro-Wilk test of normality shapiro.test(residuals) ## ## Shapiro-Wilk normality test ## ## data: residuals ## W = 0.95874, p-value &lt; 2.2e-16 Since \\(p &lt; .05\\), we reject the null hypothesis of normally distributed data, which indicates that the assumption of normality is violated. This should not be surprising based on the deviation from normality we observed in Figure 8.10. Because the assumption of normality is violated, we have two options. First, we can attempt to transform the data so that the residuals using the transformed values are normally distributed. If the data are resistant to transformation, we can leverage a nonparametric alternative to ANOVA. Let’s first try several common data transformations and then examine the resulting Q-Q plots: # Build a linear model using the natural logarithm of annual comp ln.model &lt;- lm(log(annual_comp) ~ job_sat, data = employees) # Build a linear model using the log base 10 of annual comp log10.model &lt;- lm(log10(annual_comp) ~ job_sat, data = employees) # Build a linear model using the square root of annual comp sqrt.model &lt;- lm(sqrt(annual_comp) ~ job_sat, data = employees) # Store Q-Q plots to viz objects ln.viz &lt;- ggpubr::ggqqplot(residuals(ln.model)) + ggtitle(&quot;Natural Log&quot;) log10.viz &lt;- ggpubr::ggqqplot(residuals(log10.model)) + ggtitle(&quot;Log Base 10&quot;) sqrt.viz &lt;- ggpubr::ggqqplot(residuals(sqrt.model)) + ggtitle(&quot;Square Root&quot;) # Display Q-Q plots of residuals ggpubr::ggarrange(ln.viz, log10.viz, sqrt.viz, ncol = 3, nrow = 1) Figure 8.11: Q-Q Plots of Transformed Annual Compensation Residuals Even with these transformations, there is still a clear S-shaped curve about the residuals. Though we cannot proceed with ANOVA due to violated assumptions, the implementation of ANOVA is demonstrated below. Performing ANOVA involves the aov() function along with the summary() function to display model output: # One-way ANOVA investigating mean differences in annual comp by job satisfaction one.way &lt;- aov(annual_comp ~ job_sat, data = employees) summary(one.way) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## job_sat 1 1.337e+10 1.337e+10 7.508 0.00622 ** ## Residuals 1468 2.613e+12 1.780e+09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The Kruskal Wallis H Test is the nonparametric alternative to a one-way ANOVA (Daniel, 1990) and an appropriate alternative for investigating median differences in annual comp by job satisfaction in our data. This test can be performed using the kruskal.test() function in R: # Nonparametric Kruskal one-way ANOVA kruskal.test(annual_comp ~ job_sat, data = employees) ## ## Kruskal-Wallis rank sum test ## ## data: annual_comp by job_sat ## Kruskal-Wallis chi-squared = 8.3242, df = 3, p-value = 0.03977 Since \\(p &lt; .05\\), we can conclude that there are significant differences in median compensation across the groups. However, this test does not indicate which groups are different. We can utilize the pairwise.wilcox.test() function to compute pairwise Wilcoxon rank-sum tests to identify where differences exist: pairwise.wilcox.test(employees$annual_comp, employees$job_sat, p.adjust.method = &quot;BH&quot;) ## ## Pairwise comparisons using Wilcoxon rank sum test with continuity correction ## ## data: employees$annual_comp and employees$job_sat ## ## 1 2 3 ## 2 0.298 - - ## 3 0.041 0.298 - ## 4 0.041 0.298 0.879 ## ## P value adjustment method: BH Based on the results, there are significant pairwise differences in median annual compensation for job satisfaction levels 3 and 4 relative to level 1. Factorial ANOVA Factorial ANOVA is any ANOVA which uses two or more categorical IVs, such as a two-way or three-way ANOVA. It is important to understand the hypothesis a factorial design tests. The following output reflects the cross-tabulation of average annual compensation for each combination of two factors – job satisfaction and stock option level. # Calculate mean for each IV pair combos &lt;- aggregate(annual_comp ~ job_sat + stock_opt_lvl, employees, mean) combos ## job_sat stock_opt_lvl annual_comp ## 1 1 0 141254.0 ## 2 2 0 138753.3 ## 3 3 0 132159.2 ## 4 4 0 132227.0 ## 5 1 1 141763.9 ## 6 2 1 135494.5 ## 7 3 1 135235.7 ## 8 4 1 135569.0 ## 9 1 2 146240.0 ## 10 2 2 146432.0 ## 11 3 2 145080.0 ## 12 4 2 143019.3 ## 13 1 3 154844.4 ## 14 2 3 144254.1 ## 15 3 3 135672.7 ## 16 4 3 127102.9 As we have already discussed, a difference between one or more of these contrasts may result in a decision to reject \\(H_0\\) in ANOVA. We may also find a significant pairwise difference but a non-significant result from ANOVA since the family-wise error rate adjustment is applied in the context of multiple comparisons which reduces statistical power. Factorial ANOVA can be performed by adding variables with + within the same aov() function used for one-way ANOVA: # Factorial ANOVA investigating mean differences in annual comp by job satisfaction and stock option level factorial &lt;- aov(annual_comp ~ job_sat + stock_opt_lvl, data = employees) summary(factorial) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## job_sat 1 1.337e+10 1.337e+10 7.523 0.00617 ** ## stock_opt_lvl 1 6.840e+09 6.840e+09 3.850 0.04995 * ## Residuals 1467 2.606e+12 1.777e+09 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 While mean annual compensation is significantly different by both job satisfaction and stock option level factors, this output alone is not too helpful in understanding the nature of the differences. These statistical significance markers indicate that there are meaningful differences that warrant a deeper understanding. Relationships of job satisfaction and stock option level with annual compensation are illustrated more effectively in Figure 8.12. Figure 8.12: Relationships of Job Satisfaction and Stock Option Level with Annual Compensation As we can see, there is a strong negative relationship between job satisfaction and average annual compensation among employees with the highest stock option level (3). The relationship between job satisfaction and average annual compensation appears to be negative for employees with other stock option levels as well, albeit much weaker. These relationships may initially seem counterintuitive, as one might expect higher levels of job satisfaction to contribute to higher performance and consequently, higher compensation. There may be other variables that happen to be correlated with job satisfaction and/or stock option level that are the actual determinants of annual compensation. For example, there may be a relationship between jobs that each feature a stock option level of 3 but for which there are markedly different average job satisfaction scores and annual compensation among workers in said jobs. Without accounting for additional variables that may explain why employees vary in the amount of annual compensation they earn, the limited set of relationships shown in Figure 8.12 may lead to a misleading understanding and inaccurate conclusions. Three-way factorials (and beyond) become difficult to visualize and understand in the way one-way ANOVA and two-way factorials have been explained in this chapter. In Chapter 9, we will discuss how to create linear combinations of many IVs and parse the output to understand how they independently and jointly explain variation in the DV. 8.4 Review Questions What are the main differences between a Chi-square test and Fisher’s exact test? Why is it problematic to test for significant differences using the \\({\\chi}^2\\) statistic with extremely small samples (e.g., \\(n\\) &lt; 5)? What are the general assumptions of parametric tests? What is a benefit of Welch’s \\(t\\)-test over the Student’s \\(t\\)-test? How does a paired-samples \\(t\\)-test differ from an independent samples \\(t\\)-test? In what ways does the Wilcoxon signed-rank test differ from the paired samples \\(t\\)-test? How can the magnitude of differences (i.e., practical significance) be quantified when working with data measured on a continuous scale? How can the magnitude of differences (i.e., practical significance) be quantified when working with data measured on an ordinal scale? What null hypothesis does ANOVA test? What are some ways to better understand the nature of statistical differences indicated in the output of ANOVA? "],["lm.html", "9 Linear Regression 9.1 Simple Linear Regression 9.2 Multiple Linear Regression 9.3 Model Comparisons 9.4 Review Questions", " 9 Linear Regression It’s important to draw a distinction between inferential and predictive models. Inferential models are highly interpretable and their utility is largely in understanding the nature and magnitude of the effect variables have on outcomes. Inferential models also lend to quantifying the extent to which we can generalize the observed effects to the larger population from which the sample was drawn. The objective in predictive modeling is to also to learn from patterns in historical data but for the purpose of achieving the most accurate predictions for future events – even at the expense of interpretability. To be clear, this isn’t to say that predictive models cannot be interpreted – they certainly can – but there are relatively few applications for predictive modeling in people analytics because models generally need to be highly interpretable to support action planning. This chapter is dedicated to inferential modeling to support a working understanding of how to interpret model output and communicate clear, data-driven narratives that respect the nuance and noise characteristic of people data. Chapter 12 will provide an overview of predictive modeling frameworks. Regression is perhaps the most important statistical learning technique for people analytics. If you have taken a statistics course at the undergraduate or graduate levels, you have surely already encountered it. Let’s first develop an intuitive understanding of the mechanics of regression. Imagine we are sitting at a large public park in NYC on a nice fall afternoon. If asked to estimate the annual compensation of the next person to walk by, how would you estimate this in the absence of any additional information? Most would likely estimate the average annual compensation of everyone capable of walking by. Since this would include both residents and visitors, this would be a very large population of people! The obvious limitation with this approach is that among the large group of people capable of walking by, there is likely a significant range of annual compensation values. Many walking by may be children, unemployed, or retirees who earn no annual compensation, while others may be highly compensated senior executives at the pinnacle of their careers. Since the range of annual compensation could be zero to millions of dollars, estimating the average of such a large population is likely going to be highly inaccurate without more information. Let’s consider that we are sitting outside on a weekday afternoon. Should this influence our annual compensation estimate? It is likely that we can eliminate a large segment of those likely to walk by, as we would expect most children to be in school on a typical fall weekday afternoon. It’s also unlikely that those who are employed and not on vacation will walk by on a fall weekday afternoon. Therefore, factoring in that it is a weekday should limit the size of the population which in turn may reduce the range of annual compensation values for our population of passerbys. Let’s now consider that the park is open only to invited guests for a symposium on people analytics. Though it may be difficult to believe, a relatively small subset of the population is likely interested in attending such a symposium, so this information will likely be quite helpful in reducing the size of the population who could walk by. This should further reduce the range of annual compensation since we probably have a good idea of the profile of those most likely to attend. This probably also lessens (or altogether eliminates) the importance of the weekday factor in explaining why people vary in the amount of compensation they earn each year. That an important variable may become unimportant in the presence of another variable is a key feature of regression. Furthermore, let’s consider that only those who reside in NYC and Boise were invited, and that the next person to walk by resides in Boise. Most companies apply a significant cost of living multiplier to the compensation for those in an expensive region such as NYC, resulting in a significant difference in compensation relative to those residing in a much less expensive city like Boise – all else being equal. Therefore, if we can partition attendees into two groups based on their geography, this should limit the range of annual compensation significantly within each – likely making the average compensation in each group a more nuanced and reasonable estimate. What if we also learn the specific zip code in which the next passerby from Boise resides? The important information is likely captured at the larger city level (NYC vs. Boise), as the compensation for the specific zip codes within each city are unlikely to vary to a significant degree. Assuming this is true, it probably would not make sense to consider both the city name and zip code since they are effectively redundant pieces of information with regard to explaining differences in annual compensation. What if we learn that the next person to walk by will be wearing a blue shirt? Does this influence your estimate? Unless there is research to suggest shirt color and earnings are related, this information will likely not contribute any significant information to our understanding of why people vary in the amount of compensation they earn and should, therefore, not be considered. You can probably think of many relevant variables that would help further narrow the range of annual compensation. These may include job, level, years of experience, education, among other factors. The main thing to understand is that for each group of observations with the same characteristics – such as senior analysts with a graduate degree who reside in NYC – there is a distribution of annual compensation. This distribution reflects unexplained variance. That is, we do not have information to explain why the compensation for each and every person is not the same and in social science contexts, it simply is not practical to explain 100 percent of the variance in outcomes. For example, two people may be similar on dozens of factors (experience, education, skills) but one was simply a more effective negotiator when offered the same role and commanded a higher salary. It’s likely we do not have data on salary negotiation ability so this information would leave us with unexplained variance in compensation. The goal is simply to identify the variables that provide the most information in helping us tighten the distribution so that estimating the average value will generally be an accurate estimate for those in the larger population with the same characteristics. While we can generally improve our estimates with more relevant information (not shirt color or residential zip code in this case), it is important to understand that samples which are too small (\\(n\\) &lt; 30) lend to anomalies; modeling noise in sparse data can result in models that are unlikely to generalize beyond the sample data. For example, if the only people from Boise to attend the people analytics symposium happen to be two ultra wealthy tech entrepreneurs who earn millions each year, it would not be appropriate to use this as the basis for our estimates of all future attendees from Boise. This is a phenomenon known as overfitting that will be covered in Chapter 12. This is the essence of linear regression modeling: find a limited number of variables which independently and/or jointly provide significant information that helps explain (by reducing) variance around the average value. As illustrated in this example, adding additional variables (information) can impact the importance of other variables or may offer no incremental information at all. In this chapter, we will cover how to identify which variables are important and how to quantify the effect they have on an outcome. Assumptions &amp; Diagnostics From the outset, we must determine if a linear model is appropriate for the data at hand. Consistent with the assumptions of parametric tests covered in Chapter 8, there are several assumptions that need to be validated to make this decision, and these largely relate to the residuals (\\(\\hat{y}\\) - \\(y\\)): Independence: Residuals are independent of each other; consecutive residuals in time series data are unrelated Homoscedasticity: Variance of residuals is constant across values of \\(X\\) Normality: Residuals must be normally distributed (with mean of 0) across values of \\(X\\) Linearity: Relationship between \\(X\\) and \\(Y\\) is linear In addition to these key assumptions, additional diagnostics are important to incorporate into the early data screening stage: High-Leverage Observations: Influential data that significantly changes the model fit Collinearity: Independent variables that are highly correlated (these should be independent) 9.1 Simple Linear Regression Simple linear regression is a simple technique for estimating the value of a quantitative DV, denoted as \\(Y\\), on the basis of a single IV, denoted as \\(X\\). It is assumed that there is an approximately linear relationship between \\(X\\) and \\(Y\\). Often, this relationship is expressed as regressing \\(Y\\) on \\(X\\) and is defined mathematically as: \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon, \\] where \\(\\beta_0\\) is the expected value of \\(Y\\) when \\(X = 0\\) (the intercept), and \\(\\beta_1\\) represents the average change in \\(Y\\) for a one-unit increase in \\(X\\) (the slope). \\(\\beta_0\\) and \\(\\beta_1\\) are unknown parameters or coefficients. The error term, \\(\\epsilon\\), acknowledges that there is variation in \\(Y\\) not accounted for by this simple linear model. In other words, it is highly unlikely that there is a perfectly linear relationship between \\(X\\) and \\(Y\\), as additional variables not included in the model are likely influencing \\(Y\\). Once we estimate the unknown model coefficients, \\(\\beta_0\\) and \\(\\beta_1\\), we can estimate \\(Y\\) for a particular value of \\(X\\) by calculating: \\[ \\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta_1}x_i, \\] where \\(\\hat{y}\\) represents an estimate of \\(Y\\) for the \\(i\\)-ith value of \\(X\\) equal to \\(x\\). The \\(\\hat{}\\) symbol is used to denote an estimated value of an unknown coefficient, parameter, or outcome. The earliest form of linear regression is the least squares method, which was developed at the beginning of the nineteenth century and applied to astronomy problems (James, Witten, Hastie, &amp; Tibshirani, 2013). While there are several approaches to fitting a linear regression model, ordinary least squares (OLS) is the most common. OLS selects coefficients for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) that minimize the residual sum of squares (RSS) defined by: \\[RSS = (y_1 - \\hat{\\beta}_0 - \\hat{\\beta}_1x_1)^2 + (y_2 - \\hat{\\beta}_0 - \\hat{\\beta}_1x_2)^2 + {...} + (y_n - \\hat{\\beta}_0 - \\hat{\\beta}_1x_n)^2\\] For each value of \\(X\\), OLS fits a model for which the difference between the predicted value (\\(\\hat{\\beta}_0 - \\hat{\\beta}_1x_i)^2\\)) and actual value (\\(y_i\\)) is as small as possible. Figure 9.1 illustrates the result of minimizing \\(RSS\\) using OLS: Figure 9.1: Minimizing RSS with Ordinary Least Squares (OLS) Fit The minimizers for the least squares coefficient estimates are defined by: \\[\\hat\\beta_1 = \\frac{\\displaystyle\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})} {\\displaystyle\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\] \\[\\hat\\beta_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x},\\] where \\(\\bar{y}\\) and \\(\\bar{x}\\) are sample means. It is important to understand the role sample size plays in achieving accurate estimates of \\(Y\\). Figure 9.2 illustrates the impact of fitting a model to too few observations. With \\(n\\) = 2, it would be easy to fit a perfect model to the data; that is, one representing a line that connects the two data points. However, it is highly unlikely that these data points represent the best model for a larger sample, as there would likely be some distribution of \\(Y\\) for each value of \\(X\\). Figure 9.2: Left: Least squares regression model fit to n = 2 observations. Right: Least squares regression model fit to n = 20 observations. In R, we can build (or fit) a simple linear regression model using the lm() function. The syntax is lm(Y ~ X, dataset): # Load library library(dplyr) # Load employee data employees &lt;- read.csv(&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv&quot;) # Subset employees data frame; leads are only applicable for those in sales positions data &lt;- subset(employees, job_title %in% c(&#39;Sales Executive&#39;, &#39;Sales Representative&#39;)) # Regress YTD leads on engagement slm.fit &lt;- lm(ytd_leads ~ engagement, data) In practice, linear assumptions are rarely – if ever – perfectly met, but there must be evidence that the assumptions are generally satisfied. Before performing model diagnostics, it is important to note the following: Multicollinearity diagnostics are only applicable in the context of multiple regression, as simple linear models have only one IV (this will be covered later in the chapter). Outliers are not always an issue, as we discussed in Chapter 7. Figure 9.3 illustrates differences between an outlier that does not influence the model fit (left) relative to one which has significant leverage on the model fit (right). ## ## Call: ## lm(formula = Y ~ X) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4332 -0.5482 -0.0275 0.2867 2.6413 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.4395 0.4084 1.076 0.296 ## X 3.1303 0.1515 20.659 5.5e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8829 on 18 degrees of freedom ## Multiple R-squared: 0.9595, Adjusted R-squared: 0.9573 ## F-statistic: 426.8 on 1 and 18 DF, p-value: 5.498e-14 ## ## Call: ## lm(formula = Y.out ~ X.out) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.34746 -0.56288 -0.09021 0.29951 2.66197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.5224 0.3399 1.537 0.141 ## X.out 3.0900 0.1077 28.703 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.863 on 19 degrees of freedom ## Multiple R-squared: 0.9775, Adjusted R-squared: 0.9763 ## F-statistic: 823.9 on 1 and 19 DF, p-value: &lt; 2.2e-16 ## ## Call: ## lm(formula = Y.lev ~ X.lev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.3085 -1.8776 0.8556 2.0309 4.8048 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.9645 1.3559 2.924 0.00871 ** ## X.lev 1.4180 0.4295 3.302 0.00375 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.443 on 19 degrees of freedom ## Multiple R-squared: 0.3646, Adjusted R-squared: 0.3311 ## F-statistic: 10.9 on 1 and 19 DF, p-value: 0.00375 Figure 9.3: Left: Model fit with non-influential outlier. Right: Model fit with high leverage outlier. Outlier shown in red. Black solid line represents model fit without outliers. Red dashed line represents model fit with outliers. We can conveniently perform linear model diagnostics using the plot() function in conjunction with the object holding linear model results (slm.fit). This produces the following standard plots shown in Figure 9.4: Residuals vs Fitted: Shows how residuals (\\(y\\)-axis) change across the range of fitted values (\\(x\\)-axis) Normal Q-Q: Compares two probability distributions by plotting their quantiles (data partitioned into equal-sized groups) against each other Scale-Location: Shows how standardized residuals (\\(y\\)-axis) change across the range of fitted values (\\(x\\)-axis) Residuals vs Leverage: Shows the leverage of each data point (\\(x\\)-axis) against their standardized residuals (\\(y\\)-axis) Figure 9.4: Simple linear regression model diagnostics. The Residuals vs Fitted and Scale-Location plots help evaluate assumptions of homoscedasticity, linearity, and normality – which are intricately linked. Data are heteroscedastic if there is a flarring or funnel patterning about the residuals across the range of fitted values. That is, there must be a constant variance about the residual errors in order for the assumption of homoscedasticity to be met. This occurs when there is a linear relationship between \\(X\\) and \\(Y\\), in which case residuals will be normally distributed around a mean of 0. While the spread of residuals is greater for larger fitted values in this model, resulting in the lower standardized residual error for smaller fitted values indicated in the Scale-Location plot, the slope of the line in the Residuals vs Fitted plot is effectively flat which indicates that the model does not perform significantly better for certain fitted values relative to others. Cook’s distance in the Residuals vs Leverage plot provides a measure of how much our model estimates for all observations change if high leverage observations are removed from the data. Higher numbers indicate stronger influence. R conveniently labels the three observations with the highest leverage, though the degree of leverage is only problematic for observations beyond the dashed Cook’s distance line. In this case, there are no observations with enough leverage for the dashed Cook’s distance line to show on the plot, so no action is warranted. In addition to the visual inspection, we can perform the Breusch-Pagan test using the bptest() function from the lmtest library to test the null hypothesis that the data are homoscedastic. If \\(p\\) &lt; .05 for the test statistic, we reject the null hypothesis and conclude that there is evidence of heteroscedasticity in the data. # Run the Breusch-Pagan test for evaluate homoscedasticity lmtest::bptest(slm.fit) ## ## studentized Breusch-Pagan test ## ## data: slm.fit ## BP = 0.07603, df = 1, p-value = 0.7828 Since \\(p\\) &gt; .05, we fail to reject the null hypothesis of homoscedasticity; therefore, this assumption is satisified. If this was not the case, a common approach to addressing heteroscedasticity is transforming the response variable by taking the natural logarithm (log()) or square root (sqrt()) of the raw data. While transformations may correct for violations of linear model assumptions, they also result in a less intuitive interpretation of model output relative to the raw untransformed data. Let’s illustrate how to transform the response variable: # Square root transformation of YTD leads slm.fit.trans &lt;- lm(sqrt(ytd_leads) ~ engagement, data) # Natural logarithmic transformation of YTD leads slm.fit.trans &lt;- lm(log(ytd_leads) ~ engagement, data) The Normal Q-Q Plot in Figure 9.4 is used to test the assumption of normally distributed model residuals. A perfectly normal distribution of residuals will result in data lying along the line situated at 45-degrees from the \\(x\\)-axis. Based on a visual inspection, our residuals appear to be normally distributed, as there are only a small number of minor departures in the upper and lower ends of the quantile range. We can also visualize the distribution of model residuals using a histogram. In the majority of cases, the residual should be 0 – that is, in most cases the model correctly estimates YTD leads resulting in no difference between estimated and observed values (\\(\\hat{y}\\) - \\(y\\) = 0). # Produce histogram to visualize distribution of model residuals ggplot2::ggplot() + ggplot2::aes(slm.fit$residuals) + ggplot2::labs(x = &quot;YTD Leads Residuals&quot;, y = &quot;Density&quot;) + ggplot2::geom_histogram(aes(y = ..density..), fill = &quot;#414141&quot;) + ggplot2::geom_density(fill = &quot;#ADD8E6&quot;, alpha = 0.6) + ggplot2::theme_bw() Figure 9.5: Distribution of model residuals. Based on both the Normal Q-Q Plot and histogram, the residuals conform to the assumption of normality. We can confirm using the Shapiro-Wilk test, in which a non-significant test statistic is sufficient evidence of normality: # Compute Shapiro-Wilk test of normality shapiro.test(slm.fit$residuals) ## ## Shapiro-Wilk normality test ## ## data: slm.fit$residuals ## W = 0.99339, p-value = 0.07029 Let’s use the flextable package to present our simple linear model results in an elegant tabular summary: library(flextable) # Produce tabular summary for model results using flextable flextable::as_flextable(slm.fit) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-a668f2ee{}.cl-a6538eae{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a6538ec2{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-a653a7a4{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a653a7b8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-a653ee76{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ee8a{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ee94{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ee9e{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eea8{width:81.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eeb2{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eeb3{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eebc{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eec6{width:81.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eed0{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eed1{width:81.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eeda{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eee4{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eee5{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eef8{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653eef9{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef02{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef0c{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef0d{width:81.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef16{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef20{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef21{width:81.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef2a{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef34{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-a653ef35{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} EstimateStandard Errort valuePr(&gt;|t|)(Intercept)1.6300.9971.6350.1028engagement20.0650.35756.1930.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05 &lt; '.' &lt; 0.1 &lt; '' &lt; 1Residual standard error: 5.095 on 407 degrees of freedomMultiple R-squared: 0.8858, Adjusted R-squared: 0.8855F-statistic: 3158 on 407 and 1 DF, p-value: 0.0000 There are several important pieces of information in this output: Estimate: Unstandardized Beta coefficient associated with the predictor Standard Error: Average distance between the observed and estimated values per the fitted regression line t value: Estimate / Standard Error. Larger values provide more evidence for a non-zero coefficient (relationship) in the population. Pr(&gt;|t|): \\(p\\)-value for evaluating whether there is sufficient evidence in the sample that the coefficient (relationship) between the respective predictor and response variable is not 0 in the population (i.e., \\(x\\) has a relationship with \\(y\\)) Intercept: Mean value of the response variable when all predictors are equal to 0. Note that the interpretation of the intercept is often nonsensical since many predictors cannot have 0 values (e.g., age, height, weight, IQ). Signif. codes: Symbols to quickly ascertain whether predictors are significant at key levels, such as \\(p\\) &lt; .001 (), \\(p\\) &lt; .01 (), or \\(p\\) &lt; .05 (). Residual standard error: Measure of model fit which reflects the standard deviation of the residuals (\\(\\sqrt {\\sum(y-\\hat{y})^2 / df}\\)) Degrees of freedom: \\(n\\) - \\(p\\), where \\(n\\) is the number of observations and \\(p\\) is the number of predictors Multiple R-squared: Percent of variance in \\(y\\) (when multiplied by 100) explained by the predictors in the model. This is also known as the Coefficient of Determination. For simple linear regression, this is simply the squared value of Pearson’s \\(r\\) for the bivariate relationship between the predictor and response (execute cor(data$engagement, data$ytd_leads)^2 to validate). Adjusted R-squared: Modified version of \\(R^2\\) that adjusts the estimate for non-significant predictors. A large delta between \\(R^2\\) and Adjusted \\(R^2\\) coefficients generally indicates a model containing a larger number of non-significant predictors relative to when \\(R^2\\) and Adjusted \\(R^2\\) values are similar. F-statistic: Statistic used in conjunction with the p-value for testing differences between the specified model and an intercept-only model (a model with no predictors). This test helps us evaluate whether our predictors are helpful in explaining variance in \\(y\\). The output of this simple linear regression model indicates that for each level increase in engagement, the average increase in YTD leads is about 20 (\\(\\beta\\) = 20.1, \\(t\\)(407) = 56.2, \\(p\\) &lt; .001). Had we transformed the response variable from its original unit of measurement, the interpretation would be expressed in the transformed units (e.g., \\(\\beta\\) is the square root or natural log of the average change in leads for a one-level increase in engagement). While it may be tempting to conclude that employee engagement has a significant influence on leads based on the model output, we know that bivariate relationships may be spurious; that is, engagement may be correlated with another variable that is actually influencing leads. In practice, a simple linear model is rarely sufficient for explaining a meaningful percent of variance in a response variable, and additional predictors are usually needed to capture the complex and nuanced relationships characteristic of people analytics problems. The \\(R^2\\) value indicates that 8.9% of the variance in leads can be explained by the variation in engagement levels. Put differently, this simple model does not account for 91.1% of variation in leads. Since a large portion of the variance in leads is unexplained, we need signal from additional predictors to understand the other dimensions along which leads vary. Figure 9.6 illustrates how the regression equation for this simple linear model (\\(y\\) = 20.1\\(x\\) + 1.6) fits the data points for sales employees. The spread of leads at each engagement level indicates that there are other factors that explain variance in leads that need to be accounted for in the regression equation to achieve more accurate estimates. The reduction in the spread of leads for a combination of significant predictor values increases \\(R^2\\) (explained variance in YTD leads). Figure 9.6: Simple Linear Model Fit Line for y = 20.1x + 1.6 We can use the extract_eq() function from the equatiomatic library to easily build a properly formatted LaTex regression equation from the model object: # Load library library(equatiomatic) # Convert model to LaTex regression equation extract_eq(slm.fit) \\[ \\operatorname{ytd\\_leads} = \\alpha + \\beta_{1}(\\operatorname{engagement}) + \\epsilon \\] 9.2 Multiple Linear Regression Multiple linear regression extends the simple linear model to one with two or more predictor variables. A multiple regression model generally explains more variance in the response relative to the simple linear model, and is defined by: \\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + {...} + \\beta_p X_p + \\epsilon \\] Once we estimate the unknown model coefficients, \\(\\beta_0\\) through \\(\\beta_p\\), we can estimate \\(Y\\) for a particular combination of values for \\(X_1\\) through \\(X_p\\) by calculating: \\[ \\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta_1}x_{i1} + \\hat{\\beta_2}x_{i2} + {...} + \\hat{\\beta_p} x_{ip} \\] Collinearity Diagnostics In addition to the assumptions we tested in the context of the simple linear model, multiple linear regression warrants collinearity diagnostics. Collinearity refers to situations in which predictors that are related to the response variable also have strong associations with one another. In practice, there is usually some level of collinearity between variables, so the goal of collinearity diagnostics is to identify and address problematic levels of collinearity. Models should be built with predictors that have a strong association with the outcome but not with one another. If predictors are highly correlated with each other, it indicates that they are redundant and do not provide unique information. A large amount of collinearity can cause serious issues with the underlying calculus of a regression model, which can manifest in the form of effects of significant predictors being masked or suppressed or a negative sign/effect showing in the output when a positive association between the predictor and the response actually exists (or vice versa). As a result, it would be premature to fit a linear model before running collinearity diagnostics, as there may be false negatives – predictors that appear unimportant but are actually statistical drivers of the response. If problematic collinearity is not addressed, false conclusions may be drawn from the model output which may lead to bad business decisions. Kuhn and Johnson (2013) recommend the simple procedure outlined below to identify and address problematic collinearity: Determine the two predictors associated with the largest absolute pairwise correlation (whether they are positively or negatively related does not matter) – call them predictors A and B. Determine the average absolute correlation between predictor A and the other variables. Do the same for predictor B. If predictor A has a larger average absolute correlation, remove it; otherwise, remove predictor B. The exception to this rule is when predictors A and B have similar average absolute correlations with all other predictors but the predictor with the slightly higher correlation is a key variable that, if dropped, will prevent you from addressing one or more stated objectives or hypotheses. Repeat steps 1-3 until \\(|r|\\) &lt; .7. Let’s demonstrate the procedures and mechanics for multiple linear regression by estimating YTD sales using multiple predictor variables. While not appropriate in practice, we will select a subset of the available predictors from data to simplify the model and explanation. In Chapter 12, we will discuss the use of machine learning (ML) models for more efficient and comprehensive variable selection. We can leverage the ggpairs() function introduced in Chapter 5 to efficiently compute bivariate correlations and visualize data distributions: Figure 9.7: GGpairs bivariate correlations and data distributions. Based on the correlations, org_tenure is highly correlated with both job_tenure, mgr_tenure and work_exp. These relationships indicate that job and manager changes for those in sales roles have been infrequent since joining the organization, and that a large portion of their work experience has been with this organization. Since org_tenure has the strongest relationship with our response, ytd_sales, we will drop job_tenure, mgr_tenure, and work_exp. Since \\(|r|\\) &lt; .7 for all pairwise relationships, let’s fit the more parsimonious multiple regression model using the resulting subset of predictors. We can include multiple predictors in the model using the + symbol in the lm() function: # Regress YTD sales on a combination of predictors mlm.fit &lt;- lm(ytd_sales ~ engagement + job_lvl + stock_opt_lvl + org_tenure, data) While Kuhn and Johnson’s procedure is a good first step, this may not eliminate what is known as multicollinearity, which is collinearity among three or more predictors. It is possible for collinearity to exist between three or more variables, even in the absence of a strong correlation for a pair of variables. We can evaluate the Variance Inflation Factor (VIF) for the predictors that remain following the bivariate correlation review to ensure multicollinearity is not present. \\(VIF\\) is defined by: \\[ VIF(\\hat{\\beta_j}) = \\frac{1}{1 - R^2_{X_j|X_{-j}}}, \\] where the denominator, \\(R^2_{X_j|X_{-j}}\\), is the \\(R^2\\) from regressing \\(X_j\\) onto all other predictors. The smallest value of \\(VIF\\) is 1, which indicates a complete absence of collinearity. Problematic collinearity exists if \\(VIF\\) for any variable exceeds 5. We can produce \\(VIF\\) for each variable using the vif() function from the car library: # Load library library(car) # Produce VIF for each predictor car::vif(mlm.fit) ## engagement job_lvl stock_opt_lvl org_tenure ## 1.001459 1.407192 1.009435 1.395797 Based on the output, \\(VIF\\) &lt; 5 for each predictor, which indicates that multicollinearity is not an issue. Next, let’s evaluate the linear model assumptions to validate that fitting a linear model to these data is appropriate: Figure 9.8: Multiple linear regression model diagnostics. Based on these visuals, there are obvious violations of linear model assumptions that need to first be addressed. First, given the long right tail for the ytd_sales distribution shown in Figure 9.8, let’s apply a square root transformation to the response variable: # Regress YTD sales on a combination of predictors mlm.fit &lt;- lm(sqrt(ytd_sales) ~ engagement + job_lvl + stock_opt_lvl + org_tenure, data) Additionally, the diagnostic plots indicate that there are data points with high leverage on the fit. Let’s address using Cook’s distance as the criterion: # Remove high leverage observations per Cook&#39;s distance w &lt;- abs(rstudent(mlm.fit)) &lt; 3 &amp; abs(cooks.distance(mlm.fit)) &lt; 4/nrow(mlm.fit$model) mlm.fit &lt;- update(mlm.fit, weights = as.numeric(w)) Now we can produce a refreshed set of diagnostic plots to evaluate the impact of transforming the response variable and removing high leverage observations: Figure 9.9: Multiple linear regression model diagnostics (post-transformation). There is a clear improvement towards satisifying linear model assumptions. Let’s perform the Breusch-Pagan test to validate that the assumption of homoscedasticity is met: # Run the Breusch-Pagan test for evaluate homoscedasticity lmtest::bptest(mlm.fit) ## ## studentized Breusch-Pagan test ## ## data: mlm.fit ## BP = 2.2111, df = 4, p-value = 0.697 Next, let’s ensure residuals are normally distributed around 0: # Produce histogram to visualize distribution of model residuals ggplot2::ggplot() + ggplot2::aes(mlm.fit$residuals) + ggplot2::labs(x = &quot;YTD Leads Residuals&quot;, y = &quot;Density&quot;) + ggplot2::geom_histogram(aes(y = ..density..), fill = &quot;#414141&quot;) + ggplot2::geom_density(fill = &quot;#ADD8E6&quot;, alpha = 0.6) + ggplot2::theme_bw() Figure 9.10: Distribution of model residuals # Compute Shapiro-Wilk test of normality shapiro.test(mlm.fit$residuals) ## ## Shapiro-Wilk normality test ## ## data: mlm.fit$residuals ## W = 0.99342, p-value = 0.07202 Based on the diagnostic plots and statistical tests, our data satisify the requirements for building a multiple linear regression model. Variable Selection Next, we need to reduce our model to the subset of predictors with statistically significant relationships with the response variable. Backward Stepwise Selection is a common and simple variable selection procedure, and the steps are outlined below: Remove the predictor with the highest \\(p\\)-value greater than the critical value (\\(\\alpha\\) = .05). Refit the model, and repeat step 1. Stop when all \\(p\\)-values are less than the critical value. Each predictor in our model has a statistically significant relationship with ytd_sales – indicating that the slope of the relationships with the response is unlikely 0 in the population – so further variable reduction is not required. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-aa632090{}.cl-aa5d6c2c{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aa5d6c40{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aa5d7848{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aa5d785c{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aa5dafb6{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5dafca{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5dafcb{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5dafd4{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5dafde{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5dafdf{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5dafe8{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5daff2{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5daff3{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5daffc{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5daffd{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db006{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db010{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db011{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db01a{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db024{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db02e{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db02f{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db038{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db042{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db043{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db04c{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db056{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db057{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db060{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db06a{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db06b{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db074{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db07e{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db07f{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db088{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db089{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db092{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db09c{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db0a6{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db0a7{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db0b0{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db0ba{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db0c4{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa5db0c5{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} EstimateStandard Errort valuePr(&gt;|t|)(Intercept)125.9816.90318.2500.0000***engagement10.1051.9745.1180.0000***job_lvl33.7682.42513.9220.0000***stock_opt_lvl5.1661.6213.1870.0016**org_tenure6.8120.34119.9520.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05 &lt; '.' &lt; 0.1 &lt; '' &lt; 1Residual standard error: 26.15 on 375 degrees of freedomMultiple R-squared: 0.7783, Adjusted R-squared: 0.7759F-statistic: 329.1 on 375 and 4 DF, p-value: 0.0000 We can view the multiple regression equation for estimating sqrt(ytd_sales) with these four predictors using the extract_eq function: # Convert model to LaTex regression equation extract_eq(mlm.fit) \\[ \\operatorname{sqrt(ytd\\_sales)} = \\alpha + \\beta_{1}(\\operatorname{engagement}) + \\beta_{2}(\\operatorname{job\\_lvl}) + \\beta_{3}(\\operatorname{stock\\_opt\\_lvl}) + \\beta_{4}(\\operatorname{org\\_tenure}) + \\epsilon \\] Based on the model output, the combination of predictors explains about 78% of the variance in YTD sales (\\(R^2\\) = .778). In people analytics settings, it is rare to explain three-quarters of the variance for an outcome given people data are especially noisy. By default, the coefficients on the predictors are unstandardized; that is, they represent the average change in the square root transformed response for each one-unit increase for the respective predictor. Since the predictors have different units of measurement, such as stock_opt_lvl ranging from 0 to 3 and org_tenure ranging from 0 to 40, the unstandardized coefficients cannot be compared to determine which variable has the largest effect on YTD sales. We must standardize these coefficients and adjust for differences in the units of measurement for an apples-to-apples comparison. We can scale variables by subtracting the variable’s mean from \\(x\\) and dividing the difference into the variable’s standard deviation: \\[ x_{scaled} = \\frac{x - \\bar{x}} {s} \\] We can leverage the scale() function to standardize the predictors’ units of measurement and determine which has the largest effect on ytd_sales: .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-aa94e030{}.cl-aa8ec448{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aa8ec45c{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aa8ed136{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aa8ed14a{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aa8f0dd6{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0dea{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0df4{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0dfe{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e08{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e12{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e13{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e1c{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e26{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e30{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e3a{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e3b{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e44{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e45{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e4e{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e58{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e59{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e62{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e6c{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e76{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e77{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e80{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e8a{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e8b{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e94{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0e9e{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ea8{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0eb2{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ebc{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ebd{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ec6{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ed0{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ed1{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0eda{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ee4{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ee5{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0eee{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ef8{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0ef9{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aa8f0f02{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} EstimateStandard Errort valuePr(&gt;|t|)(Intercept)-0.0000.026-0.0001.0000engagement0.1250.0264.7770.0000***job_lvl0.3850.03112.4230.0000***stock_opt_lvl0.0680.0262.5890.0100**org_tenure0.5640.03118.2850.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05 &lt; '.' &lt; 0.1 &lt; '' &lt; 1Residual standard error: 0.527 on 404 degrees of freedomMultiple R-squared: 0.7249, Adjusted R-squared: 0.7222F-statistic: 266.2 on 404 and 4 DF, p-value: 0.0000 Based on the standardized coefficients in Figure ??, org_tenure has the largest effect (\\(\\beta\\) = .56, \\(t\\)(404) = 18.29, \\(p\\) &lt; .001) and job_lvl has the second largest effect (\\(\\beta\\) = .39, \\(t\\)(404) = 12.42, \\(p\\) &lt; .001). 9.3 Model Comparisons Assuming it is warranted by the research objective, it is sometimes helpful to subset data and compare coefficients between models to determine how the strength of associations between predictors and the response compares between cohorts. To illustrate, let’s fit a multiple linear regression model to understand drivers of YTD sales for salespeople with overtime relative to those without overtime. # Partition data into overtime and non-overtime groups data_ot &lt;- subset(data, overtime == &#39;Yes&#39;) data_nonot &lt;- subset(data, overtime == &#39;No&#39;) # Regress transformed YTD sales on a combination of predictors for overtime and non-overtime groups mlm.fit.ot &lt;- lm(sqrt(ytd_sales) ~ engagement + job_lvl + stock_opt_lvl + org_tenure, data_ot) mlm.fit.nonot &lt;- lm(sqrt(ytd_sales) ~ engagement + job_lvl + stock_opt_lvl + org_tenure, data_nonot) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-aac5cbd2{}.cl-aabf25ca{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aabf25de{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aabf32d6{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aabf32ea{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aabf65a8{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65b2{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65bc{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65c6{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65c7{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65d0{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65da{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65e4{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65e5{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65ee{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf65f8{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6602{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf660c{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6616{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6617{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6620{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6621{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf662a{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6634{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf663e{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf663f{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6648{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6652{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6653{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf665c{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6666{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6667{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6670{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf667a{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf667b{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6684{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf668e{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6698{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6699{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66a2{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66ac{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66b6{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66c0{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66c1{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66ca{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66d4{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66de{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66e8{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66e9{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66f2{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf66fc{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6706{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aabf6707{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} EstimateStandard Errort valuePr(&gt;|t|)(Intercept)121.81514.7678.2490.0000***engagement13.1714.5692.8830.0047**job_lvl35.9834.7547.5700.0000***stock_opt_lvl7.1393.3422.1360.0348*org_tenure5.3690.7227.4370.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05 &lt; '.' &lt; 0.1 &lt; '' &lt; 1Residual standard error: 32.78 on 113 degrees of freedomMultiple R-squared: 0.688, Adjusted R-squared: 0.6769F-statistic: 62.29 on 113 and 4 DF, p-value: 0.0000 .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-aaf60f36{}.cl-aaeff86c{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aaeff880{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-aaf004ce{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aaf004e2{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-aaf03c1e{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c32{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c3c{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c3d{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c46{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c50{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c5a{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c5b{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c64{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c6e{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c6f{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c78{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c79{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c82{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c8c{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c8d{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03c96{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03ca0{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03ca1{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03caa{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cb4{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cb5{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cbe{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cbf{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cc8{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cd2{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cdc{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cdd{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03ce6{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03ce7{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cf0{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03cfa{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03d04{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03d05{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03d0e{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03d0f{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03d18{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03d22{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03d23{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-aaf03d2c{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} EstimateStandard Errort valuePr(&gt;|t|)(Intercept)132.3398.66915.2650.0000***engagement9.8522.4723.9850.0001***job_lvl33.1403.00111.0420.0000***stock_opt_lvl4.6382.1592.1480.0325*org_tenure6.0430.40414.9640.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05 &lt; '.' &lt; 0.1 &lt; '' &lt; 1Residual standard error: 29.98 on 286 degrees of freedomMultiple R-squared: 0.7332, Adjusted R-squared: 0.7295F-statistic: 196.5 on 286 and 4 DF, p-value: 0.0000 Since we are comparing two models, we need not scale the variables since comparing a specific predictor’s relationship with the response in the overtime model can be juxtaposed against the same predictor in the non-overtime model using the original units of measurement. Based on the output shown in figures in Figure ?? and in Figure ??, the model for salespeople who worked overtime explains more variance in square root transformed ytd_sales (\\(R^2\\) = .73) relative to the model for salespeople without overtime (\\(R^2\\) = .69). We can see that engagement has a larger effect on the transformed response among salespeople who worked overtime (\\(\\beta\\) = 13.17, \\(t\\)(113) = 2.88, \\(p\\) &lt; .01) relative to those who worked no overtime (\\(\\beta\\) = 9.85, \\(t\\)(286) = 3.99, \\(p\\) &lt; .001). In addition, job_lvl has a stronger association with the response in the overtime group (\\(\\beta\\) = 35.98, \\(t\\)(113) = 7.57, \\(p\\) &lt; .01) relative to the non-overtime group (\\(\\beta\\) = 33.14, \\(t\\)(286) = 11.04, \\(p\\) &lt; .001). Given that the intercept (average square root of ytd_sales when the values of all predictors are set to 0) is higher for the non-overtime group (\\(\\beta\\) = 132.34, \\(t\\)(286) = 15.27, \\(p\\) &lt; .001) than for the overtime group (\\(\\beta\\) = 121.82, \\(t\\)(113) = 8.25, \\(p\\) &lt; .001), differences in the coefficients on job_lvl may indicate that one’s job level is a proxy for skill and capacity to sell more in fewer hours. Hierarchical Regression This multiple model approach can also be useful for understanding the incremental value a given variable – or set of variables – provides above and beyond a set of control variables. This approach is known as hierarchical regression, in which variables are added to the model in steps and changes in model statistics are evaluated after each. Let’s use hierarchical regression to test the hypothesis below. H1: Among salespeople who work overtime, engagement has a significant positive relationship with YTD sales after controlling for job level, stock option level, and organization tenure. .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-ab2896d6{}.cl-ab219e26{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ab219e3a{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ab21b64a{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ab21b668{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ab2206d6{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2206ea{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2206fe{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220708{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220712{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220713{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab22071c{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220726{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220730{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab22073a{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab22073b{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220744{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab22074e{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220758{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220759{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220762{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab22076c{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220776{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220780{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab22078a{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220794{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab22079e{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207a8{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207a9{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207b2{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207bc{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207c6{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207c7{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207d0{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207da{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207e4{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207ee{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207f8{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab2207f9{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab220802{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} EstimateStandard Errort valuePr(&gt;|t|)(Intercept)154.6979.67615.9880.0000***job_lvl37.5724.8717.7140.0000***stock_opt_lvl5.2403.3791.5500.1238org_tenure5.4930.7437.3890.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05 &lt; '.' &lt; 0.1 &lt; '' &lt; 1Residual standard error: 33.81 on 114 degrees of freedomMultiple R-squared: 0.665, Adjusted R-squared: 0.6562F-statistic: 75.44 on 114 and 3 DF, p-value: 0.0000 .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-ab606732{}.cl-ab584b2e{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ab584b4c{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ab5859ac{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ab5859c0{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ab58a1dc{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a1f0{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a1fa{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a1fb{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a204{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a20e{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a20f{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a218{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a222{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a22c{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a236{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a237{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a240{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a24a{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a24b{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a254{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a25e{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a25f{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a268{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a272{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a273{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a27c{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a286{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a290{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a291{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a29a{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2a4{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2a5{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2ae{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2b8{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2b9{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2c2{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2cc{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2d6{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2d7{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2e0{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2ea{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2f4{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2f5{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a2fe{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a308{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a309{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a31c{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a31d{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a326{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a327{width:84.1pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a330{width:52.9pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab58a33a{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} EstimateStandard Errort valuePr(&gt;|t|)(Intercept)121.81514.7678.2490.0000***engagement13.1714.5692.8830.0047**job_lvl35.9834.7547.5700.0000***stock_opt_lvl7.1393.3422.1360.0348*org_tenure5.3690.7227.4370.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05 &lt; '.' &lt; 0.1 &lt; '' &lt; 1Residual standard error: 32.78 on 113 degrees of freedomMultiple R-squared: 0.688, Adjusted R-squared: 0.6769F-statistic: 62.29 on 113 and 4 DF, p-value: 0.0000 Comparing Figure ?? to Figure ??, we can determine that the addition of engagement to the control set explains an additional 2% of the variance in YTD sales (\\(\\Delta R^2 = .69 - .67 = .02\\)). In addition, Figure ?? shows that without engagement in the model, stock_opt_lvl is not significant. This is a good reminder that regression does not examine bivariate relationships of each predictor with the response independent of other variables; rather, the relationships among all variables in the model impact which predictors emerge as having a statistical association with the response. 9.3.1 Moderation As discussed in Chapter 3, a moderating variable is a third variable which amplifies (strengthens) or attenuates (weakens) the relationship between an IV and the response. Accounting for a moderating variable in an linear model requires an interaction term, which is the product of the two variables (\\(X1\\) * \\(X2\\)). Let’s examine whether org_tenure influences the strength of the relationship between job_lvl and sqrt(ytd_sales). We would generally expect sales to increase as the job level of salespeople increases, and longer tenure may amplify the strength of this association. Step one is testing whether the interaction term is statistically significant, and step two is determining the nature of any observed statistical interaction. Including the interaction term in the model (job_lvl * org_tenure) will add the predictors independently and jointly: .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-ab92bc1e{}.cl-ab8ce6f4{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ab8ce708{font-family:'Helvetica';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-ab8cf392{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ab8cf3a6{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-ab8d263c{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2650{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d265a{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2664{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2665{width:108pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d266e{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2678{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2679{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2682{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d268c{width:108pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2696{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2697{width:108pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26a0{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26a1{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26b4{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26b5{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26be{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26c8{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26c9{width:108pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26d2{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26d3{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26dc{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26e6{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26f0{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d26fa{width:108pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2704{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2705{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d270e{width:108pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2718{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2722{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2723{width:63.3pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d272c{width:33.4pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2736{width:54.2pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2737{width:92.7pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-ab8d2740{width:108pt;background-color:transparent;vertical-align: middle;border-bottom: 2pt solid rgba(102, 102, 102, 1.00);border-top: 2pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} EstimateStandard Errort valuePr(&gt;|t|)(Intercept)123.8356.25019.8150.0000***job_lvl50.5733.01216.7910.0000***org_tenure12.4810.86514.4250.0000***job_lvl:org_tenure-2.5140.300-8.3930.0000***Signif. codes: 0 &lt;= '***' &lt; 0.001 &lt; '**' &lt; 0.01 &lt; '*' &lt; 0.05 &lt; '.' &lt; 0.1 &lt; '' &lt; 1Residual standard error: 29.4 on 405 degrees of freedomMultiple R-squared: 0.7412, Adjusted R-squared: 0.7393F-statistic: 386.6 on 405 and 3 DF, p-value: 0.0000 The results show that both the main and interaction effects are statistically significant (\\(p\\) &lt; .001). Since interaction terms can be highly correlated with independent predictors, it is good to check for collinearity: # Produce VIF for each model term car::vif(mlm.fit.int) ## job_lvl org_tenure job_lvl:org_tenure ## 2.189565 9.246489 12.308142 \\(VIF\\) is greater than 5 for both org_tenure and the interaction term; therefore, there is a problematic level of collinearity between these variables. A common method of addressing collinearity in the context of interaction testing is variable centering, in which each value of the predictor is subtracted from its mean (\\(x - \\bar{x}\\)). Like other transformations we have explored, it is important to note that variable centering impacts the interpretation of model coefficients since variables are not modeled using their original units of measurement. # Append centered predictors to data data$job_lvl_cntrd &lt;- data$job_lvl - mean(data$job_lvl) data$org_tenure_cntrd &lt;- data$org_tenure - mean(data$org_tenure) # Regress YTD sales on a combination of centered predictors with interaction term mlm.fit.int &lt;- lm(sqrt(ytd_sales) ~ job_lvl_cntrd * org_tenure_cntrd, data) # Produce VIF for centered variables car::vif(mlm.fit.int) ## job_lvl_cntrd org_tenure_cntrd ## 1.397803 1.773279 ## job_lvl_cntrd:org_tenure_cntrd ## 1.337702 To better understand the nature of the interaction effect (\\(\\beta\\) = -2.51, \\(t\\)(405) = -8.39, \\(p\\) &lt; .001), two equations can be built to evaluate changes in the slope of the relationship with low (\\(\\bar{x}\\) - 1\\(s\\)) and high (\\(\\bar{x}\\) + 1\\(s\\)) organization tenure: Low organization tenure: \\(Y = 123.84 - 2.51 (6.57 - 5.12) * X\\)) High organization tenure: \\(Y = 123.84 - 2.51 (6.57 + 5.12) * X\\)), where 6.57 is mean(data$org_tenure), 5.12 is sd(data$org_tenure), and \\(X\\) is a vector of values for job_lvl. These equations are visualized in Figure ??. 9.3.2 Mediation 9.4 Review Questions "],["glm.html", "10 Generalized Linear Regression 10.1 Polynomial Regression 10.2 Logistic Regression 10.3 Poisson Regression 10.4 Review Questions", " 10 Generalized Linear Regression 10.1 Polynomial Regression 10.2 Logistic Regression Logistic regression is an excellent tool when the outcome is categorical. Logistic regression allows us to model the probability of different classes – a type of modeling often referred to as classification. The context for classification can be binomial for two classes (e.g., active/inactive, promoted/not promoted), multinomial for multiple unordered classes (e.g., skills, job families), or ordinal for multiple ordered classes (e.g., survey items measured on a Likert scale, performance level). 10.2.1 Binomial Logistic Regression 10.2.2 Multinomial Logistic Regression 10.2.3 Ordinal Logistic Regression 10.2.4 Proportional Odds Logistic Regression 10.3 Poisson Regression 10.4 Review Questions "],["surv-anal.html", "11 Survival Analysis 11.1 Kaplan-Meier Survival Curve 11.2 Proportional Hazards 11.3 Review Questions", " 11 Survival Analysis ##Survival and Censorship 11.1 Kaplan-Meier Survival Curve 11.2 Proportional Hazards 11.3 Review Questions "],["pred-mod.html", "12 Predictive Models 12.1 Bias-Variance Trade-Off 12.2 Cross-Validation 12.3 Balancing Classes 12.4 Model Performance 12.5 Automated Machine Learning (AutoML) 12.6 Review Questions", " 12 Predictive Models 12.1 Bias-Variance Trade-Off 12.2 Cross-Validation 12.3 Balancing Classes 12.4 Model Performance 12.5 Automated Machine Learning (AutoML) 12.6 Review Questions "],["unsup-lrn.html", "13 Unsupervised Learning 13.1 Factor Analysis 13.2 Clustering 13.3 Review Questions", " 13 Unsupervised Learning 13.1 Factor Analysis 13.2 Clustering 13.3 Review Questions "],["net-anal.html", "14 Network Analysis 14.1 Centrality Measures 14.2 Review Questions", " 14 Network Analysis 14.1 Centrality Measures Degree Centrality Eigenvector Centrality Betweenness Centrality Closeness Centrality 14.2 Review Questions "],["data-viz.html", "15 Data Visualization 15.1 Design Guidelines 15.2 Visualization Types 15.3 Review Questions", " 15 Data Visualization 15.1 Design Guidelines 15.2 Visualization Types 15.2.1 Tables 15.2.2 Heatmaps 15.2.3 Scatterplots 15.2.4 Line Graphs Single Series Two Series Multiple Series 15.2.5 Slopegraphs 15.2.6 Bar Charts Vertical Horizontal Stacked 15.2.7 Waterfall Charts 15.2.8 Area Charts 15.2.9 Pie Charts 15.3 Review Questions "],["storytelling.html", "16 Data Storytelling 16.1 Know Your Audience 16.2 TL;DR 16.3 Telling the Story 16.4 Reference Material 16.5 Review Questions", " 16 Data Storytelling 16.1 Know Your Audience 16.2 TL;DR 16.3 Telling the Story 16.4 Reference Material 16.5 Review Questions "],["bibli.html", "17 Bibliography", " 17 Bibliography Albright, S. C., &amp; Winston, W. L. (2016). Business Analytics: Data Analysis &amp; Decision Making (6th ed.). Boston, MA: Cengage. Angrist, J. D., &amp; Pischke, J.-S. (2009). Mostly harmless econometrics: An empiricist’s companion. Princeton, NJ: Princeton University Press. Baron, R. M., &amp; Kenny, D. A. (1986). The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. Journal of Personality and Social Psychology, 51(6), 1173–1182. Bartlett, R. (2013). A Practitioner’s Guide to Business Analytics: Using Data Analysis Tools to Improve Your Organization’s Decision Making and Strategy. New York, NY: McGraw-Hill. Chakravarti, I. M., Laha, R. G., &amp; Roy, J. (1967). Handbook of methods of applied statistics. New York: Wiley. Creswell, J. W., &amp; Creswell, J. D. (2018). Research Design: Qualitative, Quantitative, and Mixed Methods Approaches (5th ed.). Los Angeles: Sage. Cronbach, L. J., &amp; Meehl, P. E. (1955). Construct validity in psychological tests. Psychological Bulletin, 52(4), 281–302. https://doi.org/10.1037/h0040957 Daniel, W. W. (1990). Kruskal–Wallis one-way analysis of variance by ranks. Applied Nonparametric Statistics (2nd ed.). Boston: PWS-Kent. pp. 226–234. Daw, R. H., &amp; Pearson, E. S. (1972). Studies in the History of Probability and Statistics. XXX. Abraham De Moivre’s 1733 Derivation of the Normal Curve: A Bibliographical Note. Biometrika, 59(3), 677–680. https://doi.org/10.2307/2334818 Delacre, M., Lakens, D., &amp; Leys, C. (2017). Why Psychologists Should by Default Use Welch’s t-test Instead of Student’s t-test. International Review of Social Psychology, 30(1), 92–101. DOI: http://doi.org/10.5334/irsp.82 DeVellis, R. F. (2012). Scale development: Theory and applications. Thousand Oaks, Calif: Sage. Edmondson, A. (1999). Psychological safety and learning behavior in work teams. Administrative Science Quarterly, 44(2), 350-383. James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013). An Introduction to Statistical Learning: With Applications in R. New York: Springer. Kahneman, D. (2011). Thinking, fast and slow. New York: Farrar, Straus and Giroux. Kerlinger, F., &amp; Lee, H. (2000). Foundations of behavioral research (4th ed.). Melbourne: Wadsworth. Kuhn, M., &amp; Johnson, K. (2013). Applied predictive modeling. New York: Springer. Levene, H. (1960). Robust Tests for Equality of Variances. In: Olkin, I., Ed., Contributions to Probability and Statistics. Palo Alto: Stanford University Press. pp. 278-292. Perneger, T. V. (1998). What’s wrong with Bonferroni adjustments. BMJ, 316(7139), 1236-1238. Pishro-Nik, H. (2014). Introduction to Probability: Statistics and Random Processes. Blue Bell, PA: Kappa Research, LLC. Roethlisberger, F. J., &amp; Dickson, W. J. (1939). Management and the worker. Harvard Univ. Press. Satterthwaite, F. E. (1946). An Approximate Distribution of Estimates of Variance Components. Biometrics Bulletin, 2(6), 110–114. https://doi.org/10.2307/3002019 Shapiro, S. S., &amp; Wilk, M. B. (1965). An analysis of variance test for normality (complete samples). Biometrika, 52(3–4), 591–611. https://doi.org/10.1093/biomet/52.3-4.591 Snedecor, G. W., &amp; Cochran, W. G. (1980). Statistical methods (7th ed.). Ames, Iowa: Iowa State University Press. Spector, P. E. (2006). Method Variance in Organizational Research: Truth or Urban Legend? Organizational Research Methods, 9(2), 221-232. Starbuck, C. R. (2016). Managing insidious barriers to upward communication in organizations: An empirical investigation of relationships among implicit voice theories, power distance orientation, self-efficacy, and leader-directed voice (Doctoral dissertation, Regent University). Vargha, A. &amp; Delaney, H. D. (2000). A Critique and Improvement of the CL Common Language Effect Size Statistics of McGraw and Wong. Journal of Educational and Behavioral Statistics 25(2), 101–132. Wald, Abraham. (1943). A Method of Estimating Plane Vulnerability Based on Damage of Survivors. Statistical Research Group, Columbia University. CRC 432 — reprint from July 1980 Archived 2019-07-13 at the Wayback Machine. Center for Naval Analyses. Welch, B. L. (1947). The Generalization of `Student’s’ Problem when Several Different Population Variances are Involved. Biometrika, 34(1/2), 28–35. https://doi.org/10.2307/2332510 Wheelan, C. (2013). Naked Statistics: Stripping the Dread from the Data. New York: W.W. Norton. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
