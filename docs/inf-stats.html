<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Statistical Inference | The People Analytics Companion: An Applied Guide through the People Analytics Lifecycle</title>
  <meta name="description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Statistical Inference | The People Analytics Companion: An Applied Guide through the People Analytics Lifecycle" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="github-repo" content="crstarbuck/peopleanalytics-lifecycle-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Statistical Inference | The People Analytics Companion: An Applied Guide through the People Analytics Lifecycle" />
  
  <meta name="twitter:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  

<meta name="author" content="Craig Starbuck" />


<meta name="date" content="2022-05-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="desc-stats.html"/>
<link rel="next" href="data-wrang-prep.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The People Analytics Companion: An Applied Guide through the People Analytics Lifecycle</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="dedication.html"><a href="dedication.html"><i class="fa fa-check"></i>Dedication</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting Started</a><ul>
<li class="chapter" data-level="1.1" data-path="getting-started.html"><a href="getting-started.html#guiding-principles"><i class="fa fa-check"></i><b>1.1</b> Guiding Principles</a><ul>
<li class="chapter" data-level="1.1.1" data-path="getting-started.html"><a href="getting-started.html#pro-employee-thinking"><i class="fa fa-check"></i><b>1.1.1</b> Pro Employee Thinking</a></li>
<li class="chapter" data-level="1.1.2" data-path="getting-started.html"><a href="getting-started.html#quality"><i class="fa fa-check"></i><b>1.1.2</b> Quality</a></li>
<li class="chapter" data-level="1.1.3" data-path="getting-started.html"><a href="getting-started.html#prioritization"><i class="fa fa-check"></i><b>1.1.3</b> Prioritization</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="getting-started.html"><a href="getting-started.html#tools"><i class="fa fa-check"></i><b>1.2</b> Tools</a><ul>
<li class="chapter" data-level="1.2.1" data-path="getting-started.html"><a href="getting-started.html#r"><i class="fa fa-check"></i><b>1.2.1</b> R</a></li>
<li class="chapter" data-level="1.2.2" data-path="getting-started.html"><a href="getting-started.html#data"><i class="fa fa-check"></i><b>1.2.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="getting-started.html"><a href="getting-started.html#d-framework"><i class="fa fa-check"></i><b>1.3</b> 4D Framework</a><ul>
<li class="chapter" data-level="1.3.1" data-path="getting-started.html"><a href="getting-started.html#discover"><i class="fa fa-check"></i><b>1.3.1</b> Discover</a></li>
<li class="chapter" data-level="1.3.2" data-path="getting-started.html"><a href="getting-started.html#design"><i class="fa fa-check"></i><b>1.3.2</b> Design</a></li>
<li class="chapter" data-level="1.3.3" data-path="getting-started.html"><a href="getting-started.html#develop"><i class="fa fa-check"></i><b>1.3.3</b> Develop</a></li>
<li class="chapter" data-level="1.3.4" data-path="getting-started.html"><a href="getting-started.html#deliver"><i class="fa fa-check"></i><b>1.3.4</b> Deliver</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="r-intro.html"><a href="r-intro.html"><i class="fa fa-check"></i><b>2</b> Introduction to R</a><ul>
<li class="chapter" data-level="2.1" data-path="r-intro.html"><a href="r-intro.html#getting-started-1"><i class="fa fa-check"></i><b>2.1</b> Getting Started</a></li>
<li class="chapter" data-level="2.2" data-path="r-intro.html"><a href="r-intro.html#vectors"><i class="fa fa-check"></i><b>2.2</b> Vectors</a></li>
<li class="chapter" data-level="2.3" data-path="r-intro.html"><a href="r-intro.html#matrices"><i class="fa fa-check"></i><b>2.3</b> Matrices</a></li>
<li class="chapter" data-level="2.4" data-path="r-intro.html"><a href="r-intro.html#factors"><i class="fa fa-check"></i><b>2.4</b> Factors</a></li>
<li class="chapter" data-level="2.5" data-path="r-intro.html"><a href="r-intro.html#data-frames"><i class="fa fa-check"></i><b>2.5</b> Data Frames</a></li>
<li class="chapter" data-level="2.6" data-path="r-intro.html"><a href="r-intro.html#lists"><i class="fa fa-check"></i><b>2.6</b> Lists</a></li>
<li class="chapter" data-level="2.7" data-path="r-intro.html"><a href="r-intro.html#loops"><i class="fa fa-check"></i><b>2.7</b> Loops</a></li>
<li class="chapter" data-level="2.8" data-path="r-intro.html"><a href="r-intro.html#user-defined-functions-udfs"><i class="fa fa-check"></i><b>2.8</b> User-Defined Functions (UDFs)</a></li>
<li class="chapter" data-level="2.9" data-path="r-intro.html"><a href="r-intro.html#graphics"><i class="fa fa-check"></i><b>2.9</b> Graphics</a></li>
<li class="chapter" data-level="2.10" data-path="r-intro.html"><a href="r-intro.html#review-questions"><i class="fa fa-check"></i><b>2.10</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="measure-sampl.html"><a href="measure-sampl.html"><i class="fa fa-check"></i><b>3</b> Measurement &amp; Sampling</a><ul>
<li class="chapter" data-level="3.1" data-path="measure-sampl.html"><a href="measure-sampl.html#variable-types"><i class="fa fa-check"></i><b>3.1</b> Variable Types</a><ul>
<li class="chapter" data-level="3.1.1" data-path="measure-sampl.html"><a href="measure-sampl.html#independent-variables-iv"><i class="fa fa-check"></i><b>3.1.1</b> Independent Variables (IV)</a></li>
<li class="chapter" data-level="3.1.2" data-path="measure-sampl.html"><a href="measure-sampl.html#dependent-variables-dv"><i class="fa fa-check"></i><b>3.1.2</b> Dependent Variables (DV)</a></li>
<li class="chapter" data-level="3.1.3" data-path="measure-sampl.html"><a href="measure-sampl.html#control-variables-cv"><i class="fa fa-check"></i><b>3.1.3</b> Control Variables (CV)</a></li>
<li class="chapter" data-level="3.1.4" data-path="measure-sampl.html"><a href="measure-sampl.html#moderating-variables"><i class="fa fa-check"></i><b>3.1.4</b> Moderating Variables</a></li>
<li class="chapter" data-level="3.1.5" data-path="measure-sampl.html"><a href="measure-sampl.html#mediating-variables"><i class="fa fa-check"></i><b>3.1.5</b> Mediating Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="measure-sampl.html"><a href="measure-sampl.html#measurement-scales"><i class="fa fa-check"></i><b>3.2</b> Measurement Scales</a><ul>
<li class="chapter" data-level="3.2.1" data-path="measure-sampl.html"><a href="measure-sampl.html#discrete-variables"><i class="fa fa-check"></i><b>3.2.1</b> Discrete Variables</a></li>
<li class="chapter" data-level="3.2.2" data-path="measure-sampl.html"><a href="measure-sampl.html#continuous-variables"><i class="fa fa-check"></i><b>3.2.2</b> Continuous Variables</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="measure-sampl.html"><a href="measure-sampl.html#sampling"><i class="fa fa-check"></i><b>3.3</b> Sampling</a><ul>
<li class="chapter" data-level="3.3.1" data-path="measure-sampl.html"><a href="measure-sampl.html#sampling-nonsampling-error"><i class="fa fa-check"></i><b>3.3.1</b> Sampling &amp; Nonsampling Error</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="measure-sampl.html"><a href="measure-sampl.html#review-questions-1"><i class="fa fa-check"></i><b>3.4</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="research.html"><a href="research.html"><i class="fa fa-check"></i><b>4</b> Research Fundamentals</a><ul>
<li class="chapter" data-level="4.1" data-path="research.html"><a href="research.html#research-questions"><i class="fa fa-check"></i><b>4.1</b> Research Questions</a></li>
<li class="chapter" data-level="4.2" data-path="research.html"><a href="research.html#research-hypotheses"><i class="fa fa-check"></i><b>4.2</b> Research Hypotheses</a></li>
<li class="chapter" data-level="4.3" data-path="research.html"><a href="research.html#internal-vs.external-validity"><i class="fa fa-check"></i><b>4.3</b> Internal vs. External Validity</a></li>
<li class="chapter" data-level="4.4" data-path="research.html"><a href="research.html#research-methods"><i class="fa fa-check"></i><b>4.4</b> Research Methods</a></li>
<li class="chapter" data-level="4.5" data-path="research.html"><a href="research.html#research-designs"><i class="fa fa-check"></i><b>4.5</b> Research Designs</a></li>
<li class="chapter" data-level="4.6" data-path="research.html"><a href="research.html#review-questions-2"><i class="fa fa-check"></i><b>4.6</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="desc-stats.html"><a href="desc-stats.html"><i class="fa fa-check"></i><b>5</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="5.1" data-path="desc-stats.html"><a href="desc-stats.html#univariate-analysis"><i class="fa fa-check"></i><b>5.1</b> Univariate Analysis</a><ul>
<li class="chapter" data-level="5.1.1" data-path="desc-stats.html"><a href="desc-stats.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>5.1.1</b> Measures of Central Tendency</a></li>
<li class="chapter" data-level="5.1.2" data-path="desc-stats.html"><a href="desc-stats.html#measures-of-spread"><i class="fa fa-check"></i><b>5.1.2</b> Measures of Spread</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="desc-stats.html"><a href="desc-stats.html#bivariate-analysis"><i class="fa fa-check"></i><b>5.2</b> Bivariate Analysis</a><ul>
<li class="chapter" data-level="5.2.1" data-path="desc-stats.html"><a href="desc-stats.html#covariance"><i class="fa fa-check"></i><b>5.2.1</b> Covariance</a></li>
<li class="chapter" data-level="5.2.2" data-path="desc-stats.html"><a href="desc-stats.html#correlation"><i class="fa fa-check"></i><b>5.2.2</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="desc-stats.html"><a href="desc-stats.html#review-questions-3"><i class="fa fa-check"></i><b>5.3</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inf-stats.html"><a href="inf-stats.html"><i class="fa fa-check"></i><b>6</b> Statistical Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="inf-stats.html"><a href="inf-stats.html#introduction-to-probability"><i class="fa fa-check"></i><b>6.1</b> Introduction to Probability</a><ul>
<li class="chapter" data-level="6.1.1" data-path="inf-stats.html"><a href="inf-stats.html#probability-distributions"><i class="fa fa-check"></i><b>6.1.1</b> Probability Distributions</a></li>
<li class="chapter" data-level="6.1.2" data-path="inf-stats.html"><a href="inf-stats.html#conditional-probability"><i class="fa fa-check"></i><b>6.1.2</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inf-stats.html"><a href="inf-stats.html#central-limit-theorem"><i class="fa fa-check"></i><b>6.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="6.3" data-path="inf-stats.html"><a href="inf-stats.html#confidence-intervals"><i class="fa fa-check"></i><b>6.3</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="6.3.1" data-path="inf-stats.html"><a href="inf-stats.html#hypothesis-testing"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="6.3.2" data-path="inf-stats.html"><a href="inf-stats.html#alpha"><i class="fa fa-check"></i><b>6.3.2</b> Alpha</a></li>
<li class="chapter" data-level="6.3.3" data-path="inf-stats.html"><a href="inf-stats.html#beta"><i class="fa fa-check"></i><b>6.3.3</b> Beta</a></li>
<li class="chapter" data-level="6.3.4" data-path="inf-stats.html"><a href="inf-stats.html#type-i-ii-errors"><i class="fa fa-check"></i><b>6.3.4</b> Type I &amp; II Errors</a></li>
<li class="chapter" data-level="6.3.5" data-path="inf-stats.html"><a href="inf-stats.html#p-values"><i class="fa fa-check"></i><b>6.3.5</b> <span class="math inline">\(p\)</span>-Values</a></li>
<li class="chapter" data-level="6.3.6" data-path="inf-stats.html"><a href="inf-stats.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.3.6</b> Bonferroni Correction</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inf-stats.html"><a href="inf-stats.html#review-questions-4"><i class="fa fa-check"></i><b>6.4</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html"><i class="fa fa-check"></i><b>7</b> Data Wrangling and Preparation</a><ul>
<li class="chapter" data-level="7.1" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#data-management"><i class="fa fa-check"></i><b>7.1</b> Data Management</a></li>
<li class="chapter" data-level="7.2" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#sql"><i class="fa fa-check"></i><b>7.2</b> SQL</a></li>
<li class="chapter" data-level="7.3" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#data-screening-cleaning"><i class="fa fa-check"></i><b>7.3</b> Data Screening &amp; Cleaning</a></li>
<li class="chapter" data-level="7.4" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#one-hot-encoding"><i class="fa fa-check"></i><b>7.4</b> One-Hot Encoding</a></li>
<li class="chapter" data-level="7.5" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#feature-engineering"><i class="fa fa-check"></i><b>7.5</b> Feature Engineering</a></li>
<li class="chapter" data-level="7.6" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#review-questions-5"><i class="fa fa-check"></i><b>7.6</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="aod.html"><a href="aod.html"><i class="fa fa-check"></i><b>8</b> Analysis of Differences</a><ul>
<li class="chapter" data-level="8.1" data-path="aod.html"><a href="aod.html#parametric-vs.nonparametric-tests"><i class="fa fa-check"></i><b>8.1</b> Parametric vs. Nonparametric Tests</a></li>
<li class="chapter" data-level="8.2" data-path="aod.html"><a href="aod.html#differences-in-discrete-data"><i class="fa fa-check"></i><b>8.2</b> Differences in Discrete Data</a></li>
<li class="chapter" data-level="8.3" data-path="aod.html"><a href="aod.html#differences-in-continuous-data"><i class="fa fa-check"></i><b>8.3</b> Differences in Continuous Data</a></li>
<li class="chapter" data-level="8.4" data-path="aod.html"><a href="aod.html#review-questions-6"><i class="fa fa-check"></i><b>8.4</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>9</b> Linear Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="lm.html"><a href="lm.html#simple-linear-regression"><i class="fa fa-check"></i><b>9.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="9.2" data-path="lm.html"><a href="lm.html#multiple-linear-regression"><i class="fa fa-check"></i><b>9.2</b> Multiple Linear Regression</a></li>
<li class="chapter" data-level="9.3" data-path="lm.html"><a href="lm.html#model-comparisons"><i class="fa fa-check"></i><b>9.3</b> Model Comparisons</a><ul>
<li class="chapter" data-level="9.3.1" data-path="lm.html"><a href="lm.html#moderation"><i class="fa fa-check"></i><b>9.3.1</b> Moderation</a></li>
<li class="chapter" data-level="9.3.2" data-path="lm.html"><a href="lm.html#mediation"><i class="fa fa-check"></i><b>9.3.2</b> Mediation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="lm.html"><a href="lm.html#review-questions-7"><i class="fa fa-check"></i><b>9.4</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>10</b> Generalized Linear Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="glm.html"><a href="glm.html#polynomial-regression"><i class="fa fa-check"></i><b>10.1</b> Polynomial Regression</a></li>
<li class="chapter" data-level="10.2" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>10.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="10.2.1" data-path="glm.html"><a href="glm.html#binomial-logistic-regression"><i class="fa fa-check"></i><b>10.2.1</b> Binomial Logistic Regression</a></li>
<li class="chapter" data-level="10.2.2" data-path="glm.html"><a href="glm.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>10.2.2</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="10.2.3" data-path="glm.html"><a href="glm.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>10.2.3</b> Ordinal Logistic Regression</a></li>
<li class="chapter" data-level="10.2.4" data-path="glm.html"><a href="glm.html#proportional-odds-logistic-regression"><i class="fa fa-check"></i><b>10.2.4</b> Proportional Odds Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>10.3</b> Poisson Regression</a></li>
<li class="chapter" data-level="10.4" data-path="glm.html"><a href="glm.html#review-questions-8"><i class="fa fa-check"></i><b>10.4</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="surv-anal.html"><a href="surv-anal.html"><i class="fa fa-check"></i><b>11</b> Survival Analysis</a><ul>
<li class="chapter" data-level="11.1" data-path="surv-anal.html"><a href="surv-anal.html#kaplan-meier-survival-curve"><i class="fa fa-check"></i><b>11.1</b> Kaplan-Meier Survival Curve</a></li>
<li class="chapter" data-level="11.2" data-path="surv-anal.html"><a href="surv-anal.html#proportional-hazards"><i class="fa fa-check"></i><b>11.2</b> Proportional Hazards</a></li>
<li class="chapter" data-level="11.3" data-path="surv-anal.html"><a href="surv-anal.html#review-questions-9"><i class="fa fa-check"></i><b>11.3</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="pred-mod.html"><a href="pred-mod.html"><i class="fa fa-check"></i><b>12</b> Predictive Models</a><ul>
<li class="chapter" data-level="12.1" data-path="pred-mod.html"><a href="pred-mod.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>12.1</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="12.2" data-path="pred-mod.html"><a href="pred-mod.html#cross-validation"><i class="fa fa-check"></i><b>12.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="12.3" data-path="pred-mod.html"><a href="pred-mod.html#balancing-classes"><i class="fa fa-check"></i><b>12.3</b> Balancing Classes</a></li>
<li class="chapter" data-level="12.4" data-path="pred-mod.html"><a href="pred-mod.html#model-performance"><i class="fa fa-check"></i><b>12.4</b> Model Performance</a></li>
<li class="chapter" data-level="12.5" data-path="pred-mod.html"><a href="pred-mod.html#automated-machine-learning-automl"><i class="fa fa-check"></i><b>12.5</b> Automated Machine Learning (AutoML)</a></li>
<li class="chapter" data-level="12.6" data-path="pred-mod.html"><a href="pred-mod.html#review-questions-10"><i class="fa fa-check"></i><b>12.6</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="unsup-lrn.html"><a href="unsup-lrn.html"><i class="fa fa-check"></i><b>13</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="13.1" data-path="unsup-lrn.html"><a href="unsup-lrn.html#factor-analysis"><i class="fa fa-check"></i><b>13.1</b> Factor Analysis</a></li>
<li class="chapter" data-level="13.2" data-path="unsup-lrn.html"><a href="unsup-lrn.html#clustering"><i class="fa fa-check"></i><b>13.2</b> Clustering</a></li>
<li class="chapter" data-level="13.3" data-path="unsup-lrn.html"><a href="unsup-lrn.html#review-questions-11"><i class="fa fa-check"></i><b>13.3</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="net-anal.html"><a href="net-anal.html"><i class="fa fa-check"></i><b>14</b> Network Analysis</a><ul>
<li class="chapter" data-level="14.1" data-path="net-anal.html"><a href="net-anal.html#centrality-measures"><i class="fa fa-check"></i><b>14.1</b> Centrality Measures</a></li>
<li class="chapter" data-level="14.2" data-path="net-anal.html"><a href="net-anal.html#review-questions-12"><i class="fa fa-check"></i><b>14.2</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="data-viz.html"><a href="data-viz.html"><i class="fa fa-check"></i><b>15</b> Data Visualization</a><ul>
<li class="chapter" data-level="15.1" data-path="data-viz.html"><a href="data-viz.html#design-guidelines"><i class="fa fa-check"></i><b>15.1</b> Design Guidelines</a></li>
<li class="chapter" data-level="15.2" data-path="data-viz.html"><a href="data-viz.html#visualization-types"><i class="fa fa-check"></i><b>15.2</b> Visualization Types</a><ul>
<li class="chapter" data-level="15.2.1" data-path="data-viz.html"><a href="data-viz.html#tables"><i class="fa fa-check"></i><b>15.2.1</b> Tables</a></li>
<li class="chapter" data-level="15.2.2" data-path="data-viz.html"><a href="data-viz.html#heatmaps"><i class="fa fa-check"></i><b>15.2.2</b> Heatmaps</a></li>
<li class="chapter" data-level="15.2.3" data-path="data-viz.html"><a href="data-viz.html#scatterplots"><i class="fa fa-check"></i><b>15.2.3</b> Scatterplots</a></li>
<li class="chapter" data-level="15.2.4" data-path="data-viz.html"><a href="data-viz.html#line-graphs"><i class="fa fa-check"></i><b>15.2.4</b> Line Graphs</a></li>
<li class="chapter" data-level="15.2.5" data-path="data-viz.html"><a href="data-viz.html#slopegraphs"><i class="fa fa-check"></i><b>15.2.5</b> Slopegraphs</a></li>
<li class="chapter" data-level="15.2.6" data-path="data-viz.html"><a href="data-viz.html#bar-charts"><i class="fa fa-check"></i><b>15.2.6</b> Bar Charts</a></li>
<li class="chapter" data-level="15.2.7" data-path="data-viz.html"><a href="data-viz.html#waterfall-charts"><i class="fa fa-check"></i><b>15.2.7</b> Waterfall Charts</a></li>
<li class="chapter" data-level="15.2.8" data-path="data-viz.html"><a href="data-viz.html#area-charts"><i class="fa fa-check"></i><b>15.2.8</b> Area Charts</a></li>
<li class="chapter" data-level="15.2.9" data-path="data-viz.html"><a href="data-viz.html#pie-charts"><i class="fa fa-check"></i><b>15.2.9</b> Pie Charts</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="data-viz.html"><a href="data-viz.html#review-questions-13"><i class="fa fa-check"></i><b>15.3</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="storytelling.html"><a href="storytelling.html"><i class="fa fa-check"></i><b>16</b> Data Storytelling</a><ul>
<li class="chapter" data-level="16.1" data-path="storytelling.html"><a href="storytelling.html#know-your-audience"><i class="fa fa-check"></i><b>16.1</b> Know Your Audience</a></li>
<li class="chapter" data-level="16.2" data-path="storytelling.html"><a href="storytelling.html#tldr"><i class="fa fa-check"></i><b>16.2</b> TL;DR</a></li>
<li class="chapter" data-level="16.3" data-path="storytelling.html"><a href="storytelling.html#telling-the-story"><i class="fa fa-check"></i><b>16.3</b> Telling the Story</a></li>
<li class="chapter" data-level="16.4" data-path="storytelling.html"><a href="storytelling.html#reference-material"><i class="fa fa-check"></i><b>16.4</b> Reference Material</a></li>
<li class="chapter" data-level="16.5" data-path="storytelling.html"><a href="storytelling.html#review-questions-14"><i class="fa fa-check"></i><b>16.5</b> Review Questions</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="bibli.html"><a href="bibli.html"><i class="fa fa-check"></i><b>17</b> Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The People Analytics Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="inf-stats" class="section level1">
<h1><span class="header-section-number">6</span> Statistical Inference</h1>
<p>The objective of <strong>inferential statistics</strong> is to make inferences – with some degree of confidence – about a population based on available sample data. Several related concepts are fundamental to this goal and will be covered here.</p>
<div id="introduction-to-probability" class="section level2">
<h2><span class="header-section-number">6.1</span> Introduction to Probability</h2>
<p>Randomness and uncertainty exist all around us. In <strong>probability theory</strong>, random phenomena refer to events or experiments whose outcomes cannot be predicted with certainty (Pishro-Nik, 2014). If you’ve taken a course in probability, there is a good chance you have considered the case of a fair coin flip – one of the most intuitive applications of probability. In the absence of information on how the coin is flipped, we cannot be certain of the outcome. What we can be certain of is that with a large number of coin flips, the proportion of heads will become increasingly close to 50%, or <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<p>The <strong>Law of Large Numbers (LLN)</strong> is an important theorem for building an intuitive understanding of how probability relates to the statistical inference concepts we will cover. In the case of a fair coin flip, it is possible to observe many consecutive heads by chance. This is because small samples can lend to anomalies. However, as the number of flips increases, we will undoubtedly observe an increasing number of tails; we expect a roughly equal number of heads and tails with a large enough number of flips.</p>
<div id="probability-distributions" class="section level3">
<h3><span class="header-section-number">6.1.1</span> Probability Distributions</h3>
<p><strong>Probability distributions</strong> are statistical functions that yield the probability of obtaining possible values for a random variable. Probabilities range from 0 to 1, where the probability of a definite event is 1 and the probability of an impossible event is 0. The <strong>empirical probability (or experimental probability)</strong> of an event is the fraction of times it occurred relative to the total number of repetitions. Since a probability distribution defines the likelihood of observing all possible outcomes of an event or experiment, the sum of all probabilities for all possible values must equal 1.</p>
<p>For example, let’s look at how org tenure is distributed across employees. We can understand the general shape of the distribution using descriptive statistics:</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb158-1" data-line-number="1"><span class="co"># Load library for data wrangling</span></a>
<a class="sourceLine" id="cb158-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb158-3" data-line-number="3"></a>
<a class="sourceLine" id="cb158-4" data-line-number="4"><span class="co"># Read employee data</span></a>
<a class="sourceLine" id="cb158-5" data-line-number="5">employees &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv&quot;</span>)</a>
<a class="sourceLine" id="cb158-6" data-line-number="6"></a>
<a class="sourceLine" id="cb158-7" data-line-number="7"><span class="co"># Produce descriptive stats for org tenure</span></a>
<a class="sourceLine" id="cb158-8" data-line-number="8"><span class="kw">summary</span>(employees<span class="op">$</span>org_tenure)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   0.000   3.000   5.000   7.032   9.000  72.000</code></pre>
<p><br /></p>
<p>Comparing the higher mean value of 9.2 to the median value of 8 indicates there are larger values skewing the mean upward which can be seen in the Q3 and max values.</p>
<p>Beyond descriptives, visuals are often helpful in understanding a variable’s distribution. As shown in Figure <a href="inf-stats.html#fig:org-tenure-dist">6.1</a>, it is clear that org tenure is positively skewed, and understanding the shape (or spread) of this distribution enables us to identify which values are most likely in order to estimate the likelihood of different results:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb160-1" data-line-number="1"><span class="co"># Visualize org tenure distribution</span></a>
<a class="sourceLine" id="cb160-2" data-line-number="2">ggplot2<span class="op">::</span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb160-3" data-line-number="3">ggplot2<span class="op">::</span><span class="kw">aes</span>(employees<span class="op">$</span>org_tenure) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb160-4" data-line-number="4">ggplot2<span class="op">::</span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;Org Tenure&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb160-5" data-line-number="5">ggplot2<span class="op">::</span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb160-6" data-line-number="6">ggplot2<span class="op">::</span><span class="kw">geom_density</span>(<span class="dt">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb160-7" data-line-number="7">ggplot2<span class="op">::</span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:org-tenure-dist"></span>
<img src="The_People_Analytics_Companion_files/figure-html/org-tenure-dist-1.png" alt="Org Tenure Distribution" width="672" />
<p class="caption">
Figure 6.1: Org Tenure Distribution
</p>
</div>
<p><br /></p>
<p>You can likely image the shape of probability distributions for many common events. If we consider the probability of employees exiting an organization, the outcome is binary. That is, employees either leave or stay; there are no options between these extremes. However, the distribution of performance scores will look quite different. Most organizations have expected – or even forced – distributions in which an average rating is awarded most frequently and low and high performance ratings less frequently. This would start to look more like a bell curve as the number of performance levels increases.</p>
<p>Just as we grouped variables into discrete and continuous categories in Chapter <a href="measure-sampl.html#measure-sampl">3</a>, this is also how probability distributions are categorized. If you read Chapter <a href="measure-sampl.html#measure-sampl">3</a>, you likely already have some a priori expectations about the characteristics of discrete and continuous distributions.</p>
<p>The shape of a probability distribution is defined by parameters, which represent its essential properties (e.g., measures of central tendency and spread). These probability distributions underpin the many types of statistical tests covered in this book.</p>
<p><strong>Discrete Probability Distributions</strong></p>
<p><strong>Discrete probability distributions</strong>, also known as <strong>Probability Mass Functions (PMF)</strong>, can be leveraged to model different types of nominal and ordinal variables. Some common discrete distributions include:</p>
<ul>
<li><strong>Bernoulli</strong>: probability of success or failure for a <em>single</em> trial with two outcomes</li>
<li><strong>Binomial</strong>: number of successes and failures in a <em>sequence</em> of independent trials with two outcomes (collection of Bernoulli trials)</li>
<li><strong>Multinomial</strong>: generalization of the binomial distribution for experiments with more than two outcomes</li>
<li><strong>Negative Binomial (Pascal)</strong>: a version of the binomial distribution for a <em>fixed</em> number of trials (this is positively skewed despite what the name might suggest)</li>
<li><strong>Poisson</strong>: probability of a given number of events occurring over a specified period</li>
<li><strong>Geometric</strong>: special case of the negative binomial distribution that repeats trials until a success is observed (rather than a fixed number of times)</li>
</ul>
<p>Several functions are available in R to simulate PMFs. The precise shape of a distribution depends on the parameters, but we will simulate and visualize these common PMFs to illustrate differences in the general shape of each. First, let’s simulate the distributions by drawing 1,000 random values from each with a specified set of parameters:</p>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" data-line-number="1"><span class="co"># Set seed for reproducible random distribution</span></a>
<a class="sourceLine" id="cb161-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb161-3" data-line-number="3"></a>
<a class="sourceLine" id="cb161-4" data-line-number="4"><span class="co"># Simulate bernoulli distribution</span></a>
<a class="sourceLine" id="cb161-5" data-line-number="5">bernoulli_dist &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1000</span>, <span class="dv">1</span>, <span class="dt">prob =</span> <span class="fl">.5</span>)</a>
<a class="sourceLine" id="cb161-6" data-line-number="6"></a>
<a class="sourceLine" id="cb161-7" data-line-number="7"><span class="co"># Simulate binomial distribution</span></a>
<a class="sourceLine" id="cb161-8" data-line-number="8"><span class="co"># Notice the important difference relative to the Bernoulli simulation (100 trials vs. 1)</span></a>
<a class="sourceLine" id="cb161-9" data-line-number="9">binomial_dist &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1000</span>, <span class="dv">100</span>, <span class="dt">prob =</span> <span class="fl">.5</span>) </a>
<a class="sourceLine" id="cb161-10" data-line-number="10"></a>
<a class="sourceLine" id="cb161-11" data-line-number="11"><span class="co"># Simulate negative binomial distribution</span></a>
<a class="sourceLine" id="cb161-12" data-line-number="12">nbinomial_dist &lt;-<span class="st"> </span><span class="kw">rnbinom</span>(<span class="dv">1000</span>, <span class="dv">100</span>, <span class="dt">prob =</span> <span class="fl">.5</span>) </a>
<a class="sourceLine" id="cb161-13" data-line-number="13"></a>
<a class="sourceLine" id="cb161-14" data-line-number="14"><span class="co"># Simulate multinomial distribution with varying probabilities per level</span></a>
<a class="sourceLine" id="cb161-15" data-line-number="15">multinomial_dist &lt;-<span class="st"> </span><span class="kw">rmultinom</span>(<span class="dv">1000</span>, <span class="dv">4</span>, <span class="dt">prob =</span> <span class="kw">c</span>(.<span class="dv">4</span>, <span class="fl">.3</span>, <span class="fl">.2</span>, <span class="fl">.6</span>))</a>
<a class="sourceLine" id="cb161-16" data-line-number="16"></a>
<a class="sourceLine" id="cb161-17" data-line-number="17"><span class="co"># Simulate poisson distribution</span></a>
<a class="sourceLine" id="cb161-18" data-line-number="18">poisson_dist &lt;-<span class="st"> </span><span class="kw">rpois</span>(<span class="dv">1000</span>, <span class="dv">10</span>) </a>
<a class="sourceLine" id="cb161-19" data-line-number="19"></a>
<a class="sourceLine" id="cb161-20" data-line-number="20"><span class="co"># Simulate geometric distribution</span></a>
<a class="sourceLine" id="cb161-21" data-line-number="21">geometric_dist &lt;-<span class="st"> </span><span class="kw">rgeom</span>(<span class="dv">1000</span>, <span class="dt">prob =</span> <span class="fl">.2</span>) </a></code></pre></div>
<p><br /></p>
<p>Next, we will visualize each distribution:</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" data-line-number="1"><span class="co"># Create user-defined function (UDF) to simplify probability distribution visualization</span></a>
<a class="sourceLine" id="cb162-2" data-line-number="2"><span class="co"># Function arguments: (1) data = object containing random distribution values; (2) type = &#39;discrete&#39; or &#39;continuous&#39; probability distribution; and (3) title = name of distribution</span></a>
<a class="sourceLine" id="cb162-3" data-line-number="3">dist.viz &lt;-<span class="st"> </span><span class="cf">function</span>(data, type, title) {</a>
<a class="sourceLine" id="cb162-4" data-line-number="4">  </a>
<a class="sourceLine" id="cb162-5" data-line-number="5">  <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;discrete&quot;</span>){</a>
<a class="sourceLine" id="cb162-6" data-line-number="6">    </a>
<a class="sourceLine" id="cb162-7" data-line-number="7">    <span class="co"># Discrete distribution</span></a>
<a class="sourceLine" id="cb162-8" data-line-number="8">    viz &lt;-<span class="st"> </span>ggplot2<span class="op">::</span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-9" data-line-number="9"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">aes</span>(data) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-10" data-line-number="10"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste</span>(title), <span class="dt">x =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;count&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-11" data-line-number="11"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb162-12" data-line-number="12"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb162-13" data-line-number="13"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb162-14" data-line-number="14">    </a>
<a class="sourceLine" id="cb162-15" data-line-number="15">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb162-16" data-line-number="16">    </a>
<a class="sourceLine" id="cb162-17" data-line-number="17">    <span class="co"># Continuous distribution</span></a>
<a class="sourceLine" id="cb162-18" data-line-number="18">    viz &lt;-<span class="st"> </span>ggplot2<span class="op">::</span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-19" data-line-number="19"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">aes</span>(data) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-20" data-line-number="20"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste</span>(title), <span class="dt">x =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;density&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb162-21" data-line-number="21"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb162-22" data-line-number="22"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">geom_density</span>(<span class="dt">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb162-23" data-line-number="23"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb162-24" data-line-number="24"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb162-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb162-26" data-line-number="26"></a>
<a class="sourceLine" id="cb162-27" data-line-number="27">  <span class="kw">return</span>(viz)</a>
<a class="sourceLine" id="cb162-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb162-29" data-line-number="29"></a>
<a class="sourceLine" id="cb162-30" data-line-number="30"><span class="co"># Call UDF to build visualizations and store to objects</span></a>
<a class="sourceLine" id="cb162-31" data-line-number="31">p_bernoulli &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> bernoulli_dist, <span class="dt">type =</span> <span class="st">&quot;discrete&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Bernoulli&quot;</span>)</a>
<a class="sourceLine" id="cb162-32" data-line-number="32">p_binomial &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> binomial_dist, <span class="dt">type =</span> <span class="st">&quot;discrete&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Binomial&quot;</span>)</a>
<a class="sourceLine" id="cb162-33" data-line-number="33">p_nbinomial &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> nbinomial_dist, <span class="dt">type =</span> <span class="st">&quot;discrete&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Negative Binomial&quot;</span>)</a>
<a class="sourceLine" id="cb162-34" data-line-number="34">p_multinomial &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> multinomial_dist, <span class="dt">type =</span> <span class="st">&quot;discrete&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Multinomial&quot;</span>)</a>
<a class="sourceLine" id="cb162-35" data-line-number="35">p_poisson &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> poisson_dist, <span class="dt">type =</span> <span class="st">&quot;discrete&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Poisson&quot;</span>)</a>
<a class="sourceLine" id="cb162-36" data-line-number="36">p_geometric &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> geometric_dist, <span class="dt">type =</span> <span class="st">&quot;discrete&quot;</span>, <span class="dt">title =</span> <span class="st">&quot;Geometric&quot;</span>)</a>
<a class="sourceLine" id="cb162-37" data-line-number="37">  </a>
<a class="sourceLine" id="cb162-38" data-line-number="38"><span class="co"># Display distribution visualizations</span></a>
<a class="sourceLine" id="cb162-39" data-line-number="39">ggpubr<span class="op">::</span><span class="kw">ggarrange</span>(p_bernoulli, p_binomial, p_nbinomial, p_multinomial, p_poisson, p_geometric,</a>
<a class="sourceLine" id="cb162-40" data-line-number="40">          <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">nrow =</span> <span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:discrete-dist"></span>
<img src="The_People_Analytics_Companion_files/figure-html/discrete-dist-1.png" alt="Discrete Probability Distributions" width="672" />
<p class="caption">
Figure 6.2: Discrete Probability Distributions
</p>
</div>
<p><br /></p>
<p><strong>Continuous Probability Distributions</strong></p>
<p><strong>Continuous probability distributions</strong>, also known as <strong>Probability Density Functions (PDF)</strong>, can be leveraged to model different types of interval and ratio variables. Some common continuous distributions include:</p>
<ul>
<li><strong>Normal (Gaussian)</strong>: distribution characterized by a mean and standard deviation for which the mean, median, and mode are equal</li>
<li><strong>Uniform</strong>: values of a random variable with equal probabilities of occurring</li>
<li><strong>Log-Normal</strong>: normal distribution of log-transformed values</li>
<li><strong>Student’s T</strong>: similar to the normal distribution but with thicker tails (approaches normal as <span class="math inline">\(n\)</span> increases)</li>
<li><strong>Chi-Square</strong>: similar to the t distribution in that the shape approaches normal as <span class="math inline">\(n\)</span> increases</li>
<li><strong>F</strong>: developed to examine variances from random samples taken from two independent normal populations</li>
</ul>
<p>A number of functions are available in R to simulate PDFs. While the precise shape of a distribution depends on the parameters, we will simulate and visualize these common PDFs to illustrate differences in the general shape of each. Let’s first draw 1,000 random values from each distribution with a specified set of parameters:</p>
<div class="sourceCode" id="cb163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb163-1" data-line-number="1"><span class="co"># Set seed for reproducible random distribution</span></a>
<a class="sourceLine" id="cb163-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb163-3" data-line-number="3"></a>
<a class="sourceLine" id="cb163-4" data-line-number="4"><span class="co"># Simulate normal distribution</span></a>
<a class="sourceLine" id="cb163-5" data-line-number="5">normal_dist &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dt">mean =</span> <span class="dv">50</span>, <span class="dt">sd =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb163-6" data-line-number="6"></a>
<a class="sourceLine" id="cb163-7" data-line-number="7"><span class="co"># Simulate log-normal distribution</span></a>
<a class="sourceLine" id="cb163-8" data-line-number="8">lnormal_dist &lt;-<span class="st"> </span><span class="kw">rlnorm</span>(<span class="dv">1000</span>, <span class="dt">meanlog =</span> <span class="dv">0</span>, <span class="dt">sdlog =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb163-9" data-line-number="9"></a>
<a class="sourceLine" id="cb163-10" data-line-number="10"><span class="co"># Simulate uniform distribution</span></a>
<a class="sourceLine" id="cb163-11" data-line-number="11">uniform_dist &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>, <span class="dt">min =</span> <span class="dv">1</span>, <span class="dt">max =</span> <span class="dv">100</span>) </a>
<a class="sourceLine" id="cb163-12" data-line-number="12"></a>
<a class="sourceLine" id="cb163-13" data-line-number="13"><span class="co"># Simulate student&#39;s t distribution</span></a>
<a class="sourceLine" id="cb163-14" data-line-number="14">t_dist &lt;-<span class="st"> </span><span class="kw">rt</span>(<span class="dv">1000</span>, <span class="dt">df =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb163-15" data-line-number="15"></a>
<a class="sourceLine" id="cb163-16" data-line-number="16"><span class="co"># Simulate chi-square distribution</span></a>
<a class="sourceLine" id="cb163-17" data-line-number="17">chisq_dist &lt;-<span class="st"> </span><span class="kw">rchisq</span>(<span class="dv">1000</span>, <span class="dt">df =</span> <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb163-18" data-line-number="18"></a>
<a class="sourceLine" id="cb163-19" data-line-number="19"><span class="co"># Simulate F distribution</span></a>
<a class="sourceLine" id="cb163-20" data-line-number="20">f_dist &lt;-<span class="st"> </span><span class="kw">rf</span>(<span class="dv">1000</span>, <span class="dt">df1 =</span> <span class="dv">5</span>, <span class="dt">df2 =</span> <span class="dv">200</span>)</a></code></pre></div>
<p><br /></p>
<p>Next, we will visualize each distribution. Since these continuous distributions are probability <em>density</em> functions, we will superimpose density plots over each:</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1"><span class="co"># Call UDF to build visualizations and store to objects</span></a>
<a class="sourceLine" id="cb164-2" data-line-number="2"><span class="co"># Note that as long as the arguments are in the order specified in the function (see our UDF definition above), the argument names do not need to be specified. To illustrate, we will drop the argument names from these function calls:</span></a>
<a class="sourceLine" id="cb164-3" data-line-number="3">p_normal &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(normal_dist, <span class="st">&quot;continuous&quot;</span>, <span class="st">&quot;Normal&quot;</span>)</a>
<a class="sourceLine" id="cb164-4" data-line-number="4">p_lnormal &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(lnormal_dist, <span class="st">&quot;continuous&quot;</span>, <span class="st">&quot;Log-Normal&quot;</span>)</a>
<a class="sourceLine" id="cb164-5" data-line-number="5">p_uniform &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(uniform_dist, <span class="st">&quot;continuous&quot;</span>, <span class="st">&quot;Uniform&quot;</span>)</a>
<a class="sourceLine" id="cb164-6" data-line-number="6">p_t &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(t_dist, <span class="st">&quot;continuous&quot;</span>, <span class="st">&quot;Student&#39;s T&quot;</span>)</a>
<a class="sourceLine" id="cb164-7" data-line-number="7">p_chisq &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(chisq_dist, <span class="st">&quot;continuous&quot;</span>, <span class="st">&quot;Chi-Square&quot;</span>)</a>
<a class="sourceLine" id="cb164-8" data-line-number="8">p_f &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(f_dist, <span class="st">&quot;continuous&quot;</span>, <span class="st">&quot;F&quot;</span>)</a>
<a class="sourceLine" id="cb164-9" data-line-number="9"></a>
<a class="sourceLine" id="cb164-10" data-line-number="10"><span class="co"># Display distribution visualizations</span></a>
<a class="sourceLine" id="cb164-11" data-line-number="11">ggpubr<span class="op">::</span><span class="kw">ggarrange</span>(p_normal, p_lnormal, p_uniform, p_t, p_chisq, p_f,</a>
<a class="sourceLine" id="cb164-12" data-line-number="12">          <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">nrow =</span> <span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:continous-dist"></span>
<img src="The_People_Analytics_Companion_files/figure-html/continous-dist-1.png" alt="Continuous Probability Distributions" width="672" />
<p class="caption">
Figure 6.3: Continuous Probability Distributions
</p>
</div>
<p><br /></p>
<p>The distribution of data is critically important in statistics. The accuracy of many statistical tests is based on assumptions rooted in underlying data distributions, and violating these assumptions can result in serious errors due to misaligned probability distributions. Though there are many more discrete and continuous probability distributions, we will leverage several of these common types to assess the likelihood of differences, effects, and associations in later chapters of this book.</p>
</div>
<div id="conditional-probability" class="section level3">
<h3><span class="header-section-number">6.1.2</span> Conditional Probability</h3>
<p><strong>Conditional probability</strong> reflects the probability conditioned on the occurrence of a previous event or outcome. For example, we may find that the proportion of heads is greater or less than <span class="math inline">\(\frac{1}{2}\)</span> with a large number of fair coin flips when the coin is consistently heads up when flipped. The outcome is, therefore, conditioned on the fixed – rather than random – positioning of the coin when flipped.</p>
<p>Formally, <strong>Bayes’ Theorem (alternatively, Bayes’ Rule)</strong> states that for any two events A and B wherein the probability of A is not 0 (<span class="math inline">\(P(A) \neq 0\)</span>):</p>
<p><span class="math display">\[ P(A \vert B) = \frac{P(B \vert A) P(A)}{P(B)}, \]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(A\)</span> = an event</li>
<li><span class="math inline">\(B\)</span> = another event</li>
<li><span class="math inline">\(P(A|B)\)</span> = conditional probability that event A occurs, given event B occurs (posterior)</li>
<li><span class="math inline">\(P(B|A)\)</span> = conditional probability that event B occurs, given event A occurs (likelihood)</li>
<li><span class="math inline">\(P(B)\)</span> = normalizing constant, constraining the probability distribution to sum to 1 (evidence)</li>
<li><span class="math inline">\(P(A)\)</span> = probability event A occurs before knowing if event B occurs (prior)</li>
</ul>
<p>Bayes’ Rule allows us to predict the outcome more accurately by conditioning the probability on known factors rather than assuming all events operate under the same conditions. Bayes’ Rule is pervasive in people analytics, as the probability of outcomes can vary widely when conditioned on a person’s age, tenure, education, job, perceptions, relationships, and many other factors. For example, if we consider a company with 100 terminations over a 12-month period and average headcount of 1,000, the probability of attrition not conditioned on any other factor is 10%, or <span class="math inline">\(\frac{1}{10}\)</span>. Aside from trending this probability over time to identify if overall attrition is becoming more or less of a concern, this isn’t too helpful at the company level. However, if we condition the probability of attrition on an event – such as a recent manager exit – and find that the probability of attrition among those whose manager has left in the last six months is 70%, or <span class="math inline">\(\frac{7}{10}\)</span>, this is far more actionable (and concerning).</p>
<p>The <strong>Monty Hall Problem</strong> is an excellent example of how our intuition is often at odds with the laws of conditional probability.</p>
<p>In the classic game show, Let’s Make a Deal, Monty Hall asks contestants to choose one of three closed doors. Behind one door is a prize while the other two doors contain nothing. After the contestant selects a door, Monty opens one of the other two doors which does not contain a prize. At this point, there are two closed doors: the door the contestant selected and another for which the content remains unknown. All that is known at this point is that the prize is behind one of the two closed doors.</p>
<p>It is at this juncture that Monty introduces a twist by asking if the contestant would like to switch doors. Most assume that the two closed doors have an equal (50/50) chance of containing the prize, because we generally think of probabilities as independent, random events. However, this is incorrect. Contestants who switch from their original selection have a 66% chance (rather than 50%) of winning. This may be counterintuitive, because the brain wants to reduce the problem to a simple coin flip. There is a major difference between the Monty Hall problem and a coin flip; for two outcomes to have the same probability, randomness and independence are required. In the case of the Monty Hall problem, neither assumption is satisfied.</p>
<p>When all three doors are closed, each has the same probability of being selected. The probability of choosing the door with a prize is .33. Monty’s knowledge of the door containing the prize does not impact the probability of selecting the winning door. This is because the choice is completely random given we have no information that would increase the probability of a door containing the prize. The process is no longer random when Monty uses his insider knowledge about the prize’s location and opens a door he knows does not contain the prize. The probabilities change. Since Monty will never show the door containing the prize, he is careful to always open a door that has nothing behind it. If he was not constrained by the requirement to not reveal the prize’s location and instead chose to open one of the remaining doors at random, the probabilities would be equal (and he may end up opening the door that contains the prize).</p>
<p>Seeing is believing, so let’s prove this with a simulation in R:</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb165-1" data-line-number="1"><span class="co"># Set seed for reproducible simulations</span></a>
<a class="sourceLine" id="cb165-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">12345</span>)</a>
<a class="sourceLine" id="cb165-3" data-line-number="3"></a>
<a class="sourceLine" id="cb165-4" data-line-number="4"><span class="co"># Set number of simulations</span></a>
<a class="sourceLine" id="cb165-5" data-line-number="5">trials =<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb165-6" data-line-number="6"></a>
<a class="sourceLine" id="cb165-7" data-line-number="7"><span class="co"># Store switch/keep decisions</span></a>
<a class="sourceLine" id="cb165-8" data-line-number="8">decisions =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;switch&quot;</span>, <span class="st">&quot;keep&quot;</span>)</a>
<a class="sourceLine" id="cb165-9" data-line-number="9"></a>
<a class="sourceLine" id="cb165-10" data-line-number="10"><span class="co"># Store integer for each door</span></a>
<a class="sourceLine" id="cb165-11" data-line-number="11">doors =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">3</span></a>
<a class="sourceLine" id="cb165-12" data-line-number="12"></a>
<a class="sourceLine" id="cb165-13" data-line-number="13"><span class="co"># Initialize empty data frame for results</span></a>
<a class="sourceLine" id="cb165-14" data-line-number="14">results =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb165-15" data-line-number="15"></a>
<a class="sourceLine" id="cb165-16" data-line-number="16"><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>trials){</a>
<a class="sourceLine" id="cb165-17" data-line-number="17">  </a>
<a class="sourceLine" id="cb165-18" data-line-number="18">  <span class="cf">for</span> (decision <span class="cf">in</span> decisions){</a>
<a class="sourceLine" id="cb165-19" data-line-number="19">    </a>
<a class="sourceLine" id="cb165-20" data-line-number="20">    <span class="co"># Select correct door</span></a>
<a class="sourceLine" id="cb165-21" data-line-number="21">    correct_door &lt;-<span class="st"> </span><span class="kw">sample</span>(doors, <span class="dv">1</span>, <span class="dt">replace =</span> T)</a>
<a class="sourceLine" id="cb165-22" data-line-number="22"></a>
<a class="sourceLine" id="cb165-23" data-line-number="23">    <span class="co"># Contestant chooses a door at random</span></a>
<a class="sourceLine" id="cb165-24" data-line-number="24">    selected_door &lt;-<span class="st"> </span><span class="kw">sample</span>(doors, <span class="dv">1</span>, <span class="dt">replace =</span> T)</a>
<a class="sourceLine" id="cb165-25" data-line-number="25">  </a>
<a class="sourceLine" id="cb165-26" data-line-number="26">    <span class="co"># Open door that was neither selected by the contestant nor contains the prize</span></a>
<a class="sourceLine" id="cb165-27" data-line-number="27">    <span class="co"># Choose one door to open if multiple remain without the prize (i.e., the contestant didn&#39;t initially select the door containing the prize)</span></a>
<a class="sourceLine" id="cb165-28" data-line-number="28">    remaining_doors &lt;-<span class="st"> </span><span class="kw">which</span>(<span class="op">!</span>doors <span class="op">==</span><span class="st"> </span>correct_door <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span>doors <span class="op">==</span><span class="st"> </span>selected_door)</a>
<a class="sourceLine" id="cb165-29" data-line-number="29">    open_door &lt;-<span class="st"> </span><span class="kw">sample</span>(remaining_doors, <span class="dv">1</span>, <span class="dt">replace =</span> T) </a>
<a class="sourceLine" id="cb165-30" data-line-number="30">  </a>
<a class="sourceLine" id="cb165-31" data-line-number="31">    <span class="co"># Contestant makes decision to switch doors or keep with the originally selected door</span></a>
<a class="sourceLine" id="cb165-32" data-line-number="32">    selected_door &lt;-<span class="st"> </span><span class="kw">ifelse</span>(decision <span class="op">==</span><span class="st"> &quot;switch&quot;</span>, <span class="kw">which</span>(<span class="op">!</span>doors <span class="op">==</span><span class="st"> </span>selected_door <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span>doors <span class="op">==</span><span class="st"> </span>open_door), selected_door)</a>
<a class="sourceLine" id="cb165-33" data-line-number="33">    </a>
<a class="sourceLine" id="cb165-34" data-line-number="34">    <span class="co"># Store results in data frame</span></a>
<a class="sourceLine" id="cb165-35" data-line-number="35">    results &lt;-<span class="st"> </span><span class="kw">rbind</span>(results, <span class="kw">cbind.data.frame</span>(</a>
<a class="sourceLine" id="cb165-36" data-line-number="36">                     <span class="dt">trial =</span> n,</a>
<a class="sourceLine" id="cb165-37" data-line-number="37">                     <span class="dt">decision =</span> decision,</a>
<a class="sourceLine" id="cb165-38" data-line-number="38">                     <span class="dt">result =</span> <span class="kw">ifelse</span>(correct_door <span class="op">==</span><span class="st"> </span>selected_door, <span class="st">&quot;win&quot;</span>, <span class="st">&quot;lose&quot;</span>)))</a>
<a class="sourceLine" id="cb165-39" data-line-number="39">  }</a>
<a class="sourceLine" id="cb165-40" data-line-number="40">}</a>
<a class="sourceLine" id="cb165-41" data-line-number="41"></a>
<a class="sourceLine" id="cb165-42" data-line-number="42"><span class="co"># Calculate percentage difference in wins for switch vs. keep decisions</span></a>
<a class="sourceLine" id="cb165-43" data-line-number="43">switch_wins &lt;-<span class="st"> </span><span class="kw">nrow</span>(results[results<span class="op">$</span>decision <span class="op">==</span><span class="st"> &quot;switch&quot;</span> <span class="op">&amp;</span><span class="st"> </span>results<span class="op">$</span>result <span class="op">==</span><span class="st"> &quot;win&quot;</span>, ]) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(results) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb165-44" data-line-number="44">keep_wins &lt;-<span class="st"> </span><span class="kw">nrow</span>(results[results<span class="op">$</span>decision <span class="op">==</span><span class="st"> &quot;keep&quot;</span> <span class="op">&amp;</span><span class="st"> </span>results<span class="op">$</span>result <span class="op">==</span><span class="st"> &quot;win&quot;</span>, ]) <span class="op">/</span><span class="st"> </span><span class="kw">nrow</span>(results) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb165-45" data-line-number="45"><span class="kw">round</span>((switch_wins <span class="op">-</span><span class="st"> </span>keep_wins) <span class="op">/</span><span class="st"> </span>keep_wins <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] 45</code></pre>
<p><br /></p>
<p>As we can see, wins occur nearly 50% more often when contestants switch doors. This exercise hopefully demonstrates the importance of conditional probability and statistical assumptions like randomness. Also, if ever you find yourself playing Let’s Make a Deal, switch doors.</p>
</div>
</div>
<div id="central-limit-theorem" class="section level2">
<h2><span class="header-section-number">6.2</span> Central Limit Theorem</h2>
<p>The <strong>Central Limit Theorem (CLT)</strong> is a mainstay of statistics and probability and fundamental to understanding the mechanics of statistical inference. The CLT was initially coined by a French-born mathematician named Abraham De Moivre in the 1700s. While initially unpopular, it was later reintroduced and attracted new interest from theorists and academics (Daw &amp; Pearson, 1972).</p>
<p>The CLT states that the average of independent random variables, when increased in number, tend to follow a normal (or Gaussian) distribution. The distribution of sample means approaches a normal distribution regardless of the shape of the population distribution from which the samples are drawn. This is important because the normal distribution has properties that can be used to test the likelihood that an observed value, difference, or relationship in a sample is also present in the population.</p>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:emp-rule"></span>
<img src="graphics/normal_distribution.png" alt="The Empirical Rule" width="75%" />
<p class="caption">
Figure 6.4: The Empirical Rule
</p>
</div>
<p><br /></p>
<p>Let’s begin with an intuitive example of CLT. Imagine that we have a reliable way to measure how fun a population is on a 100-point scale, where 100 indicates maximum fun (life of the party) and 1 indicates maximum boringness. Consider that a small statistics conference is in progress at a nearby convention center, and there are 40 statisticians in attendance. In a separate room at the same convention center, there is also a group of 40 random people (non-statisticians) who are gathered to discuss some less interesting topic. Our job is to walk into one of the rooms and determine – based on the “fun” factor alone – whether we have entered the statistics conference or the other, less interesting gathering of non-statisticians.</p>
<p>Instinctively, we already know the statisticians will be more fun than the other group. However, let’s assume we need the mean fun score and standard deviation of these two groups for this example. The group of statisticians have, on average, a fun score of 85 with a standard deviation of 2, while the group of non-statisticians are a bit less fun with a mean score of 65 and a standard deviation of 4. With a known population mean and standard deviation, the standard error (the standard deviation of the sample means) provides the ability to calculate the probability that the sample (the room of 40 people) belongs to the population of interest (fellow statisticians).</p>
<p>Herein lies the beauty of the CLT: roughly 68 percent of sample means will lie within one standard error of the population mean, roughly 95 percent within two standard errors of the population mean, and roughly 99 percent within three standard errors of the population mean. Therefore, any room whose members have an average fun score that is not within two standard errors of the population mean (between 81 and 89 for our statisticians) is statistically unlikely to be the group of statisticians for which we are searching. This is because in less than 5 in 100 cases could we randomly draw a ‘reasonably sized’ sample of statisticians with average funness so extremely different from the population average.</p>
<p>Because small samples lend to anomalies, we could – by chance – select a single person who happens to fall in the tails (extremely boring or extremely fun); however, as the sample size increases, it becomes more and more likely that the observed average reflects the average of the larger population. It would be virtually impossible (in less than 1 in 100 cases) to draw a random sample of statisticians from the population with average funness that is not within three standard errors of the population mean (between 79 and 91). Therefore, if we find that the room of people have an average fun score of 75, we will likely have far more fun in the other room!</p>
<p>Let’s now see the CLT in action by simulating a random uniform population distribution from which we can draw random samples. Remember, the shape of the population distribution does not matter; we could simulate an Exponential, Gamma, Poisson, Binomial, or other distribution and observe the same behavior.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb167-1" data-line-number="1"><span class="co"># Set seed for reproducible random distribution</span></a>
<a class="sourceLine" id="cb167-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb167-3" data-line-number="3"></a>
<a class="sourceLine" id="cb167-4" data-line-number="4"><span class="co"># Generate uniform population distribution with 1000 values ranging from 1 to 100</span></a>
<a class="sourceLine" id="cb167-5" data-line-number="5">rand.unif &lt;-<span class="st"> </span><span class="kw">runif</span>(<span class="dv">1000</span>, <span class="dt">min =</span> <span class="dv">1</span>, <span class="dt">max =</span> <span class="dv">100</span>)</a></code></pre></div>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb168-1" data-line-number="1"><span class="co"># Calculate population mean</span></a>
<a class="sourceLine" id="cb168-2" data-line-number="2"><span class="kw">mean</span>(rand.unif)</a></code></pre></div>
<pre><code>## [1] 51.22007</code></pre>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1"><span class="co"># Calculate population variance</span></a>
<a class="sourceLine" id="cb170-2" data-line-number="2">N =<span class="st"> </span><span class="kw">length</span>(rand.unif)</a>
<a class="sourceLine" id="cb170-3" data-line-number="3"><span class="kw">var</span>(rand.unif) <span class="op">*</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span>N</a></code></pre></div>
<pre><code>## [1] 830.3155</code></pre>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb172-1" data-line-number="1"><span class="co"># Produce histogram to visualize population distribution</span></a>
<a class="sourceLine" id="cb172-2" data-line-number="2">ggplot2<span class="op">::</span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb172-3" data-line-number="3">ggplot2<span class="op">::</span><span class="kw">aes</span>(rand.unif) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb172-4" data-line-number="4">ggplot2<span class="op">::</span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb172-5" data-line-number="5">ggplot2<span class="op">::</span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb172-6" data-line-number="6">ggplot2<span class="op">::</span><span class="kw">geom_density</span>(<span class="dt">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb172-7" data-line-number="7">ggplot2<span class="op">::</span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-89"></span>
<img src="The_People_Analytics_Companion_files/figure-html/unnamed-chunk-89-1.png" alt="Uniform Population Distribution (N = 1000)" width="672" />
<p class="caption">
Figure 6.5: Uniform Population Distribution (N = 1000)
</p>
</div>
<p><br /></p>
<p>As expected, these randomly generated data are uniformly distributed. Next, we will draw 100 random samples of various sizes and plot the average of each.</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" data-line-number="1"><span class="co"># Define number of samples to draw from population distribution</span></a>
<a class="sourceLine" id="cb173-2" data-line-number="2">samples &lt;-<span class="st"> </span><span class="dv">10000</span></a>
<a class="sourceLine" id="cb173-3" data-line-number="3"></a>
<a class="sourceLine" id="cb173-4" data-line-number="4"><span class="co"># Populate vector with sample sizes</span></a>
<a class="sourceLine" id="cb173-5" data-line-number="5">sample_n &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">25</span>,<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb173-6" data-line-number="6"></a>
<a class="sourceLine" id="cb173-7" data-line-number="7"><span class="co"># Initialize empty data frame to hold sample means</span></a>
<a class="sourceLine" id="cb173-8" data-line-number="8">sample_means =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb173-9" data-line-number="9"></a>
<a class="sourceLine" id="cb173-10" data-line-number="10"><span class="co"># Set seed for reproducible random samples</span></a>
<a class="sourceLine" id="cb173-11" data-line-number="11"><span class="kw">set.seed</span>(<span class="dv">456</span>)</a>
<a class="sourceLine" id="cb173-12" data-line-number="12"></a>
<a class="sourceLine" id="cb173-13" data-line-number="13"><span class="co"># For each n, draw random samples</span></a>
<a class="sourceLine" id="cb173-14" data-line-number="14"><span class="cf">for</span> (n <span class="cf">in</span> sample_n) {</a>
<a class="sourceLine" id="cb173-15" data-line-number="15">  </a>
<a class="sourceLine" id="cb173-16" data-line-number="16">  <span class="cf">for</span> (draw <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>samples) {</a>
<a class="sourceLine" id="cb173-17" data-line-number="17">    </a>
<a class="sourceLine" id="cb173-18" data-line-number="18">      <span class="co"># Store sample means in data frame</span></a>
<a class="sourceLine" id="cb173-19" data-line-number="19">      sample_means &lt;-<span class="st"> </span><span class="kw">rbind</span>(sample_means, <span class="kw">cbind.data.frame</span>(</a>
<a class="sourceLine" id="cb173-20" data-line-number="20">                            <span class="dt">n =</span> n, </a>
<a class="sourceLine" id="cb173-21" data-line-number="21">                            <span class="dt">x_bar =</span> <span class="kw">mean</span>(<span class="kw">sample</span>(rand.unif, n, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="ot">NULL</span>))))</a>
<a class="sourceLine" id="cb173-22" data-line-number="22">  }</a>
<a class="sourceLine" id="cb173-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb173-24" data-line-number="24"></a>
<a class="sourceLine" id="cb173-25" data-line-number="25"><span class="co"># Produce histograms to visualize distributions of sample means, grouped by n-count</span></a>
<a class="sourceLine" id="cb173-26" data-line-number="26">sample_means <span class="op">%&gt;%</span><span class="st"> </span>ggplot2<span class="op">::</span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb173-27" data-line-number="27"><span class="st">                 </span>ggplot2<span class="op">::</span><span class="kw">aes</span>(<span class="dt">x =</span> x_bar, <span class="dt">fill =</span> n) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb173-28" data-line-number="28"><span class="st">                 </span>ggplot2<span class="op">::</span><span class="kw">labs</span>(<span class="dt">x =</span> <span class="st">&quot;x-bar&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb173-29" data-line-number="29"><span class="st">                 </span>ggplot2<span class="op">::</span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb173-30" data-line-number="30"><span class="st">                 </span>ggplot2<span class="op">::</span><span class="kw">geom_density</span>(<span class="dt">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="dt">alpha =</span> <span class="fl">0.6</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb173-31" data-line-number="31"><span class="st">                 </span>ggplot2<span class="op">::</span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb173-32" data-line-number="32"><span class="st">                 </span>ggplot2<span class="op">::</span><span class="kw">facet_wrap</span>(<span class="op">~</span>n)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-90"></span>
<img src="The_People_Analytics_Companion_files/figure-html/unnamed-chunk-90-1.png" alt="Distribution of 10,000 Sample Means of Varied Size" width="672" />
<p class="caption">
Figure 6.6: Distribution of 10,000 Sample Means of Varied Size
</p>
</div>
<p><br /></p>
<p>Per the CLT, we can see that as n increases, the sample means become more normally distributed.</p>
</div>
<div id="confidence-intervals" class="section level2">
<h2><span class="header-section-number">6.3</span> Confidence Intervals</h2>
<p>A <strong>Confidence Interval (CI)</strong> is a range of values that likely contains the value of an unknown population parameter. These unknown population parameters are often <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\sigma\)</span>, though we will also leverage CIs in later chapters for regression coefficients, proportions, rates, and differences.</p>
<p>If we draw random samples from a population, we can compute a CI for each sample. Building on the CLT, for a given confidence level (usually 95%, though 99% or 90% are sometimes used), the specified percent of sample intervals is expected to include the estimated population parameter. For example, for a 95% CI we would expect 19 in every 20 (or 95 in every 100) intervals across the samples to include the true population parameter. This is illustrated in Figure <a href="inf-stats.html#fig:conf-int">6.7</a>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:conf-int"></span>
<img src="graphics/confidence_intervals.png" alt="Intervals for 95 Percent Confidence Level" width="75%" />
<p class="caption">
Figure 6.7: Intervals for 95 Percent Confidence Level
</p>
</div>
<p>It is important to note that CIs should not be applied to the distribution of sample values; CIs relate to <em>population parameters</em>. A common misinterpretation of a CI is that it represents an interval within which a certain percent of sample values exists, and this is inaccurate. Because this misinterpretation is so prevalent, there is a good chance you will be tested on your understanding of CIs when applying to positions involving statistical analyses!</p>
<p>A related concept that is fundamental to estimating CIs is the <strong>standard error (SE)</strong>, which is the standard deviation of sample means. While the standard deviation is a measure of variability for a random variable, the variability captured by the SE reflects how well a sample represents the population. Since sample statistics will approach the actual population parameters as the size of the sample increases, the SE and sample size are inversely related; that is, the SE decreases as the sample size increases. The SE is defined by:</p>
<p><span class="math display">\[ SE = \frac{\sigma}{\sqrt{n}} \]</span></p>
<p>Since the CLT is fundamental to inferential statistics, let’s validate that our simulated distribution of sample means adheres to the properties of normally distributed data per the Empirical Rule:</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb174-1" data-line-number="1"><span class="co"># Store sample means with n = 50</span></a>
<a class="sourceLine" id="cb174-2" data-line-number="2">x_bars &lt;-<span class="st"> </span>sample_means[sample_means<span class="op">$</span>n <span class="op">==</span><span class="st"> </span><span class="dv">50</span>, <span class="st">&quot;x_bar&quot;</span>]</a>
<a class="sourceLine" id="cb174-3" data-line-number="3"></a>
<a class="sourceLine" id="cb174-4" data-line-number="4"><span class="co"># Store sample size</span></a>
<a class="sourceLine" id="cb174-5" data-line-number="5">n &lt;-<span class="st"> </span><span class="kw">length</span>(x_bars)</a>
<a class="sourceLine" id="cb174-6" data-line-number="6"></a>
<a class="sourceLine" id="cb174-7" data-line-number="7"><span class="co"># Calculate percent of sample means within +/- 2 SEs</span></a>
<a class="sourceLine" id="cb174-8" data-line-number="8"><span class="kw">length</span>(<span class="kw">subset</span>(x_bars, x_bars <span class="op">&lt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars) <span class="op">&amp;</span><span class="st"> </span>x_bars <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars))) <span class="op">/</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a></code></pre></div>
<pre><code>## [1] 95.35</code></pre>
<p>95% of sample means are within 2 SEs, which is what we expect per the characteristics of the normal distribution.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb176-1" data-line-number="1"><span class="co"># Calculate percent of sample means within +/- 3 SEs</span></a>
<a class="sourceLine" id="cb176-2" data-line-number="2"><span class="kw">length</span>(<span class="kw">subset</span>(x_bars, x_bars <span class="op">&lt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">+</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars) <span class="op">&amp;</span><span class="st"> </span>x_bars <span class="op">&gt;</span><span class="st"> </span><span class="kw">mean</span>(x_bars) <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">sd</span>(x_bars))) <span class="op">/</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a></code></pre></div>
<pre><code>## [1] 99.79</code></pre>
<p>Nearly all of the sample means are within 3 SEs, indicating that it would be highly unlikely – nearly impossible even – to observe a sample mean ‘from the same population’ that falls outside this interval.</p>
<p>Now, let’s illustrate the relationship between CIs and standard errors using sample data from our uniform population distribution. In our example, both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are known and our sample size <span class="math inline">\(n\)</span> is at least 30; therefore, we can use a <strong>Z-Test</strong> to calculate the 95% CI. A <span class="math inline">\(z\)</span> score of 1.96 corresponds to the 95% CI for a two-tailed distribution; that is, we are looking for significantly different values in either the larger or smaller direction. The 95% CI represents the range of values we would expect to include <span class="math inline">\(\mu\)</span> in at least 95 of 100 random samples taken from the population.</p>
<p>The CI in this case is defined by:</p>
<p><span class="math display">\[ CI = \bar{x} \pm z_{\alpha/_2} \frac{\sigma}{\sqrt{n}} \]</span></p>
<p>Let’s randomly take <span class="math inline">\(n\)</span> = 100 from the population, and compute sample statistics to estimate the 95% CI:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1"><span class="co"># Set seed for reproducible random samples</span></a>
<a class="sourceLine" id="cb178-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">456</span>)</a>
<a class="sourceLine" id="cb178-3" data-line-number="3"></a>
<a class="sourceLine" id="cb178-4" data-line-number="4"><span class="co"># Sample 100 values from uniform population distribution</span></a>
<a class="sourceLine" id="cb178-5" data-line-number="5">x &lt;-<span class="st"> </span><span class="kw">sample</span>(rand.unif, <span class="dv">100</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb178-6" data-line-number="6"></a>
<a class="sourceLine" id="cb178-7" data-line-number="7"><span class="co"># Calculate 95% CI</span></a>
<a class="sourceLine" id="cb178-8" data-line-number="8">ci95_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a>
<a class="sourceLine" id="cb178-9" data-line-number="9">ci95_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">1.96</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a></code></pre></div>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" data-line-number="1"><span class="co"># Print lower bound for 95% CI</span></a>
<a class="sourceLine" id="cb179-2" data-line-number="2">ci95_lower_bound</a></code></pre></div>
<pre><code>## [1] 47.90733</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" data-line-number="1"><span class="co"># Print upper bound for 95% CI</span></a>
<a class="sourceLine" id="cb181-2" data-line-number="2">ci95_upper_bound</a></code></pre></div>
<pre><code>## [1] 58.98773</code></pre>
<p>Our known <span class="math inline">\(\mu\)</span> is 51.2, which is covered by our 95% CI (47.9 - 59.0). Per the CLT, in less than 5% of cases would we expect to draw a random sample from the population that results in a 95% CI which does not include <span class="math inline">\(\mu\)</span>. Note that our CI narrows with larger samples since our confidence that the range includes <span class="math inline">\(\mu\)</span> increases with more data.</p>
<p>Next, let’s look at a 99% CI. We will enter 2.576 for <span class="math inline">\(z\)</span>:</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1"><span class="co"># Calculate 99% CI</span></a>
<a class="sourceLine" id="cb183-2" data-line-number="2">ci99_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span><span class="fl">2.576</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a>
<a class="sourceLine" id="cb183-3" data-line-number="3">ci99_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">2.576</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">100</span>))</a></code></pre></div>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1"><span class="co"># Print lower bound for 99% CI</span></a>
<a class="sourceLine" id="cb184-2" data-line-number="2">ci99_lower_bound</a></code></pre></div>
<pre><code>## [1] 46.16612</code></pre>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" data-line-number="1"><span class="co"># Print upper bound for 99% CI</span></a>
<a class="sourceLine" id="cb186-2" data-line-number="2">ci99_upper_bound</a></code></pre></div>
<pre><code>## [1] 60.72893</code></pre>
<p>Like the 95% CI, this slightly wider 99% CI (46.2 - 60.7) also includes our <span class="math inline">\(\mu\)</span> of 51.2.</p>
<p>If <span class="math inline">\(\sigma\)</span> is not known, and/or we have a small sample (<span class="math inline">\(n\)</span> &lt; 30), we need to use a <span class="math inline">\(t\)</span><strong>-test</strong> to calculate the CIs. In a people analytics setting, the reality is that population parameters are often unknown. For example, if we knew how engagement scores vary in the employee population, there would be no need to survey a sample of employees and make inferences about said population.</p>
<p>As we will see, the <span class="math inline">\(t\)</span>-test underpins many statistical tests and models germane to the people analytics discipline since we are often working with small datasets, so it is important to understand the mechanics. As shown in Figure <a href="inf-stats.html#fig:t-distribution">6.8</a>, the <span class="math inline">\(t\)</span> distribution is increasingly wider and shorter relative to the normal distribution as the sample size decreases; this is also characteristic of the sampling distribution of means for smaller samples we observed in our CLT example. Specifically, <strong>degrees of freedom (df)</strong> is used to determine the shape of the probability distribution. Degrees of freedom represents the number of observations in the data that are free to vary when estimating statistical parameters, which is a function of the sample size (<span class="math inline">\(n - 1\)</span>). For example, if we could choose 1 of 5 projects to work on each day between Monday and Friday, we would only be able to <em>choose</em> 4 out of the 5 days; on Friday, only 1 project would remain to be selected, so our degrees of freedom (the number of days in which we have a choice between projects) would be 4.</p>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:t-distribution"></span>
<img src="graphics/t_distribution.png" alt="t Distribution Shape by Degrees of Freedom" width="75%" />
<p class="caption">
Figure 6.8: t Distribution Shape by Degrees of Freedom
</p>
</div>
<p><br /></p>
<p>When estimating the CI for smaller samples, we need to leverage the wider, more platykurtic <span class="math inline">\(t\)</span> distribution to achieve greater accuracy. Therefore, the CI for a two-tailed test in this case is defined by:</p>
<p><span class="math display">\[ CI = \bar{x} \pm t_{\alpha/_2} \frac{\sigma}{\sqrt{n}} \]</span></p>
<p>Let’s compare CIs calculated using a <span class="math inline">\(t\)</span>-test to those calculated using the Z-Test. While a fixed <span class="math inline">\(z\)</span> score can be used for each CI level when <span class="math inline">\(n\)</span> &gt; 30, the <span class="math inline">\(t\)</span> statistic varies based on both the CI level and <span class="math inline">\(df\)</span>. Though R will determine the correct <span class="math inline">\(t\)</span> statistic for us, let’s reference the table shown in Figure <a href="inf-stats.html#fig:t-crit">6.9</a> to manually lookup the <span class="math inline">\(t\)</span> statistic:</p>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:t-crit"></span>
<img src="graphics/t-table.png" alt="Critical Values of Student's t Distribution" width="50%" />
<p class="caption">
Figure 6.9: Critical Values of Student’s t Distribution
</p>
</div>
<p><br /></p>
<p>For illustrative purposes, let’s draw a smaller sample of <span class="math inline">\(n\)</span> = 25 from our uniform population distribution and calculate the 95% CI using the <span class="math inline">\(t\)</span> statistic from the table (<span class="math inline">\(df\)</span> = 24). The <span class="math inline">\(t\)</span> statistic for this CI and <span class="math inline">\(df\)</span> is 2.064:</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb188-1" data-line-number="1"><span class="co"># Set seed for reproducible random samples</span></a>
<a class="sourceLine" id="cb188-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">456</span>)</a>
<a class="sourceLine" id="cb188-3" data-line-number="3"></a>
<a class="sourceLine" id="cb188-4" data-line-number="4"><span class="co"># Sample 25 values from uniform population distribution</span></a>
<a class="sourceLine" id="cb188-5" data-line-number="5">x &lt;-<span class="st"> </span><span class="kw">sample</span>(rand.unif, <span class="dv">25</span>, <span class="dt">replace =</span> <span class="ot">TRUE</span>, <span class="dt">prob =</span> <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb188-6" data-line-number="6"></a>
<a class="sourceLine" id="cb188-7" data-line-number="7"><span class="co"># Calculate 95% CI</span></a>
<a class="sourceLine" id="cb188-8" data-line-number="8">ci95_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span><span class="fl">2.064</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>))</a>
<a class="sourceLine" id="cb188-9" data-line-number="9">ci95_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">2.064</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>))</a></code></pre></div>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1"><span class="co"># Print lower bound for 95% CI</span></a>
<a class="sourceLine" id="cb189-2" data-line-number="2">ci95_lower_bound</a></code></pre></div>
<pre><code>## [1] 35.24305</code></pre>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1"><span class="co"># Print upper bound for 95% CI</span></a>
<a class="sourceLine" id="cb191-2" data-line-number="2">ci95_upper_bound</a></code></pre></div>
<pre><code>## [1] 59.60959</code></pre>
<p>As expected, the 95% CI using the <span class="math inline">\(t\)</span> statistic is much wider (35.2 - 59.6), acknowledging the increased uncertainty in estimating population parameters given the limited information in this smaller sample. To increase our confidence to the 99% level, the interval widens even further (30.9 - 63.9):</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb193-1" data-line-number="1"><span class="co"># Calculate 99% CI</span></a>
<a class="sourceLine" id="cb193-2" data-line-number="2">ci99_lower_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">-</span><span class="st"> </span><span class="fl">2.797</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>))</a>
<a class="sourceLine" id="cb193-3" data-line-number="3">ci99_upper_bound &lt;-<span class="st"> </span><span class="kw">mean</span>(x) <span class="op">+</span><span class="st"> </span><span class="fl">2.797</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">sd</span>(x) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">25</span>))</a></code></pre></div>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb194-1" data-line-number="1"><span class="co"># Print lower bound for 99% CI</span></a>
<a class="sourceLine" id="cb194-2" data-line-number="2">ci99_lower_bound</a></code></pre></div>
<pre><code>## [1] 30.91633</code></pre>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb196-1" data-line-number="1"><span class="co"># Print upper bound for 99% CI</span></a>
<a class="sourceLine" id="cb196-2" data-line-number="2">ci99_upper_bound</a></code></pre></div>
<pre><code>## [1] 63.93631</code></pre>
<div id="hypothesis-testing" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Hypothesis Testing</h3>
<p><strong>Hypothesis testing</strong> is how we leverage CIs to test whether a significant difference or relationship exists in the data. Sir Ronald Fisher invented what is known as the null hypothesis, which states that there is no relationship/difference; disprove me if you can! The null hypothesis is defined by:</p>
<p><span class="math display">\[ H_0: \mu_A = \mu_B \]</span></p>
<p>The objective of hypothesis testing is to determine if there is sufficient evidence to reject the null hypothesis in favor of an alternative hypothesis. The null hypothesis always states that there is ‘nothing’ of significance. For example, if we want to test whether an intervention has an effect on an outcome in a population, the null hypothesis states that there is no effect. If we want to test whether there is a difference in average scores between two groups in a population, the null hypothesis states that there is no difference.</p>
<p>An alternative hypothesis may simply state that there is a difference or relationship in the population, or it may specify the expected direction (e.g., Population A has a significantly ‘larger’ or ‘smaller’ average value than Population B; Variable A is ‘positively’ or ‘negatively’ related to Variable B). Therefore, alternative hypotheses are defined by:</p>
<p><span class="math display">\[ H_A: \mu_A \neq \mu_B \]</span></p>
<p><span class="math display">\[ H_A: \mu_A &lt; \mu_B \]</span></p>
<p><span class="math display">\[ H_A: \mu_A &gt; \mu_B \]</span></p>
</div>
<div id="alpha" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Alpha</h3>
<p>The <strong>alpha</strong> level of a hypothesis test, denoted by <span class="math inline">\(\alpha\)</span>, represents the probability of obtaining observed results due to chance if the null hypothesis is true. In other words, <span class="math inline">\(\alpha\)</span> is the probability of rejecting the null hypothesis (and therefore claiming that there is a significant difference or relationship) when in fact we should have failed to reject it because there is insufficient evidence to support the alternative hypothesis.</p>
<p><span class="math inline">\(\alpha\)</span> is often set at .05 but is sometimes set at a more rigorous .01, depending upon the context and tolerance for error. An <span class="math inline">\(\alpha\)</span> of .05 corresponds to a 95% CI (1 - .05), and .01 to a 99% CI (1 - .01). With non-directional alternative hypotheses, we must divide <span class="math inline">\(\alpha\)</span> by 2 (i.e., we could observe a significant result in either tail of the distribution), while one-tailed tests position the rejection region entirely within one tail based on what is being hypothesized.</p>
<p>At the .05 level, we would conclude that a finding is statistically significant if the chance of observing a value at least as extreme as the one observed is less than 1 in 20 if the null hypothesis is true. Note that we observed this behavior with our simulated distribution of sample means. While we could observe more extreme values by chance with repeated attempts, in less than 1 in every 20 times would we expect a 95% CI that does not capture <span class="math inline">\(\mu\)</span>. Moreover, in less than 1 in every 100 times should we expect a sample with a 99% CI that does not capture <span class="math inline">\(\mu\)</span>.</p>
</div>
<div id="beta" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Beta</h3>
<p>Another key value is <strong>Beta</strong>, denoted by <span class="math inline">\(\beta\)</span>, which relates to the power of the analysis. Simply put, power reflects our ability to find a difference or relationship if there is one. Power is calculated by 1 - <span class="math inline">\(\beta\)</span>. At this point, it should be intuitive that larger samples increase our chances of observing significant results. As we observed in the <span class="math inline">\(t\)</span>-test example, CIs for small samples (<span class="math inline">\(n\)</span> &lt; 30) are quite wide relative to those for large samples; therefore, the power of the analysis to detect significance is limited given how extremely different values of <span class="math inline">\(x\)</span> must be to observe non-overlapping CIs.</p>
</div>
<div id="type-i-ii-errors" class="section level3">
<h3><span class="header-section-number">6.3.4</span> Type I &amp; II Errors</h3>
<p>A <strong>Type I Error</strong> is a false positive, wherein we conclude that there is a significant difference or relationship when there is not. A <strong>Type II Error</strong> is a false negative, wherein we fail to capture a significant finding. <span class="math inline">\(\alpha\)</span> represents our chance of making a Type I Error, while <span class="math inline">\(\beta\)</span> represents our chance of making a Type II Error. I once had a professor explain that committing a Type I error is a shame, while committing a Type II error is a pity, and I’ve found this to be a helpful way to remember what each type of error represents.</p>
<p><br /></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyp-errs"></span>
<img src="graphics/hypothesis_testing_errors.png" alt="Type I and II Errors" width="50%" />
<p class="caption">
Figure 6.10: Type I and II Errors
</p>
</div>
<p><br /></p>
</div>
<div id="p-values" class="section level3">
<h3><span class="header-section-number">6.3.5</span> <span class="math inline">\(p\)</span>-Values</h3>
<p>In statistical tests, the <strong>p-value</strong> is referenced to determine whether the null hypothesis can be rejected. The p-value represents the probability of obtaining a result at least as extreme as the one observed if the null hypothesis is true. As a general rule, if <span class="math inline">\(p\)</span> &lt; .05, we can confidently reject the null hypothesis and conclude that the observed difference or relationship was unlikely a chance observation.</p>
<p>While statistical significance helps us understand the probability of observing results by chance when there is no difference or effect in the population, it does not tell us anything about the size of the difference or effect. Analysis should never be reduced to inspecting p-values; in fact, p-values have been the subject of much controversy among researchers and practitioners in recent years. Later chapters will cover how to interpret results of statistical tests to surface the story and determine if there is anything ‘practically’ significant among statistically significant findings.</p>
</div>
<div id="bonferroni-correction" class="section level3">
<h3><span class="header-section-number">6.3.6</span> Bonferroni Correction</h3>
<p>One caveat when leveraging a p-value to determine statistical significance is that when multiple testing is performed – that is, multiple tests using the same sample data – the probability of a Type I error increases by a factor equivalent to the number of tests performed. It’s important to note that there is not agreement among statisticians about how (or even whether) the p-value threshold for statistical significance needs to be adjusted to account for this increased risk. Nevertheless, we will cover this conservative approach for mitigating this risk.</p>
<p>Thus far, we have only discussed statistical significance in the context of a <strong>per analysis error rate</strong> – that is, the probability of committing a Type I error for a single statistical test. However, when two or more tests are being conducted on the same sample, the <strong>familywise error rate</strong> is an important factor in determining statistical significance. The familywise error rate reflects the fact that as we conduct more and more analyses on the same sample, the probability of a Type I error across the set (or family) of analyses increases. The familywise error rate can be calculated by:</p>
<p><span class="math display">\[ \alpha_{FW} = 1 - (1 - \alpha_{PC})^C,  \]</span></p>
<p>where <span class="math inline">\(c\)</span> is equal to the number of comparisons (or statistical tests) performed, and <span class="math inline">\(\alpha_{PC}\)</span> is equal to the specified per analysis error rate (usually .05). For example, if <span class="math inline">\(\alpha\)</span> = .05 per analysis, the probability of a Type I error with three tests on the same data increases from 5% to 14.3%: <span class="math inline">\(1 - (1 - .05)^3 = .143\)</span>.</p>
<p>The most common method of adjusting the familywise error rate down to the specified per analysis error rate is the <strong>Bonferroni Correction</strong>. To implement this correction, we can simply divide <span class="math inline">\(\alpha\)</span> by the number of analyses performed on the dataset – such as <span class="math inline">\(\alpha / 3 = .017\)</span> in the case of three analyses with <span class="math inline">\(\alpha = .05\)</span>. This means that for each statistical test, we must achieve <span class="math inline">\(p &lt; .017\)</span> to report a statistically significant result. An alternative which allows us to achieve the same number of statistically significant results is to multiply the unadjusted per analysis p-values for each statistical test by the number of tests. For example, if we run three statistical tests and receive <span class="math inline">\(p = .014\)</span>, <span class="math inline">\(p = .047\)</span>, and <span class="math inline">\(p = .125\)</span>, we would achieve one significant result with the first method (<span class="math inline">\(p &lt; .017\)</span>) as well as with the alternative since the first statistical test satisfies the per analysis error rate (<span class="math inline">\(p &lt; .05\)</span>): <span class="math inline">\(p = .014 * 3 = .042\)</span>.</p>
<p>Perneger (1998) is one of many who oppose the use of the Bonferroni Correction, suggesting that these “adjustments are, at best, unnecessary and, at worst, deleterious to sound statistical inference.” The Bonferroni Correction is controversial among researchers because while applying the correction reduces the chance of a Type I error, it also increases the chance of a Type II error. Because this correction makes it more difficult to detect significant results, it is rare to find such a correction reported in published research, though research often involves multiple testing on the same sample. Perneger suggests that simply describing the statistical tests that were performed, and why, is sufficient for dealing with potential problems introduced by multiple testing.</p>
</div>
</div>
<div id="review-questions-4" class="section level2">
<h2><span class="header-section-number">6.4</span> Review Questions</h2>
<ol style="list-style-type: decimal">
<li><p>What are some examples of a null hypothesis?</p></li>
<li><p>What is the difference between Type I and Type II errors?</p></li>
<li><p>What is the primary purpose of inferential statistics, and how does it differ from descriptive statistics?</p></li>
<li><p>What is the Central Limit Theorem (CLT), and why is it important?</p></li>
<li><p>Is randomness a requirement for probabilistic methods? Why or why not?</p></li>
<li><p>What does the Bonferroni Correction seek to achieve?</p></li>
<li><p>What is a confidence interval (CI)?</p></li>
<li><p>What are some examples of how the context influences what level of confidence is appropriate for statistical significance testing? Should we always use a 95 CI?</p></li>
<li><p>When population parameters are unknown, which test would be appropriate for testing the following null hypothesis: <span class="math inline">\(\mu_A = \mu_B\)</span>?</p></li>
<li><p>According to the Empirical Rule, 95% of normally distributed data lie within how many standard deviations of the mean?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="desc-stats.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data-wrang-prep.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["The_People_Analytics_Companion.pdf", "The_People_Analytics_Companion.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
