<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Analysis of Differences | The People Analytics Companion: An End-to-End Guide through the People Analytics Lifecycle</title>
  <meta name="description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Analysis of Differences | The People Analytics Companion: An End-to-End Guide through the People Analytics Lifecycle" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="github-repo" content="crstarbuck/peopleanalytics-lifecycle-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Analysis of Differences | The People Analytics Companion: An End-to-End Guide through the People Analytics Lifecycle" />
  
  <meta name="twitter:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  

<meta name="author" content="Craig Starbuck" />


<meta name="date" content="2022-04-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-wrang-prep.html"/>
<link rel="next" href="lm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The People Analytics Companion: An End-to-End Guide through the People Analytics Lifecycle</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i><b>1</b> Foreword</a></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>3</b> Getting Started</a><ul>
<li class="chapter" data-level="3.1" data-path="getting-started.html"><a href="getting-started.html#guiding-principles"><i class="fa fa-check"></i><b>3.1</b> Guiding Principles</a><ul>
<li class="chapter" data-level="3.1.1" data-path="getting-started.html"><a href="getting-started.html#pro-employee-thinking"><i class="fa fa-check"></i><b>3.1.1</b> Pro Employee Thinking</a></li>
<li class="chapter" data-level="3.1.2" data-path="getting-started.html"><a href="getting-started.html#quality"><i class="fa fa-check"></i><b>3.1.2</b> Quality</a></li>
<li class="chapter" data-level="3.1.3" data-path="getting-started.html"><a href="getting-started.html#prioritization"><i class="fa fa-check"></i><b>3.1.3</b> Prioritization</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="getting-started.html"><a href="getting-started.html#tools"><i class="fa fa-check"></i><b>3.2</b> Tools</a><ul>
<li class="chapter" data-level="3.2.1" data-path="getting-started.html"><a href="getting-started.html#r"><i class="fa fa-check"></i><b>3.2.1</b> R</a></li>
<li class="chapter" data-level="3.2.2" data-path="getting-started.html"><a href="getting-started.html#data"><i class="fa fa-check"></i><b>3.2.2</b> Data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="getting-started.html"><a href="getting-started.html#d-framework"><i class="fa fa-check"></i><b>3.3</b> 4D Framework</a><ul>
<li class="chapter" data-level="3.3.1" data-path="getting-started.html"><a href="getting-started.html#discover"><i class="fa fa-check"></i><b>3.3.1</b> Discover</a></li>
<li class="chapter" data-level="3.3.2" data-path="getting-started.html"><a href="getting-started.html#design"><i class="fa fa-check"></i><b>3.3.2</b> Design</a></li>
<li class="chapter" data-level="3.3.3" data-path="getting-started.html"><a href="getting-started.html#develop"><i class="fa fa-check"></i><b>3.3.3</b> Develop</a></li>
<li class="chapter" data-level="3.3.4" data-path="getting-started.html"><a href="getting-started.html#deliver"><i class="fa fa-check"></i><b>3.3.4</b> Deliver</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="r-intro.html"><a href="r-intro.html"><i class="fa fa-check"></i><b>4</b> Introduction to R</a><ul>
<li class="chapter" data-level="4.1" data-path="r-intro.html"><a href="r-intro.html#getting-started-1"><i class="fa fa-check"></i><b>4.1</b> Getting Started</a></li>
<li class="chapter" data-level="4.2" data-path="r-intro.html"><a href="r-intro.html#vectors"><i class="fa fa-check"></i><b>4.2</b> Vectors</a></li>
<li class="chapter" data-level="4.3" data-path="r-intro.html"><a href="r-intro.html#matrices"><i class="fa fa-check"></i><b>4.3</b> Matrices</a></li>
<li class="chapter" data-level="4.4" data-path="r-intro.html"><a href="r-intro.html#factors"><i class="fa fa-check"></i><b>4.4</b> Factors</a></li>
<li class="chapter" data-level="4.5" data-path="r-intro.html"><a href="r-intro.html#data-frames"><i class="fa fa-check"></i><b>4.5</b> Data Frames</a></li>
<li class="chapter" data-level="4.6" data-path="r-intro.html"><a href="r-intro.html#lists"><i class="fa fa-check"></i><b>4.6</b> Lists</a></li>
<li class="chapter" data-level="4.7" data-path="r-intro.html"><a href="r-intro.html#loops"><i class="fa fa-check"></i><b>4.7</b> Loops</a></li>
<li class="chapter" data-level="4.8" data-path="r-intro.html"><a href="r-intro.html#user-defined-functions-udfs"><i class="fa fa-check"></i><b>4.8</b> User-Defined Functions (UDFs)</a></li>
<li class="chapter" data-level="4.9" data-path="r-intro.html"><a href="r-intro.html#graphics"><i class="fa fa-check"></i><b>4.9</b> Graphics</a></li>
<li class="chapter" data-level="4.10" data-path="r-intro.html"><a href="r-intro.html#exercises"><i class="fa fa-check"></i><b>4.10</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="measure-sampl.html"><a href="measure-sampl.html"><i class="fa fa-check"></i><b>5</b> Measurement &amp; Sampling</a><ul>
<li class="chapter" data-level="5.1" data-path="measure-sampl.html"><a href="measure-sampl.html#variable-types"><i class="fa fa-check"></i><b>5.1</b> Variable Types</a><ul>
<li class="chapter" data-level="5.1.1" data-path="measure-sampl.html"><a href="measure-sampl.html#independent-variables-iv"><i class="fa fa-check"></i><b>5.1.1</b> Independent Variables (IV)</a></li>
<li class="chapter" data-level="5.1.2" data-path="measure-sampl.html"><a href="measure-sampl.html#dependent-variables-dv"><i class="fa fa-check"></i><b>5.1.2</b> Dependent Variables (DV)</a></li>
<li class="chapter" data-level="5.1.3" data-path="measure-sampl.html"><a href="measure-sampl.html#control-variables-cv"><i class="fa fa-check"></i><b>5.1.3</b> Control Variables (CV)</a></li>
<li class="chapter" data-level="5.1.4" data-path="measure-sampl.html"><a href="measure-sampl.html#moderating-variables"><i class="fa fa-check"></i><b>5.1.4</b> Moderating Variables</a></li>
<li class="chapter" data-level="5.1.5" data-path="measure-sampl.html"><a href="measure-sampl.html#mediating-variables"><i class="fa fa-check"></i><b>5.1.5</b> Mediating Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="measure-sampl.html"><a href="measure-sampl.html#measurement-scales"><i class="fa fa-check"></i><b>5.2</b> Measurement Scales</a><ul>
<li class="chapter" data-level="5.2.1" data-path="measure-sampl.html"><a href="measure-sampl.html#discrete-variables"><i class="fa fa-check"></i><b>5.2.1</b> Discrete Variables</a></li>
<li class="chapter" data-level="5.2.2" data-path="measure-sampl.html"><a href="measure-sampl.html#continuous-variables"><i class="fa fa-check"></i><b>5.2.2</b> Continuous Variables</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="measure-sampl.html"><a href="measure-sampl.html#sampling"><i class="fa fa-check"></i><b>5.3</b> Sampling</a><ul>
<li class="chapter" data-level="5.3.1" data-path="measure-sampl.html"><a href="measure-sampl.html#sampling-nonsampling-error"><i class="fa fa-check"></i><b>5.3.1</b> Sampling &amp; Nonsampling Error</a></li>
<li class="chapter" data-level="5.3.2" data-path="measure-sampl.html"><a href="measure-sampl.html#sampling-methods"><i class="fa fa-check"></i><b>5.3.2</b> Sampling Methods</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="measure-sampl.html"><a href="measure-sampl.html#exercises-1"><i class="fa fa-check"></i><b>5.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="research.html"><a href="research.html"><i class="fa fa-check"></i><b>6</b> Research Fundamentals</a><ul>
<li class="chapter" data-level="6.1" data-path="research.html"><a href="research.html#research-questions"><i class="fa fa-check"></i><b>6.1</b> Research Questions</a></li>
<li class="chapter" data-level="6.2" data-path="research.html"><a href="research.html#research-hypotheses"><i class="fa fa-check"></i><b>6.2</b> Research Hypotheses</a></li>
<li class="chapter" data-level="6.3" data-path="research.html"><a href="research.html#internal-vs.external-validity"><i class="fa fa-check"></i><b>6.3</b> Internal vs. External Validity</a></li>
<li class="chapter" data-level="6.4" data-path="research.html"><a href="research.html#research-methods"><i class="fa fa-check"></i><b>6.4</b> Research Methods</a></li>
<li class="chapter" data-level="6.5" data-path="research.html"><a href="research.html#research-designs"><i class="fa fa-check"></i><b>6.5</b> Research Designs</a></li>
<li class="chapter" data-level="6.6" data-path="research.html"><a href="research.html#exercises-2"><i class="fa fa-check"></i><b>6.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="desc-stats.html"><a href="desc-stats.html"><i class="fa fa-check"></i><b>7</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="7.1" data-path="desc-stats.html"><a href="desc-stats.html#univariate-analysis"><i class="fa fa-check"></i><b>7.1</b> Univariate Analysis</a><ul>
<li class="chapter" data-level="7.1.1" data-path="desc-stats.html"><a href="desc-stats.html#measures-of-central-tendency"><i class="fa fa-check"></i><b>7.1.1</b> Measures of Central Tendency</a></li>
<li class="chapter" data-level="7.1.2" data-path="desc-stats.html"><a href="desc-stats.html#measures-of-spread"><i class="fa fa-check"></i><b>7.1.2</b> Measures of Spread</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="desc-stats.html"><a href="desc-stats.html#bivariate-analysis"><i class="fa fa-check"></i><b>7.2</b> Bivariate Analysis</a><ul>
<li class="chapter" data-level="7.2.1" data-path="desc-stats.html"><a href="desc-stats.html#covariance"><i class="fa fa-check"></i><b>7.2.1</b> Covariance</a></li>
<li class="chapter" data-level="7.2.2" data-path="desc-stats.html"><a href="desc-stats.html#correlation"><i class="fa fa-check"></i><b>7.2.2</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="desc-stats.html"><a href="desc-stats.html#exercises-3"><i class="fa fa-check"></i><b>7.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="inf-stats.html"><a href="inf-stats.html"><i class="fa fa-check"></i><b>8</b> Statistical Inference</a><ul>
<li class="chapter" data-level="8.1" data-path="inf-stats.html"><a href="inf-stats.html#introduction-to-probability"><i class="fa fa-check"></i><b>8.1</b> Introduction to Probability</a><ul>
<li class="chapter" data-level="8.1.1" data-path="inf-stats.html"><a href="inf-stats.html#probability-distributions"><i class="fa fa-check"></i><b>8.1.1</b> Probability Distributions</a></li>
<li class="chapter" data-level="8.1.2" data-path="inf-stats.html"><a href="inf-stats.html#conditional-probability"><i class="fa fa-check"></i><b>8.1.2</b> Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="inf-stats.html"><a href="inf-stats.html#central-limit-theorem"><i class="fa fa-check"></i><b>8.2</b> Central Limit Theorem</a></li>
<li class="chapter" data-level="8.3" data-path="inf-stats.html"><a href="inf-stats.html#confidence-intervals"><i class="fa fa-check"></i><b>8.3</b> Confidence Intervals</a><ul>
<li class="chapter" data-level="8.3.1" data-path="inf-stats.html"><a href="inf-stats.html#hypothesis-testing"><i class="fa fa-check"></i><b>8.3.1</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="8.3.2" data-path="inf-stats.html"><a href="inf-stats.html#alpha"><i class="fa fa-check"></i><b>8.3.2</b> Alpha</a></li>
<li class="chapter" data-level="8.3.3" data-path="inf-stats.html"><a href="inf-stats.html#beta"><i class="fa fa-check"></i><b>8.3.3</b> Beta</a></li>
<li class="chapter" data-level="8.3.4" data-path="inf-stats.html"><a href="inf-stats.html#type-i-ii-errors"><i class="fa fa-check"></i><b>8.3.4</b> Type I &amp; II Errors</a></li>
<li class="chapter" data-level="8.3.5" data-path="inf-stats.html"><a href="inf-stats.html#p-values"><i class="fa fa-check"></i><b>8.3.5</b> <span class="math inline">\(p\)</span>-Values</a></li>
<li class="chapter" data-level="8.3.6" data-path="inf-stats.html"><a href="inf-stats.html#bonferroni-correction"><i class="fa fa-check"></i><b>8.3.6</b> Bonferroni Correction</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="inf-stats.html"><a href="inf-stats.html#exercises-4"><i class="fa fa-check"></i><b>8.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html"><i class="fa fa-check"></i><b>9</b> Data Wrangling and Preparation</a><ul>
<li class="chapter" data-level="9.1" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#data-management"><i class="fa fa-check"></i><b>9.1</b> Data Management</a></li>
<li class="chapter" data-level="9.2" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#sql"><i class="fa fa-check"></i><b>9.2</b> SQL</a></li>
<li class="chapter" data-level="9.3" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#data-screening-cleaning"><i class="fa fa-check"></i><b>9.3</b> Data Screening &amp; Cleaning</a></li>
<li class="chapter" data-level="9.4" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.4</b> One-Hot Encoding</a></li>
<li class="chapter" data-level="9.5" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#feature-engineering"><i class="fa fa-check"></i><b>9.5</b> Feature Engineering</a></li>
<li class="chapter" data-level="9.6" data-path="data-wrang-prep.html"><a href="data-wrang-prep.html#exercises-5"><i class="fa fa-check"></i><b>9.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="aod.html"><a href="aod.html"><i class="fa fa-check"></i><b>10</b> Analysis of Differences</a><ul>
<li class="chapter" data-level="10.1" data-path="aod.html"><a href="aod.html#parametric-vs.nonparametric-tests"><i class="fa fa-check"></i><b>10.1</b> Parametric vs. Nonparametric Tests</a></li>
<li class="chapter" data-level="10.2" data-path="aod.html"><a href="aod.html#differences-in-discrete-data"><i class="fa fa-check"></i><b>10.2</b> Differences in Discrete Data</a></li>
<li class="chapter" data-level="10.3" data-path="aod.html"><a href="aod.html#differences-in-continuous-data"><i class="fa fa-check"></i><b>10.3</b> Differences in Continuous Data</a><ul>
<li class="chapter" data-level="10.3.1" data-path="aod.html"><a href="aod.html#post-hoc-tests"><i class="fa fa-check"></i><b>10.3.1</b> Post-Hoc Tests</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="aod.html"><a href="aod.html#exercises-6"><i class="fa fa-check"></i><b>10.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>11</b> Linear Regression</a><ul>
<li class="chapter" data-level="11.1" data-path="lm.html"><a href="lm.html#simple-linear-regression"><i class="fa fa-check"></i><b>11.1</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="11.1.1" data-path="lm.html"><a href="lm.html#parameter-estimation"><i class="fa fa-check"></i><b>11.1.1</b> Parameter Estimation</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="lm.html"><a href="lm.html#multiple-linear-regression"><i class="fa fa-check"></i><b>11.2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="11.2.1" data-path="lm.html"><a href="lm.html#moderation"><i class="fa fa-check"></i><b>11.2.1</b> Moderation</a></li>
<li class="chapter" data-level="11.2.2" data-path="lm.html"><a href="lm.html#mediation"><i class="fa fa-check"></i><b>11.2.2</b> Mediation</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="lm.html"><a href="lm.html#polynomial-regression"><i class="fa fa-check"></i><b>11.3</b> Polynomial Regression</a></li>
<li class="chapter" data-level="11.4" data-path="lm.html"><a href="lm.html#hierarchical-models"><i class="fa fa-check"></i><b>11.4</b> Hierarchical Models</a></li>
<li class="chapter" data-level="11.5" data-path="lm.html"><a href="lm.html#exercises-7"><i class="fa fa-check"></i><b>11.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>12</b> Generalized Linear Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="glm.html"><a href="glm.html#logistic-regression"><i class="fa fa-check"></i><b>12.1</b> Logistic Regression</a><ul>
<li class="chapter" data-level="12.1.1" data-path="glm.html"><a href="glm.html#binomial-logistic-regression"><i class="fa fa-check"></i><b>12.1.1</b> Binomial Logistic Regression</a></li>
<li class="chapter" data-level="12.1.2" data-path="glm.html"><a href="glm.html#multinomial-logistic-regression"><i class="fa fa-check"></i><b>12.1.2</b> Multinomial Logistic Regression</a></li>
<li class="chapter" data-level="12.1.3" data-path="glm.html"><a href="glm.html#ordinal-logistic-regression"><i class="fa fa-check"></i><b>12.1.3</b> Ordinal Logistic Regression</a></li>
<li class="chapter" data-level="12.1.4" data-path="glm.html"><a href="glm.html#proportional-odds-logistic-regression"><i class="fa fa-check"></i><b>12.1.4</b> Proportional Odds Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="glm.html"><a href="glm.html#poisson-regression"><i class="fa fa-check"></i><b>12.2</b> Poisson Regression</a></li>
<li class="chapter" data-level="12.3" data-path="glm.html"><a href="glm.html#exercises-8"><i class="fa fa-check"></i><b>12.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="surv-anal.html"><a href="surv-anal.html"><i class="fa fa-check"></i><b>13</b> Survival Analysis</a><ul>
<li class="chapter" data-level="13.1" data-path="surv-anal.html"><a href="surv-anal.html#exercises-9"><i class="fa fa-check"></i><b>13.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="pred-mod.html"><a href="pred-mod.html"><i class="fa fa-check"></i><b>14</b> Predictive Models</a><ul>
<li class="chapter" data-level="14.1" data-path="pred-mod.html"><a href="pred-mod.html#bias-variance-trade-off"><i class="fa fa-check"></i><b>14.1</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="14.2" data-path="pred-mod.html"><a href="pred-mod.html#cross-validation"><i class="fa fa-check"></i><b>14.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="14.3" data-path="pred-mod.html"><a href="pred-mod.html#balancing-classes"><i class="fa fa-check"></i><b>14.3</b> Balancing Classes</a></li>
<li class="chapter" data-level="14.4" data-path="pred-mod.html"><a href="pred-mod.html#model-performance"><i class="fa fa-check"></i><b>14.4</b> Model Performance</a></li>
<li class="chapter" data-level="14.5" data-path="pred-mod.html"><a href="pred-mod.html#automated-machine-learning-automl"><i class="fa fa-check"></i><b>14.5</b> Automated Machine Learning (AutoML)</a></li>
<li class="chapter" data-level="14.6" data-path="pred-mod.html"><a href="pred-mod.html#exercises-10"><i class="fa fa-check"></i><b>14.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="unsup-lrn.html"><a href="unsup-lrn.html"><i class="fa fa-check"></i><b>15</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="15.1" data-path="unsup-lrn.html"><a href="unsup-lrn.html#factor-analysis"><i class="fa fa-check"></i><b>15.1</b> Factor Analysis</a></li>
<li class="chapter" data-level="15.2" data-path="unsup-lrn.html"><a href="unsup-lrn.html#clustering"><i class="fa fa-check"></i><b>15.2</b> Clustering</a></li>
<li class="chapter" data-level="15.3" data-path="unsup-lrn.html"><a href="unsup-lrn.html#exercises-11"><i class="fa fa-check"></i><b>15.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="net-anal.html"><a href="net-anal.html"><i class="fa fa-check"></i><b>16</b> Network Analysis</a><ul>
<li class="chapter" data-level="16.1" data-path="net-anal.html"><a href="net-anal.html#centrality-measures"><i class="fa fa-check"></i><b>16.1</b> Centrality Measures</a></li>
<li class="chapter" data-level="16.2" data-path="net-anal.html"><a href="net-anal.html#exercises-12"><i class="fa fa-check"></i><b>16.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="data-viz.html"><a href="data-viz.html"><i class="fa fa-check"></i><b>17</b> Data Visualization</a><ul>
<li class="chapter" data-level="17.1" data-path="data-viz.html"><a href="data-viz.html#design-guidelines"><i class="fa fa-check"></i><b>17.1</b> Design Guidelines</a></li>
<li class="chapter" data-level="17.2" data-path="data-viz.html"><a href="data-viz.html#visualization-types"><i class="fa fa-check"></i><b>17.2</b> Visualization Types</a><ul>
<li class="chapter" data-level="17.2.1" data-path="data-viz.html"><a href="data-viz.html#tables"><i class="fa fa-check"></i><b>17.2.1</b> Tables</a></li>
<li class="chapter" data-level="17.2.2" data-path="data-viz.html"><a href="data-viz.html#heatmaps"><i class="fa fa-check"></i><b>17.2.2</b> Heatmaps</a></li>
<li class="chapter" data-level="17.2.3" data-path="data-viz.html"><a href="data-viz.html#scatterplots"><i class="fa fa-check"></i><b>17.2.3</b> Scatterplots</a></li>
<li class="chapter" data-level="17.2.4" data-path="data-viz.html"><a href="data-viz.html#line-graphs"><i class="fa fa-check"></i><b>17.2.4</b> Line Graphs</a></li>
<li class="chapter" data-level="17.2.5" data-path="data-viz.html"><a href="data-viz.html#slopegraphs"><i class="fa fa-check"></i><b>17.2.5</b> Slopegraphs</a></li>
<li class="chapter" data-level="17.2.6" data-path="data-viz.html"><a href="data-viz.html#bar-charts"><i class="fa fa-check"></i><b>17.2.6</b> Bar Charts</a></li>
<li class="chapter" data-level="17.2.7" data-path="data-viz.html"><a href="data-viz.html#waterfall-charts"><i class="fa fa-check"></i><b>17.2.7</b> Waterfall Charts</a></li>
<li class="chapter" data-level="17.2.8" data-path="data-viz.html"><a href="data-viz.html#area-charts"><i class="fa fa-check"></i><b>17.2.8</b> Area Charts</a></li>
<li class="chapter" data-level="17.2.9" data-path="data-viz.html"><a href="data-viz.html#pie-charts"><i class="fa fa-check"></i><b>17.2.9</b> Pie Charts</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="data-viz.html"><a href="data-viz.html#exercises-13"><i class="fa fa-check"></i><b>17.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="storytelling.html"><a href="storytelling.html"><i class="fa fa-check"></i><b>18</b> Data Storytelling</a><ul>
<li class="chapter" data-level="18.1" data-path="storytelling.html"><a href="storytelling.html#know-your-audience"><i class="fa fa-check"></i><b>18.1</b> Know Your Audience</a></li>
<li class="chapter" data-level="18.2" data-path="storytelling.html"><a href="storytelling.html#tldr"><i class="fa fa-check"></i><b>18.2</b> TL;DR</a></li>
<li class="chapter" data-level="18.3" data-path="storytelling.html"><a href="storytelling.html#telling-the-story"><i class="fa fa-check"></i><b>18.3</b> Telling the Story</a></li>
<li class="chapter" data-level="18.4" data-path="storytelling.html"><a href="storytelling.html#reference-material"><i class="fa fa-check"></i><b>18.4</b> Reference Material</a></li>
<li class="chapter" data-level="18.5" data-path="storytelling.html"><a href="storytelling.html#exercises-14"><i class="fa fa-check"></i><b>18.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="bibli.html"><a href="bibli.html"><i class="fa fa-check"></i><b>19</b> Bibliography</a></li>
<li class="chapter" data-level="20" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>20</b> Appendix</a><ul>
<li class="chapter" data-level="20.1" data-path="appendix.html"><a href="appendix.html#exercise-solutions"><i class="fa fa-check"></i><b>20.1</b> Exercise Solutions</a></li>
<li class="chapter" data-level="20.2" data-path="appendix.html"><a href="appendix.html#d-framework-1"><i class="fa fa-check"></i><b>20.2</b> 4D Framework</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The People Analytics Companion</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="aod" class="section level1">
<h1><span class="header-section-number">10</span> Analysis of Differences</h1>
<p>There are many statistical tests that can be used to test for differences within or between two or more groups. This chapter will cover the most common types of differences in people analytics and the tests applicable to each.</p>
<div id="parametric-vs.nonparametric-tests" class="section level2">
<h2><span class="header-section-number">10.1</span> Parametric vs. Nonparametric Tests</h2>
<p>In the context of data measured on a continuous scale (quantitative), we will cover parametric tests along with their nonparametric counterparts. When the hypothesis relates to average (mean) differences and <span class="math inline">\(n\)</span> is large, <strong>parametric tests</strong> are preferred as they generally have more statistical power. <strong>Nonparametric tests</strong> are <strong>distribution-free tests</strong> that do not require the population’s distribution to be characterized by certain parameters, such as a normal distribution defined by a mean and standard deviation. Nonparametric tests are great for qualitative data since the distribution of non-numeric data cannot be characterized by parameters.</p>
<p>Beyond ensuring the data were generated from a random and representative process, as well as following the data screening procedures outlined in Chapter <a href="data-wrang-prep.html#data-wrang-prep">9</a> (e.g., addressing concerning outliers), parametric tests of differences <em>generally</em> feature three key assumptions:</p>
<ol style="list-style-type: decimal">
<li><strong>Independence</strong>: Observations within each group are independent of each other</li>
<li><strong>Homogeneity of Variance</strong>: Variances of populations from which samples were drawn are equal</li>
<li><strong>Normality</strong>: Residuals must be normally distributed (with mean of 0) within each group</li>
</ol>
<p>While homogeneity of variance assumes the variances across multiple groups are equal, parametric tests are generally robust to violations of equal variances when the sample sizes are large. Also, you may recall that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are sufficient to characterize a population distribution when data are situated symmetrically around the mean. However, the mean can be sensitive to outliers so if outliers are present in the data, the median may be a better way of representing the data’s center (i.e., nonparametric tests); just remember that the use of nonparametric tests requires hypotheses to be modified to adjust for <em>median</em> – rather than <em>mean</em> – centers.</p>
<p>You may be wondering whether the magical elixir that is the CLT, which we covered in Chapter <a href="inf-stats.html#inf-stats">8</a>, influences our ability to utilize parametric tests with respect to the assumption of normality. It’s important to remember that the normal distribution properties under the CLT relate to the <em>sampling distribution of means</em> – not to the distribution of the population or to the data for an individual sample. The CLT is important for estimating population parameters, but it does not transform a population distribution from nonnormal to normal. If we know the population distribution is nonnormal (e.g., ordinal, nominal, or skewed data), nonparametric tests should be considered. This is why we used Spearman’s correlation coefficient – a nonparametric test – in Chapter <a href="desc-stats.html#desc-stats">7</a> to evaluate the relationship between job level and education; these ordinal data are not normally distributed in the population.</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb251-1" data-line-number="1"><span class="co"># Load employee data</span></a>
<a class="sourceLine" id="cb251-2" data-line-number="2">employees &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv&quot;</span>)</a></code></pre></div>
</div>
<div id="differences-in-discrete-data" class="section level2">
<h2><span class="header-section-number">10.2</span> Differences in Discrete Data</h2>
<p>Discrete data require nonparametric tests since these data do not come from normally distributed populations. The two most commonly used tests to analyze discrete variables are the <em>Chi-square test</em> and <em>Fischer’s exact test</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:discrete-tests"></span>
<img src="graphics/discrete_differences_test_table.png" alt="Chi-Square and Fisher Exact Test Criteria for Discrete Data" width="100%" />
<p class="caption">
Figure 10.1: Chi-Square and Fisher Exact Test Criteria for Discrete Data
</p>
</div>
<p>Both tests organize data within 2x2 <strong>contingency tables</strong> which enables us to understand interrelations between variables. Figure <a href="aod.html#fig:contingency-tbl">10.2</a> illustrates a 2x2 contingency table for promotion frequency between remote and non-remote workers:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:contingency-tbl"></span>
<img src="graphics/contingency_table.png" alt="2x2 Contingency Table of Promotions for Remote and Non-Remote Workers" width="75%" />
<p class="caption">
Figure 10.2: 2x2 Contingency Table of Promotions for Remote and Non-Remote Workers
</p>
</div>
<p><strong>Chi-Square Test</strong></p>
<p>The <strong>Chi-Square Test of Independence</strong> evaluates patterns of observations to determine if categories occur more frequently than we would expect by chance. The Chi-square statistic is defined by:</p>
<p><span class="math inline">\({\chi}^2 = \sum\frac{(O_i - E_i)^2}{E_i},\)</span></p>
<p>where <span class="math inline">\(O_i\)</span> is the observed value, and <span class="math inline">\(E_i\)</span> is the expected value.</p>
<p><span class="math inline">\(H_0\)</span> states that each variable is independent of one other (i.e., there is no relationship). In addition to the test statistic, <span class="math inline">\({\chi}^2\)</span>, <span class="math inline">\(df\)</span> for the contingency table – defined by <span class="math inline">\(df = (rows - 1) * (columns - 1)\)</span> – is required to determine whether we reject or fail to reject <span class="math inline">\(H_0\)</span>.</p>
<p>While there is not consensus on the minimum sample size for this test, it is important to note that the <span class="math inline">\({\chi}^2\)</span> statistic follows a chi-square distribution <em>asymptotically</em>. This means we can only calculate accurate <span class="math inline">\(p\)</span>-values for larger samples, and a general rule of thumb is that the expected value for each cell needs to be at least 5. The challenge with small <span class="math inline">\(n\)</span>-counts is illustrated in Figure <a href="aod.html#fig:chisq-dist">10.3</a>; the chi-square distribution approaches a vertical line as <span class="math inline">\(df\)</span> drops below 5.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chisq-dist"></span>
<img src="People_Analytics_Lifecycle_files/figure-html/chisq-dist-1.png" alt="Chi-Square Distributions by Degrees of Freedom (df)" width="100%" />
<p class="caption">
Figure 10.3: Chi-Square Distributions by Degrees of Freedom (df)
</p>
</div>
<p>We will demonstrate how to perform a chi-square test of independence by evaluating whether exit rates are independent of whether an employee works overtime. Let’s first construct a 2x2 contingency table:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" data-line-number="1"><span class="co"># Create contingency table</span></a>
<a class="sourceLine" id="cb252-2" data-line-number="2">cont_tbl &lt;-<span class="st"> </span><span class="kw">table</span>(employees<span class="op">$</span>active, employees<span class="op">$</span>overtime)</a>
<a class="sourceLine" id="cb252-3" data-line-number="3">cont_tbl</a></code></pre></div>
<pre><code>##      
##        No Yes
##   No  110 127
##   Yes 944 289</code></pre>
<p>A <strong>mosaic plot</strong> is a great way to visualize the delta between expected and observed frequencies for each cell. This can be produced by using the <code>mosaicplot()</code> function from the <code>graphics</code> library:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mosaic-plot"></span>
<img src="People_Analytics_Lifecycle_files/figure-html/mosaic-plot-1.png" alt="Mosaic Plot of Residuals for Overtime x Active Status" width="75%" />
<p class="caption">
Figure 10.4: Mosaic Plot of Residuals for Overtime x Active Status
</p>
</div>
<p>In Figure <a href="aod.html#fig:mosaic-plot">10.4</a>, blue indicates that the observed value is higher than the expected value. Red indicates that the observed value is lower than the expected value. Based on this plot, there appears to be some meaningful departures from expected values in both the high and low directions.</p>
<p>Let’s run the chi-square test of independence to determine whether these residuals are significant. This test can be performed in R using the <code>chisq.test()</code> function:</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb254-1" data-line-number="1"><span class="co"># Perform chi-square test of independence</span></a>
<a class="sourceLine" id="cb254-2" data-line-number="2"><span class="kw">chisq.test</span>(cont_tbl)</a></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  cont_tbl
## X-squared = 87.564, df = 1, p-value &lt; 2.2e-16</code></pre>
<p>Based on the results, exit rates are not independent of overtime (<span class="math inline">\({\chi}^2(1) = 87.56, p &lt; .05\)</span>). Therefore, there is a relationship between an employee working overtime and the rate at which they change from an active to inactive status.</p>
<p>The most common strength test when a significant Chi-Square statistic is observed is Cramer’s V.</p>
<p><strong>Fisher’s Exact Test</strong></p>
<p>When the sample size is small, <strong>Fisher’s Exact Test</strong> can be used to calculate the <em>exact</em> <span class="math inline">\(p\)</span>-value rather than an approximation – as with many statistical tests, such as the chi-square test.</p>
<p><span class="math inline">\(H_0\)</span> for Fisher’s exact test is the same as <span class="math inline">\(H_0\)</span> for the chi-square test of independence: There is no relationship between the two categorical variables (i.e., they are independent). We can perform Fisher’s exact test using the <code>fisher.test()</code> function in R:</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb256-1" data-line-number="1"><span class="co"># Perform Fisher&#39;s exact test</span></a>
<a class="sourceLine" id="cb256-2" data-line-number="2"><span class="kw">fisher.test</span>(cont_tbl)</a></code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  cont_tbl
## p-value &lt; 2.2e-16
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.1969101 0.3572582
## sample estimates:
## odds ratio 
##  0.2654384</code></pre>
<p>Based on results from Fisher’s exact test, which are consistent with the chi-square test of independence, we reject the null hypothesis and conclude that exit rates are related to working overtime.</p>
</div>
<div id="differences-in-continuous-data" class="section level2">
<h2><span class="header-section-number">10.3</span> Differences in Continuous Data</h2>
<p>A variety of parametric and nonparametric tests are available for evaluating differences between variables measured on a continuous scale. Figure <a href="aod.html#fig:continuous-tests">10.5</a> provides a side-by-side of these parametric and corresponding nonparametric tests of differences.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:continuous-tests"></span>
<img src="graphics/continuous_differences_test_table.png" alt="Parametric and Nonparametric Tests of Differences for Continuous Data" width="100%" />
<p class="caption">
Figure 10.5: Parametric and Nonparametric Tests of Differences for Continuous Data
</p>
</div>
<p>### Independent Samples <span class="math inline">\(t\)</span>-Test</p>
<p>When evaluating differences between two independent samples, social psychology researchers generally select from two tests: <em>Student’s</em> <span class="math inline">\(t\)</span><em>-test</em> and <em>Welch’s</em> <span class="math inline">\(t\)</span><em>-test</em>. There are other less common alternatives, such as <em>Yuen’s</em> <span class="math inline">\(t\)</span><em>-test</em> and a <em>bootstrapped</em> <span class="math inline">\(t\)</span><em>-test</em>, but these are less commonly reported in scholarly social science journals and will not be covered in this book.</p>
<p>The <strong>Student’s</strong> <span class="math inline">\(t\)</span><strong>-test</strong> is a parametric test whose assumptions of equal variances seldom hold in people analytics. <strong>Welch’s</strong> <span class="math inline">\(t\)</span><strong>-test</strong> is generally preferred to the Student’s <span class="math inline">\(t\)</span>-test because it has been shown to provide better control of Type 1 error rates when homogeneity of variance is not met, whilst losing little robustness (e.g., Delacre, Lakens, &amp; Leys, 2017). When <span class="math inline">\(n\)</span> is equal between groups, the Student’s <span class="math inline">\(t\)</span>-test is known to be robust to violations of the equal variance assumption, as long as <span class="math inline">\(n\)</span> is sufficiently high to accurately estimate parameters and the underlying distribution is not characterized by high skewness and kurtosis.</p>
<p>Let’s explore the mechanics of independent samples <span class="math inline">\(t\)</span>-tests. Figure <a href="aod.html#fig:mean-group-diff">10.6</a> illustrates mean differences (<span class="math inline">\(MD\)</span>) for nine Welch’s <span class="math inline">\(t\)</span>-tests based on random sample data generated from independent normal populations. Remember that statistical power increases with a large <span class="math inline">\(n\)</span>, as <span class="math inline">\(t\)</span> distributions approximate a standard normal distribution with larger <span class="math inline">\(df\)</span>. In the context of analysis of differences, this translates to an increase in the likelihood of detecting statistical differences in the means of two distributions. Note that for the two cases where both <span class="math inline">\(MD\)</span> and <span class="math inline">\(n\)</span> are relatively small, mean differences are not statistically significant (<span class="math inline">\(p &gt; .05\)</span>). You may also notice that as the absolute value of the <span class="math inline">\(t\)</span>-statistic approaches 0, statistical differences become less likely since a smaller <span class="math inline">\(t\)</span>-statistic indicates a smaller difference between mean values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mean-group-diff"></span>
<img src="People_Analytics_Lifecycle_files/figure-html/mean-group-diff-1.png" alt="Density plots comparing distributions with a mean of 100 (blue) and 120 (grey) with sample sizes of 100, 1,000, and 10,000 (rows) and standard deviations of 25, 50, and 75 (columns). Dashed vertical lines represent distribution means." width="100%" />
<p class="caption">
Figure 10.6: Density plots comparing distributions with a mean of 100 (blue) and 120 (grey) with sample sizes of 100, 1,000, and 10,000 (rows) and standard deviations of 25, 50, and 75 (columns). Dashed vertical lines represent distribution means.
</p>
</div>
<p>Next, we will walk through the steps involved in performing Welch’s <span class="math inline">\(t\)</span>-test. Let’s first visualize the distribution of data for each group using boxplots:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:comp-job-boxplots"></span>
<img src="People_Analytics_Lifecycle_files/figure-html/comp-job-boxplots-1.png" alt="Annual Compensation Distributions for Managers and Research Scientists" width="100%" />
<p class="caption">
Figure 10.7: Annual Compensation Distributions for Managers and Research Scientists
</p>
</div>
<p>While the median – rather than the mean which is being evaluated with Welch’s <span class="math inline">\(t\)</span>-test – is shown in these boxplots, this is a great way to visually inspect whether there are meaningful differences in the distribution of data between groups. We could of course use density plots as an alternative. As we can see, annual compensation for employees with a Manager job title tends to be slightly higher than for those with a Research Scientist job title. However, these distributions appear to be roughly similar.</p>
<p>Next, let’s test whether the slight differences we observed visually are statistically significant. To perform Welch’s <span class="math inline">\(t\)</span>-test in R, we can simply pass into the <code>t.test()</code> function a numeric vector for each of the two groups.</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb258-1" data-line-number="1"><span class="co"># Create compensation vectors for two jobs</span></a>
<a class="sourceLine" id="cb258-2" data-line-number="2">comp_mgr &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, job_title <span class="op">==</span><span class="st"> &#39;Manager&#39;</span>, <span class="dt">select =</span> annual_comp))</a>
<a class="sourceLine" id="cb258-3" data-line-number="3">comp_rsci &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, job_title <span class="op">==</span><span class="st"> &#39;Research Scientist&#39;</span>, <span class="dt">select =</span> annual_comp))</a>
<a class="sourceLine" id="cb258-4" data-line-number="4"></a>
<a class="sourceLine" id="cb258-5" data-line-number="5"><span class="co"># Run Welch&#39;s t-test</span></a>
<a class="sourceLine" id="cb258-6" data-line-number="6"><span class="kw">t.test</span>(comp_mgr, comp_rsci)</a></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  comp_mgr and comp_rsci
## t = 0.22623, df = 159.55, p-value = 0.8213
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -8860.685 11153.244
## sample estimates:
## mean of x mean of y 
##  139900.8  138754.5</code></pre>
<p>Based on this output, we can see that <span class="math inline">\(\bar{x}\)</span> = 139,901 for Managers and <span class="math inline">\(\bar{x}\)</span> = 138,755 for Research Scientists, and these are not statistically different (<span class="math inline">\(t(159.55)\)</span> = .23, <span class="math inline">\(p\)</span> = .82).</p>
<p>Note that we can access specific metrics from this output by storing results to an object and then referencing specific elements by name or index:</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" data-line-number="1"><span class="co"># This assigns each element of results from Welch&#39;s t-test to an indexed position in the object</span></a>
<a class="sourceLine" id="cb260-2" data-line-number="2">t_rslts &lt;-<span class="st"> </span><span class="kw">t.test</span>(comp_mgr, comp_rsci)</a>
<a class="sourceLine" id="cb260-3" data-line-number="3"></a>
<a class="sourceLine" id="cb260-4" data-line-number="4">t_rslts<span class="op">$</span>statistic <span class="co"># t-statistic</span></a></code></pre></div>
<pre><code>##         t 
## 0.2262262</code></pre>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" data-line-number="1">t_rslts<span class="op">$</span>parameter <span class="co"># df</span></a></code></pre></div>
<pre><code>##       df 
## 159.5544</code></pre>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" data-line-number="1">t_rslts<span class="op">$</span>p.value <span class="co"># p-value</span></a></code></pre></div>
<pre><code>## [1] 0.8213151</code></pre>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" data-line-number="1">t_rslts<span class="op">$</span>method <span class="co"># type of t-test</span></a></code></pre></div>
<pre><code>## [1] &quot;Welch Two Sample t-test&quot;</code></pre>
<p>When object elements are referenced by index, the element name is displayed in the output to clarify what the metric represents:</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb268-1" data-line-number="1">t_rslts[<span class="dv">1</span>] <span class="co"># t-statistic</span></a></code></pre></div>
<pre><code>## $statistic
##         t 
## 0.2262262</code></pre>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb270-1" data-line-number="1">t_rslts[<span class="dv">2</span>] <span class="co"># df</span></a></code></pre></div>
<pre><code>## $parameter
##       df 
## 159.5544</code></pre>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb272-1" data-line-number="1">t_rslts[<span class="dv">3</span>] <span class="co"># p-value</span></a></code></pre></div>
<pre><code>## $p.value
## [1] 0.8213151</code></pre>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb274-1" data-line-number="1">t_rslts[<span class="dv">9</span>] <span class="co"># type of t-test</span></a></code></pre></div>
<pre><code>## $method
## [1] &quot;Welch Two Sample t-test&quot;</code></pre>
<p>Given <span class="math inline">\(df\)</span> = 159.55, you may be wondering how <span class="math inline">\(df\)</span> is calculated for Welch’s <span class="math inline">\(t\)</span>-test given that thus far, we have only discussed the basic <span class="math inline">\(df\)</span> calculation outlined in Chapter <a href="inf-stats.html#inf-stats">8</a>; namely, <span class="math inline">\(df = n - 1\)</span>. Welch’s <span class="math inline">\(t\)</span>-test uses the <strong>Welch-Satterthwaite equation</strong> for <span class="math inline">\(df\)</span> (Satterthwaite, 1946; Welch, 1947). This equation approximates <span class="math inline">\(df\)</span> for a linear combination of independent sample variances; which means that if samples are not independent, this approximation may not be valid. The Welch-Satterthwaite equation is defined by:</p>
<p><span class="math display">\[ df = \frac {(\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2})^2} {\frac{1}{n_1 - 1} (\frac{s^2_1}{n_1})^2 + \frac{1}{n_2 - 1} (\frac{s^2_2}{n_2})^2} \]</span></p>
<p>### Mann-Whitney U Test</p>
<p>A popular nonparametric (distribution-free) alternative to Welch’s <span class="math inline">\(t\)</span>-test is the <strong>Mann-Whitney U Test</strong>, sometimes referred to as the <strong>Wilcoxon Rank-Sum Test</strong>. Rather than comparing the mean between two groups, like the Student’s <span class="math inline">\(t\)</span>-test or Welch’s <span class="math inline">\(t\)</span>-test, the Mann-Whitney U test considers the entire distribution by evaluating the extent to which the <em>ranks</em> are consistent between groups (i.e., similarity in the proportion of records with each value). When distributions are similar, the medians of the two groups are compared.</p>
<p>The <code>wilcox.test()</code> function is used to run this test in R. Let’s illustrate by examining whether engagement (an ordinal variable in our dataset) is significantly different between those who have been promoted in the past year and those who have not:</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb276-1" data-line-number="1"><span class="co"># Create dummy-coded promotion variable</span></a>
<a class="sourceLine" id="cb276-2" data-line-number="2">employees<span class="op">$</span>promo &lt;-<span class="st"> </span><span class="kw">ifelse</span>(employees<span class="op">$</span>last_promo <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb276-3" data-line-number="3"></a>
<a class="sourceLine" id="cb276-4" data-line-number="4"><span class="co"># Create numeric engagement vectors for promo groups</span></a>
<a class="sourceLine" id="cb276-5" data-line-number="5">no_promo &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, promo <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, <span class="dt">select =</span> engagement))</a>
<a class="sourceLine" id="cb276-6" data-line-number="6">promo &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, promo <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">select =</span> engagement))</a>
<a class="sourceLine" id="cb276-7" data-line-number="7"></a>
<a class="sourceLine" id="cb276-8" data-line-number="8"><span class="co"># Perform the Mann-Whitney U (aka Wilcoxon rank sum) test</span></a>
<a class="sourceLine" id="cb276-9" data-line-number="9"><span class="kw">wilcox.test</span>(no_promo, promo)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  no_promo and promo
## W = 196056, p-value = 0.6707
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Based on these results, we fail to reject the null hypothesis, which states that there is no difference in engagement between those with and without promotions (<span class="math inline">\(W\)</span> = 196,056, <span class="math inline">\(p\)</span> = .67). Note the reference to continuity correction in the output. <strong>Continuity correction</strong> is applied when using a continuous distribution to approximate a discrete distribution. The Mann-Whitney U test we performed approximated a continuous distribution for testing differences between our ordinal (discrete) engagement data by applying this continuity correction.</p>
<p>### Paired Samples <span class="math inline">\(t\)</span>-Test</p>
<p>A <strong>Paired Samples <span class="math inline">\(t\)</span>-Test</strong> is used to compare means between pairs of measurements. This test is known by many other names, such as a <strong>dependent samples</strong> <span class="math inline">\(t\)</span><strong>-test</strong>, <strong>paired-difference</strong> <span class="math inline">\(t\)</span><strong>-test</strong>, <strong>matched pairs</strong> <span class="math inline">\(t\)</span><strong>-test</strong>, and <strong>repeated-samples</strong> <span class="math inline">\(t\)</span><strong>-test</strong>.</p>
<p>The assumption of normality in the context of a paired samples <span class="math inline">\(t\)</span>-test relates to normally distributed paired differences. This is important, as the <span class="math inline">\(p\)</span>-value for the test statistic will not be valid if this assumption is violated.</p>
<p>To illustrate, let’s design an experiment. Let’s assume morale has declined for employees who travel frequently, and a number of actions have been proposed by a task force to help address this. The task force has decided to pilot a new flexible work benefit over a six-month period to determine if it has a meaningful effect on morale. This new benefit is piloted to a random sample of frequent travelers, and our task is to test whether the outcomes warrant a broader rollout to frequent travelers.</p>
<p>Our DV (happiness) will be measured using a composite index derived from individual engagement, environment satisfaction, job satisfaction, and relationship satisfaction scores. Our objective is to determine if there is a significant improvement in this happiness index for the treatment group (those who are part of the flexible work pilot) relative to the pre/post difference for the control group (those not selected for the flexible work pilot).</p>
<p>While we could simply look at the pre/post differences for the treatment group, we understand from Chapter <a href="research.html#research">6</a> that this would be a weak design with material shortcomings. There could be alternative explanations for any observed increases in happiness that are unrelated to the intevention itself. For example, between time 1 and time 2, travel frequency may have decreased for everyone, which may contribute to overall happiness. By comparing pre/post differences between the treatment and control groups, we gain more confidence in isolating the effect of the flexible work benefit on happiness since alternative explanationss should be reflected in any pre/post changes observed for the control group.</p>
<p><strong>Difference-in-differences (DiD)</strong> estimation is a quasi-experimental approach that originated from econometrics for this same purpose, but it is beyond the scope of this book. Angrist and Pischke (2009) is an excellent resource for learning about these methods.</p>
<p>Let’s prepare the data for this experiment. Since <code>employees</code> is a cross-sectional dataset (single point-in-time), we will generate simulated data for the post-intervention comparison.</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb278-1" data-line-number="1"><span class="co"># Set seed for reproducible results</span></a>
<a class="sourceLine" id="cb278-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">1234</span>)</a>
<a class="sourceLine" id="cb278-3" data-line-number="3"></a>
<a class="sourceLine" id="cb278-4" data-line-number="4"><span class="co"># Derive happiness index from survey variables</span></a>
<a class="sourceLine" id="cb278-5" data-line-number="5">employees<span class="op">$</span>happiness_ind &lt;-<span class="st"> </span>(employees<span class="op">$</span>engagement <span class="op">+</span><span class="st"> </span>employees<span class="op">$</span>env_sat <span class="op">+</span><span class="st"> </span>employees<span class="op">$</span>job_sat <span class="op">+</span><span class="st"> </span>employees<span class="op">$</span>rel_sat) <span class="op">/</span><span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb278-6" data-line-number="6"></a>
<a class="sourceLine" id="cb278-7" data-line-number="7"><span class="co"># Sample size of frequent travelers</span></a>
<a class="sourceLine" id="cb278-8" data-line-number="8">n =<span class="st"> </span><span class="kw">nrow</span>(<span class="kw">subset</span>(employees, business_travel <span class="op">==</span><span class="st"> &#39;Travel_Frequently&#39;</span>, <span class="dt">select =</span> employee_id))</a>
<a class="sourceLine" id="cb278-9" data-line-number="9"></a>
<a class="sourceLine" id="cb278-10" data-line-number="10"><span class="co"># Randomly assign half of frequent travelers to treatment and control groups</span></a>
<a class="sourceLine" id="cb278-11" data-line-number="11">treat_ids &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">unlist</span>(<span class="kw">subset</span>(employees, business_travel <span class="op">==</span><span class="st"> &#39;Travel_Frequently&#39;</span>, <span class="dt">select =</span> employee_id)), <span class="kw">floor</span>(n <span class="op">*</span><span class="st"> </span><span class="fl">.5</span>))</a>
<a class="sourceLine" id="cb278-12" data-line-number="12">ctrl_ids &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, business_travel <span class="op">==</span><span class="st"> &#39;Travel_Frequently&#39;</span> <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span>employee_id <span class="op">%in%</span><span class="st"> </span>treat_ids, <span class="dt">select =</span> employee_id))</a>
<a class="sourceLine" id="cb278-13" data-line-number="13"></a>
<a class="sourceLine" id="cb278-14" data-line-number="14"><span class="co"># Initialize dfs for pre/post metrics</span></a>
<a class="sourceLine" id="cb278-15" data-line-number="15">treat_metrics =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">pre_ind =</span> <span class="kw">numeric</span>(<span class="kw">length</span>(treat_ids)),</a>
<a class="sourceLine" id="cb278-16" data-line-number="16">                           <span class="dt">rand_num =</span> <span class="kw">rnorm</span>(<span class="kw">length</span>(treat_ids), <span class="dt">mean =</span> <span class="dv">15</span>, <span class="dt">sd =</span> <span class="dv">5</span>) <span class="op">*</span><span class="st"> </span><span class="fl">.001</span>,</a>
<a class="sourceLine" id="cb278-17" data-line-number="17">                           <span class="dt">post_ind =</span> <span class="kw">numeric</span>(<span class="kw">length</span>(treat_ids)), </a>
<a class="sourceLine" id="cb278-18" data-line-number="18">                           <span class="dt">diff =</span> <span class="kw">numeric</span>(<span class="kw">length</span>(treat_ids)))</a>
<a class="sourceLine" id="cb278-19" data-line-number="19"></a>
<a class="sourceLine" id="cb278-20" data-line-number="20">ctrl_metrics =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">pre_ind =</span> <span class="kw">numeric</span>(<span class="kw">length</span>(ctrl_ids)), </a>
<a class="sourceLine" id="cb278-21" data-line-number="21">                          <span class="dt">rand_num =</span> <span class="kw">rnorm</span>(<span class="kw">length</span>(ctrl_ids), <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="fl">.001</span>, </a>
<a class="sourceLine" id="cb278-22" data-line-number="22">                          <span class="dt">post_ind =</span> <span class="kw">numeric</span>(<span class="kw">length</span>(ctrl_ids)),</a>
<a class="sourceLine" id="cb278-23" data-line-number="23">                          <span class="dt">diff =</span> <span class="kw">numeric</span>(<span class="kw">length</span>(ctrl_ids)))</a>
<a class="sourceLine" id="cb278-24" data-line-number="24"></a>
<a class="sourceLine" id="cb278-25" data-line-number="25"><span class="co"># Store happiness indices for treatment and control groups</span></a>
<a class="sourceLine" id="cb278-26" data-line-number="26">treat_metrics<span class="op">$</span>pre_ind &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, employee_id <span class="op">%in%</span><span class="st"> </span>treat_ids, <span class="dt">select =</span> happiness_ind))</a>
<a class="sourceLine" id="cb278-27" data-line-number="27">ctrl_metrics<span class="op">$</span>pre_ind &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, employee_id <span class="op">%in%</span><span class="st"> </span>ctrl_ids, <span class="dt">select =</span> happiness_ind))</a>
<a class="sourceLine" id="cb278-28" data-line-number="28"></a>
<a class="sourceLine" id="cb278-29" data-line-number="29"><span class="co"># Create vectors with artificially inflated post-intervention happiness indices</span></a>
<a class="sourceLine" id="cb278-30" data-line-number="30">treat_metrics<span class="op">$</span>post_ind &lt;-<span class="st"> </span>treat_metrics<span class="op">$</span>pre_ind <span class="op">+</span><span class="st"> </span>treat_metrics<span class="op">$</span>rand_num</a>
<a class="sourceLine" id="cb278-31" data-line-number="31">ctrl_metrics<span class="op">$</span>post_ind &lt;-<span class="st"> </span>ctrl_metrics<span class="op">$</span>pre_ind <span class="op">+</span><span class="st"> </span>ctrl_metrics<span class="op">$</span>rand_num</a>
<a class="sourceLine" id="cb278-32" data-line-number="32"></a>
<a class="sourceLine" id="cb278-33" data-line-number="33"><span class="co"># Force an upper bound of 4 to adjusted index scores (variables were measured using a 4-point Likert scale)</span></a>
<a class="sourceLine" id="cb278-34" data-line-number="34">treat_metrics<span class="op">$</span>post_ind &lt;-<span class="st"> </span><span class="cf">if</span>(treat_metrics<span class="op">$</span>post_ind <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span>) {<span class="dv">4</span>} <span class="cf">else</span> {treat_metrics<span class="op">$</span>post_ind}</a>
<a class="sourceLine" id="cb278-35" data-line-number="35">ctrl_metrics<span class="op">$</span>post_ind &lt;-<span class="st"> </span><span class="cf">if</span>(ctrl_metrics<span class="op">$</span>post_ind <span class="op">&gt;</span><span class="st"> </span><span class="dv">4</span>) {<span class="dv">4</span>} <span class="cf">else</span> {ctrl_metrics<span class="op">$</span>post_ind}</a></code></pre></div>
<p>It’s important to remember that a paired samples <span class="math inline">\(t\)</span>-test requires that each of the paired measurements be obtained from the same subject. Therefore, if an employee terms between time 1 and time 2, or does not provide the survey responses needed to calculate the happiness index at both time 1 and time 2, the employee must be removed from the data since paired measurements would not be available.</p>
<p>Next, we will evaluate whether paired differences are normally distributed using the Shapiro Wilk test. While individual survey items are measured on an ordinal scale, our derived happiness index is the average of multiple ordinal items and can be considered an <em>approximately</em> continuous variable. There are <span class="math inline">\(2^n - n - 1\)</span> combinations of scores, where <span class="math inline">\(n\)</span> is the number of variables (not observations). For our happiness index, there are <span class="math inline">\(2^4 - 4 - 1 = 11\)</span> combinations.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" data-line-number="1"><span class="co"># Load library</span></a>
<a class="sourceLine" id="cb279-2" data-line-number="2"><span class="kw">library</span>(ggpubr)</a>
<a class="sourceLine" id="cb279-3" data-line-number="3"></a>
<a class="sourceLine" id="cb279-4" data-line-number="4"><span class="co"># Calculate pre/post differences</span></a>
<a class="sourceLine" id="cb279-5" data-line-number="5">treat_metrics<span class="op">$</span>diff &lt;-<span class="st"> </span>treat_metrics<span class="op">$</span>post_ind <span class="op">-</span><span class="st"> </span>treat_metrics<span class="op">$</span>pre_ind</a>
<a class="sourceLine" id="cb279-6" data-line-number="6">ctrl_metrics<span class="op">$</span>diff &lt;-<span class="st"> </span>ctrl_metrics<span class="op">$</span>post_ind <span class="op">-</span><span class="st"> </span>ctrl_metrics<span class="op">$</span>pre_ind</a>
<a class="sourceLine" id="cb279-7" data-line-number="7"></a>
<a class="sourceLine" id="cb279-8" data-line-number="8"><span class="co"># Histogram for distribution of pre/post treatment group differences</span></a>
<a class="sourceLine" id="cb279-9" data-line-number="9">p_treat &lt;-<span class="st"> </span>ggplot2<span class="op">::</span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-10" data-line-number="10"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">aes</span>(treat_metrics<span class="op">$</span>diff) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-11" data-line-number="11"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Treatment Group&quot;</span>, <span class="dt">x =</span> <span class="st">&quot;Happiness Index Differences&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Frequency&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-12" data-line-number="12"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb279-13" data-line-number="13"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb279-14" data-line-number="14"><span class="st">           </span>ggplot2<span class="op">::</span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb279-15" data-line-number="15"></a>
<a class="sourceLine" id="cb279-16" data-line-number="16"><span class="co"># Histogram for distribution of pre/post control group differences</span></a>
<a class="sourceLine" id="cb279-17" data-line-number="17">p_ctrl &lt;-<span class="st"> </span>ggplot2<span class="op">::</span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-18" data-line-number="18"><span class="st">          </span>ggplot2<span class="op">::</span><span class="kw">aes</span>(ctrl_metrics<span class="op">$</span>diff) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-19" data-line-number="19"><span class="st">          </span>ggplot2<span class="op">::</span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Control Group&quot;</span>, <span class="dt">x =</span> <span class="st">&quot;Happiness Index Differences&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Frequency&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb279-20" data-line-number="20"><span class="st">          </span>ggplot2<span class="op">::</span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb279-21" data-line-number="21"><span class="st">          </span>ggplot2<span class="op">::</span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb279-22" data-line-number="22"><span class="st">          </span>ggplot2<span class="op">::</span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb279-23" data-line-number="23"></a>
<a class="sourceLine" id="cb279-24" data-line-number="24"><span class="co"># Display histograms side-by-side</span></a>
<a class="sourceLine" id="cb279-25" data-line-number="25">ggpubr<span class="op">::</span><span class="kw">ggarrange</span>(p_treat, p_ctrl, <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> <span class="dv">1</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pre-post-diff"></span>
<img src="People_Analytics_Lifecycle_files/figure-html/pre-post-diff-1.png" alt="Pre/Post Differences for Treatment and Control Groups" width="672" />
<p class="caption">
Figure 10.8: Pre/Post Differences for Treatment and Control Groups
</p>
</div>
<p>Based on a visual inspection, the distributions of differences appear to be roughly normal. This should not be surprising given random values were sampled from normal distributions to derive post-intervention happiness indices.</p>
<p>There are several alternatives to a visual inspection of normality, such as the <span class="math inline">\({\chi}^2\)</span> <strong>Goodness-of-Fit</strong> test (Snedecor &amp; Cochran, 1980), <strong>Kolmogorov-Smirnov (K-S)</strong> test (Chakravarti, Laha, &amp; Roy, 1967), or <strong>Shapiro-Wilk</strong> test (Shapiro &amp; Wilk, 1965). The general idea is consistent for each of these tests: compare observed data to what would be expected if data are sampled from a normally distributed population. The <span class="math inline">\({\chi}^2\)</span> Goodness-of-Fit test compares the <em>count</em> of data points across the range of values relative to what would be expected in each for a sample with the same dimensions taken from a normal distribution. For example, if data are sampled from a normal population distribution, it follows that roughly half the values should exist below the mean and half above the mean. The K-S test evaluates how the observed <em>cumulative</em> distribution compares to the properties of a normal <em>cumulative</em> distribution. The Shapiro-Wilk test is based on <em>correlations</em> between observed and expected data.</p>
<p>We will test for normality using the Shapiro-Wilk test. The null hypothesis for the Shapiro-Wilk test is that the data are normally distributed, so a high <span class="math inline">\(p\)</span>-value indicates that the assumption of normality is satisfied. The <code>shapiro.test()</code> function can be used to run this test in R using a vector of differences:</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb280-1" data-line-number="1"><span class="co"># Compute Shapiro-Wilk test of normality</span></a>
<a class="sourceLine" id="cb280-2" data-line-number="2"><span class="kw">shapiro.test</span>(treat_metrics<span class="op">$</span>diff)</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  treat_metrics$diff
## W = 0.98936, p-value = 0.3738</code></pre>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb282-1" data-line-number="1"><span class="kw">shapiro.test</span>(ctrl_metrics<span class="op">$</span>diff)</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  ctrl_metrics$diff
## W = 0.99096, p-value = 0.5134</code></pre>
<p>Since <span class="math inline">\(p\)</span> &gt; .05 for both tests, the assumption of normally distributed differences is met, as we fail to reject the null hypothesis of normality. Given the data generative process implemented for this example, differences would become increasingly normal as the sample size increases. We have the greenlight to perform the paired samples <span class="math inline">\(t\)</span>-test.</p>
<p>We can run a paired samples <span class="math inline">\(t\)</span>-test in R by passing <code>paired = TRUE</code> as an argument to the same <code>t.test()</code> function used for the independent samples <span class="math inline">\(t\)</span>-test. Since we are investigating whether the average post-intervention happiness index is significantly <em>greater</em> than the average pre-intervention happiness index, we also need the <code>alternative = &quot;greater&quot;</code> argument since the default two-tailed test should only test whether the average indices are significantly different (regardless of whether the pre- or post-intervention index is larger).</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb284-1" data-line-number="1"><span class="co"># Perform one-tailed paired samples t-test for treatment group</span></a>
<a class="sourceLine" id="cb284-2" data-line-number="2"><span class="kw">t.test</span>(treat_metrics<span class="op">$</span>post_ind, treat_metrics<span class="op">$</span>pre_ind, <span class="dt">paired =</span> <span class="ot">TRUE</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Paired t-test
## 
## data:  treat_metrics$post_ind and treat_metrics$pre_ind
## t = 35.906, df = 137, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.01490482        Inf
## sample estimates:
## mean of the differences 
##              0.01562551</code></pre>
<p>These results indicate that the post-intervention happiness index is significantly larger than the pre-intervention happiness index. This is encouraging with respect to the potential efficacy of the flexible work pilot, but the question about whether the control group experienced a commensurate improvement over the observation period remains unanswered.</p>
<p>Let’s run the same paired samples <span class="math inline">\(t\)</span>-test using the control group indices:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb286-1" data-line-number="1"><span class="co"># Perform one-tailed paired samples t-test for control group</span></a>
<a class="sourceLine" id="cb286-2" data-line-number="2"><span class="kw">t.test</span>(ctrl_metrics<span class="op">$</span>post_ind, ctrl_metrics<span class="op">$</span>pre_ind, <span class="dt">paired =</span> <span class="ot">TRUE</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Paired t-test
## 
## data:  ctrl_metrics$post_ind and ctrl_metrics$pre_ind
## t = 0.59995, df = 138, p-value = 0.2748
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -8.719262e-05           Inf
## sample estimates:
## mean of the differences 
##            4.953658e-05</code></pre>
<p>Since <span class="math inline">\(p\)</span> &gt; .05, we can conclude that there was not a significant increase in happiness indices for the control group, which provides additional – but not conclusive – support for the effectiveness of the flexible work benefit.</p>
<p>### Wilcoxon Signed-Rank Test</p>
<p>The <strong>Wilcoxon Signed-Rank Test</strong> is the nonparametric alternative to the paired samples <span class="math inline">\(t\)</span>-test. This distribution-free test does not require normally distributed differences.</p>
<p>The matched Wilcoxon Signed-Rank test is performed in R using the same <code>wilcox.test()</code> function used to perform the unmatched Wilcoxon Rank-Sum test. Though we can use a paired samples <span class="math inline">\(t\)</span>-test to test differences for our flexible work benefit study, let’s run a Wilcoxon Signed-Rank test for demonstrative purposes:</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb288-1" data-line-number="1"><span class="co"># Perform Wilcoxon Signed-Rank test</span></a>
<a class="sourceLine" id="cb288-2" data-line-number="2"><span class="kw">wilcox.test</span>(treat_metrics<span class="op">$</span>post_ind, treat_metrics<span class="op">$</span>pre_ind, <span class="dt">paired =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  treat_metrics$post_ind and treat_metrics$pre_ind
## V = 9591, p-value &lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb290-1" data-line-number="1"><span class="co"># Perform Wilcoxon Signed-Rank test</span></a>
<a class="sourceLine" id="cb290-2" data-line-number="2"><span class="kw">wilcox.test</span>(ctrl_metrics<span class="op">$</span>post_ind, ctrl_metrics<span class="op">$</span>pre_ind, <span class="dt">paired =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  ctrl_metrics$post_ind and ctrl_metrics$pre_ind
## V = 5166, p-value = 0.5275
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Consistent with the results of the paired samples <span class="math inline">\(t\)</span>-tests, significantly higher post-intervention happiness indices were observed for the treatment group but not for the control group.</p>
<p>In addition to testing for statistical significance, it is important to understand the magnitude of any observed differences. We must compare standardized mean differences between groups, as the magnitude of difference is scale-dependent. There are several standardized measures available for quantifying the size of observed differences beyond significance testing.</p>
<p>### Cohen’s <span class="math inline">\(d\)</span></p>
<p><strong>Cohen’s</strong> <span class="math inline">\(\textbf d\)</span> is a standardized measure of the difference between two means. Cohen’s <span class="math inline">\(d\)</span> is defined by:</p>
<p><span class="math display">\[ d = \frac{\bar{x}_1 - \bar{x}_2} {s_p},  \]</span></p>
<p>where <span class="math inline">\(s_p\)</span> represents the pooled standard deviation defined by:</p>
<p><span class="math display">\[ s_p = \sqrt\frac{s^2_1 + s^2_2}{2} \]</span></p>
<p>Cohen’s <span class="math inline">\(d\)</span> can be produced using the <code>cohen.d()</code> function from the <code>effsize</code> package in R. The following thresholds can be referenced as a <em>general</em> rule of thumb for interpreting effect size:</p>
<ul>
<li><strong>Small</strong> = 0.2</li>
<li><strong>Medium</strong> = 0.5</li>
<li><strong>Large</strong> = 0.8</li>
</ul>
<p>### Cliff’s Delta</p>
<p><strong>Cliff’s delta</strong> provides the effect size for ordinal variables. Simply put, Cliff’s delta measures how often a value in one distribution is higher than values in another, and this is appropriate in situations in which a nonparametric test of differences is used. This statistic can be produced using the <code>cliff.delta()</code> function from the <code>effsize</code> package in R.</p>
<p>Some (e.g., Vargha &amp; Delaney, 2000) have endeavored to categorize the Cliff’s delta statistic, which ranges from -1 to 1, into effect size buckets. However, such categorizations are far more controversial than thresholds attributed to Cohen’s d.</p>
<p>### Analysis of Variance (ANOVA)</p>
<p><strong>Analysis of Variance (ANOVA)</strong> is used to determine whether the means of scale-level DVs are equal across nominal-level variables with three or more independent categories.</p>
<p>It is important to understand that <span class="math inline">\(H_0\)</span> in ANOVA not only requires all group means to be equal but their complex contrasts as well. For example, if we have four groups named A, B, C, and D, <span class="math inline">\(H_0\)</span> requires that <span class="math inline">\(\mu_A = \mu_B = \mu_C = \mu_D\)</span> is true as well as the various complex contrasts such as <span class="math inline">\(\mu_{A,B} = \mu_{C,D}\)</span> and <span class="math inline">\(\mu_A = \mu_{B,C,D}\)</span> and <span class="math inline">\(\mu_D = \mu_{B,C}\)</span>. Therefore, in order to reject <span class="math inline">\(H_0\)</span> in ANOVA, at least one of the possible contrasts must be different. As a result, we may find a significant <span class="math inline">\(F\)</span>-statistic but no significant differences between pairwise means.</p>
<p>In addition, it is possible to find a significant pairwise mean difference but a non-significant result from ANOVA. As you may recall from Chapter <a href="inf-stats.html#inf-stats">8</a>, multiple comparisons reduce the power of statistical tests. Since multiple tests of mean differences are performed with ANOVA, the family-wise error rate is used to adjust for the increased probability of a Type I error across the set of analyses. Since the power of a single pairwise test is greater relative to the power of familywise comparisons, we may find a significant result for the former but not the latter.</p>
<p>ANOVA requires IVs to be categorical (nominal or ordinal) and the DV to be continuous (interval or ratio). A <strong>one-way ANOVA</strong> is used to determine how one categorical IV influences a continuous DV. A <strong>two-way ANOVA</strong> is used to determine how two categorical IVs influence a continuous DV, while a <strong>three-way ANOVA</strong> is used to evaluate how three categorical IVs influence a continuous DV. An ANOVA that uses two or more categorical IVs is often referred to as a <strong>factorial ANOVA</strong>. As discussed in Chapter <a href="getting-started.html#getting-started">3</a>, it is important to remain grounded in specific hypotheses, as a significant ANOVA may not actually test what is being hypothesized.</p>
<p>ANOVA is not a test, per se, but a <span class="math inline">\(F\)</span>-test underpins it. The mathematical procedure behind the <span class="math inline">\(F\)</span>-test is relatively straightforward:</p>
<ol style="list-style-type: decimal">
<li>Compute the <strong>within-group variance</strong>, which is also known as residual variance. Simply put, this tells us how different each member of the group is from the average.</li>
<li>Compute the <strong>between-group variance</strong>. This represents how different the group means are from one another.</li>
<li>Produce the <span class="math inline">\(F\)</span>-statistic, which is the <em>within-group variance</em> / <em>between-group variance</em>.</li>
</ol>
<p><strong>One-Way ANOVA</strong></p>
<p>To illustrate how to perform a one-way ANOVA, we will test the hypothesis that mean annual compensation is equal across job satisfaction levels.</p>
<p>Each observation in <code>employees</code> represents a unique employee, and a given employee can only have one job satisfaction score and one annual compensation value. The assumption of independence is met since each record exists independent of one another and each job satisfaction group is comprised of different employees.</p>
<p>Levene’s test (Levene, 1960) can be used to test the homogeneity of variance assumption. This can be performed in R using the <code>leveneTest()</code> function from the <code>car</code> package:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb292-1" data-line-number="1"><span class="co"># Load library for Levene&#39;s test</span></a>
<a class="sourceLine" id="cb292-2" data-line-number="2"><span class="kw">library</span>(car)</a>
<a class="sourceLine" id="cb292-3" data-line-number="3"></a>
<a class="sourceLine" id="cb292-4" data-line-number="4"><span class="co"># Perform Levene&#39;s test for homogeneity of variance</span></a>
<a class="sourceLine" id="cb292-5" data-line-number="5">car<span class="op">::</span><span class="kw">leveneTest</span>(annual_comp <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(job_sat), <span class="dt">data =</span> employees)</a></code></pre></div>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##         Df F value Pr(&gt;F)
## group    3  0.3293 0.8042
##       1466</code></pre>
<p>The test statistic associated with Levene’s test relates to the null hypothesis that there are no significant differences in variances across the job satisfaction levels. Since <span class="math inline">\(p &gt; .05\)</span>, we fail to reject this null hypothesis and can assume equal variances.</p>
<p>Next, let’s test the assumption of normality. It is important to note that the assumption of normality <em>does not</em> apply to the distribution of the DV but to the distribution of residuals for each group of the IV. Residuals in the context of ANOVA represent the difference between the actual values of the continuous DV relative to its mean value for each level of the categorical IV (e.g., <span class="math inline">\(y - \bar{y}_A\)</span>, <span class="math inline">\(y - \bar{y}_B\)</span>, <span class="math inline">\(y - \bar{y}_C\)</span>). In ANOVA, we expect the residuals to be normally distributed around a mean of 0 (the balance point) when the data are normally distributed within each IV category; the more skewed the data, the larger the average distance of each DV value from the mean.</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb294-1" data-line-number="1"><span class="co"># Load data viz library</span></a>
<a class="sourceLine" id="cb294-2" data-line-number="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb294-3" data-line-number="3"><span class="kw">library</span>(ggpubr)</a>
<a class="sourceLine" id="cb294-4" data-line-number="4"></a>
<a class="sourceLine" id="cb294-5" data-line-number="5"><span class="co"># Create function to visualize distribution</span></a>
<a class="sourceLine" id="cb294-6" data-line-number="6">dist.viz &lt;-<span class="st"> </span><span class="cf">function</span>(data, x) {</a>
<a class="sourceLine" id="cb294-7" data-line-number="7">  </a>
<a class="sourceLine" id="cb294-8" data-line-number="8">viz &lt;-<span class="st"> </span>ggplot2<span class="op">::</span><span class="kw">ggplot</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb294-9" data-line-number="9"><span class="st">       </span>ggplot2<span class="op">::</span><span class="kw">aes</span>(data) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb294-10" data-line-number="10"><span class="st">       </span>ggplot2<span class="op">::</span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="kw">paste</span>(<span class="st">&quot;Job Sat = &quot;</span>, x), <span class="dt">x =</span> <span class="st">&quot;Annual Compensation&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;Frequency&quot;</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb294-11" data-line-number="11"><span class="st">       </span>ggplot2<span class="op">::</span><span class="kw">geom_histogram</span>(<span class="dt">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb294-12" data-line-number="12"><span class="st">       </span>ggplot2<span class="op">::</span><span class="kw">theme_bw</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb294-13" data-line-number="13"><span class="st">       </span>ggplot2<span class="op">::</span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">hjust =</span> <span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb294-14" data-line-number="14"></a>
<a class="sourceLine" id="cb294-15" data-line-number="15">  <span class="kw">return</span>(viz)</a>
<a class="sourceLine" id="cb294-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb294-17" data-line-number="17"></a>
<a class="sourceLine" id="cb294-18" data-line-number="18"><span class="co"># Produce annual compensation vectors for each job satisfaction level</span></a>
<a class="sourceLine" id="cb294-19" data-line-number="19"><span class="co"># Unlist() is needed to convert the default object from subset() into a numeric vector</span></a>
<a class="sourceLine" id="cb294-20" data-line-number="20">group_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, job_sat <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">select =</span> annual_comp))</a>
<a class="sourceLine" id="cb294-21" data-line-number="21">group_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, job_sat <span class="op">==</span><span class="st"> </span><span class="dv">2</span>, <span class="dt">select =</span> annual_comp))</a>
<a class="sourceLine" id="cb294-22" data-line-number="22">group_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, job_sat <span class="op">==</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">select =</span> annual_comp))</a>
<a class="sourceLine" id="cb294-23" data-line-number="23">group_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">subset</span>(employees, job_sat <span class="op">==</span><span class="st"> </span><span class="dv">4</span>, <span class="dt">select =</span> annual_comp))</a>
<a class="sourceLine" id="cb294-24" data-line-number="24"></a>
<a class="sourceLine" id="cb294-25" data-line-number="25"><span class="co"># Call UDF to build annual comp histogram for each job satisfaction level</span></a>
<a class="sourceLine" id="cb294-26" data-line-number="26">viz_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> group_<span class="dv">1</span>, <span class="dt">x =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb294-27" data-line-number="27">viz_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> group_<span class="dv">2</span>, <span class="dt">x =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb294-28" data-line-number="28">viz_<span class="dv">3</span> &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> group_<span class="dv">3</span>, <span class="dt">x =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb294-29" data-line-number="29">viz_<span class="dv">4</span> &lt;-<span class="st"> </span><span class="kw">dist.viz</span>(<span class="dt">data =</span> group_<span class="dv">4</span>, <span class="dt">x =</span> <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb294-30" data-line-number="30"></a>
<a class="sourceLine" id="cb294-31" data-line-number="31"><span class="co"># Display distribution visualizations</span></a>
<a class="sourceLine" id="cb294-32" data-line-number="32">ggpubr<span class="op">::</span><span class="kw">ggarrange</span>(viz_<span class="dv">1</span>, viz_<span class="dv">2</span>, viz_<span class="dv">3</span>, viz_<span class="dv">4</span>,</a>
<a class="sourceLine" id="cb294-33" data-line-number="33">          <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">nrow =</span> <span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:comp-dist"></span>
<img src="People_Analytics_Lifecycle_files/figure-html/comp-dist-1.png" alt="Annual Compensation Distribution by Job Satisfaction Level" width="672" />
<p class="caption">
Figure 10.9: Annual Compensation Distribution by Job Satisfaction Level
</p>
</div>
<p>As we can see, annual compensation is not normally distributed within job satisfaction groups. Therefore, we would not expect the distribution of residuals to be normally distributed within these groups either.</p>
<p>To test whether the assumption of normality is met, we will first produce and review a <strong>quantile-quantile (Q-Q) plot</strong>. A Q-Q plot compares two probability distributions by plotting their quantiles (data partitioned into equal-sized groups) against each other. After partitioning annual compensation into groups differentiated by job satisfaction level, we can use the <code>ggqqplot()</code> function from the <code>ggpubr</code> library to build a Q-Q plot and evaluate the distribution of residuals.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" data-line-number="1"><span class="co"># Generate residuals for each group</span></a>
<a class="sourceLine" id="cb295-2" data-line-number="2">residuals &lt;-<span class="st"> </span><span class="kw">c</span>(group_<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(group_<span class="dv">1</span>), group_<span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(group_<span class="dv">2</span>), group_<span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(group_<span class="dv">3</span>), group_<span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(group_<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb295-3" data-line-number="3"></a>
<a class="sourceLine" id="cb295-4" data-line-number="4"><span class="co"># Create a Q-Q plot of residuals</span></a>
<a class="sourceLine" id="cb295-5" data-line-number="5">ggpubr<span class="op">::</span><span class="kw">ggqqplot</span>(residuals)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qq-plot"></span>
<img src="People_Analytics_Lifecycle_files/figure-html/qq-plot-1.png" alt="Q-Q Plot of Annual Compensation Residuals" width="672" />
<p class="caption">
Figure 10.10: Q-Q Plot of Annual Compensation Residuals
</p>
</div>
<p>To satisfy the assumption of normality, residuals must lie along the linear line. Based on the Q-Q plot in Figure <a href="aod.html#fig:qq-plot">10.10</a>, there is a clear departure from normality at both ends of the theoretical range.</p>
<p>Let’s test for normality using the Shapiro-Wilk test:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb296-1" data-line-number="1"><span class="co"># Compute Shapiro-Wilk test of normality</span></a>
<a class="sourceLine" id="cb296-2" data-line-number="2"><span class="kw">shapiro.test</span>(residuals)</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals
## W = 0.95874, p-value &lt; 2.2e-16</code></pre>
<p>Since <span class="math inline">\(p &lt; .05\)</span>, we reject the null hypothesis of normally distributed data, which indicates that the assumption of normality is violated. This should not be surprising based on the deviation from normality we observed in Figure <a href="aod.html#fig:qq-plot">10.10</a>.</p>
<p>Because the assumption of normality is violated, we have two options. First, we can attempt to transform the data so that the residuals using the transformed values are normally distributed. If the data are resistant to transformation, we can leverage a nonparametric alternative to ANOVA.</p>
<p>Let’s first try several common data transformations and then examine the resulting Q-Q plots:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb298-1" data-line-number="1"><span class="co"># Build a linear model using the natural logarithm of annual comp</span></a>
<a class="sourceLine" id="cb298-2" data-line-number="2">ln.model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(annual_comp) <span class="op">~</span><span class="st"> </span>job_sat, <span class="dt">data =</span> employees)</a>
<a class="sourceLine" id="cb298-3" data-line-number="3"></a>
<a class="sourceLine" id="cb298-4" data-line-number="4"><span class="co"># Build a linear model using the log base 10 of annual comp</span></a>
<a class="sourceLine" id="cb298-5" data-line-number="5">log10.model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log10</span>(annual_comp) <span class="op">~</span><span class="st"> </span>job_sat, <span class="dt">data =</span> employees)</a>
<a class="sourceLine" id="cb298-6" data-line-number="6"></a>
<a class="sourceLine" id="cb298-7" data-line-number="7"><span class="co"># Build a linear model using the square root of annual comp</span></a>
<a class="sourceLine" id="cb298-8" data-line-number="8">sqrt.model &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">sqrt</span>(annual_comp) <span class="op">~</span><span class="st"> </span>job_sat, <span class="dt">data =</span> employees)</a>
<a class="sourceLine" id="cb298-9" data-line-number="9"></a>
<a class="sourceLine" id="cb298-10" data-line-number="10"><span class="co"># Store Q-Q plots to viz objects</span></a>
<a class="sourceLine" id="cb298-11" data-line-number="11">ln.viz &lt;-<span class="st"> </span>ggpubr<span class="op">::</span><span class="kw">ggqqplot</span>(<span class="kw">residuals</span>(ln.model)) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Natural Log&quot;</span>)</a>
<a class="sourceLine" id="cb298-12" data-line-number="12">log10.viz &lt;-<span class="st"> </span>ggpubr<span class="op">::</span><span class="kw">ggqqplot</span>(<span class="kw">residuals</span>(log10.model)) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Log Base 10&quot;</span>)</a>
<a class="sourceLine" id="cb298-13" data-line-number="13">sqrt.viz &lt;-<span class="st"> </span>ggpubr<span class="op">::</span><span class="kw">ggqqplot</span>(<span class="kw">residuals</span>(sqrt.model)) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="st">&quot;Square Root&quot;</span>)</a>
<a class="sourceLine" id="cb298-14" data-line-number="14"></a>
<a class="sourceLine" id="cb298-15" data-line-number="15"><span class="co"># Display Q-Q plots of residuals</span></a>
<a class="sourceLine" id="cb298-16" data-line-number="16">ggpubr<span class="op">::</span><span class="kw">ggarrange</span>(ln.viz, log10.viz, sqrt.viz,</a>
<a class="sourceLine" id="cb298-17" data-line-number="17">          <span class="dt">ncol =</span> <span class="dv">3</span>, <span class="dt">nrow =</span> <span class="dv">1</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qq-plots-trans"></span>
<img src="People_Analytics_Lifecycle_files/figure-html/qq-plots-trans-1.png" alt="Q-Q Plots of Transformed Annual Compensation Residuals" width="672" />
<p class="caption">
Figure 10.11: Q-Q Plots of Transformed Annual Compensation Residuals
</p>
</div>
<p>Even with these transformations, there is still a clear S-shaped curve about the residuals.</p>
<p>The <strong>Kruskal Wallis H Test</strong> is the nonparametric alternative to a one-way ANOVA (Daniel, 1990). This test can be performed using the <code>kruskal.test()</code> function in R:</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb299-1" data-line-number="1"><span class="co"># Nonparametric Kruskal one-way ANOVA investigating median differences in annual comp by job satisfaction</span></a>
<a class="sourceLine" id="cb299-2" data-line-number="2"><span class="kw">kruskal.test</span>(annual_comp <span class="op">~</span><span class="st"> </span>job_sat, <span class="dt">data =</span> employees)</a></code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  annual_comp by job_sat
## Kruskal-Wallis chi-squared = 8.3242, df = 3, p-value = 0.03977</code></pre>
<p>Since <span class="math inline">\(p &lt; .05\)</span>, we can conclude that there are significant differences in median compensation across the groups. However, this test does not indicate which groups are different.</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb301-1" data-line-number="1"><span class="kw">pairwise.wilcox.test</span>(employees<span class="op">$</span>annual_comp, employees<span class="op">$</span>job_sat, <span class="dt">p.adjust.method =</span> <span class="st">&quot;BH&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using Wilcoxon rank sum test with continuity correction 
## 
## data:  employees$annual_comp and employees$job_sat 
## 
##   1     2     3    
## 2 0.298 -     -    
## 3 0.041 0.298 -    
## 4 0.041 0.298 0.879
## 
## P value adjustment method: BH</code></pre>
<p>ANOVA can be performed using the <code>aov()</code> function, followed by the <code>summary()</code> function to display model output:</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" data-line-number="1"><span class="co"># One-way ANOVA investigating mean differences in annual comp by job satisfaction</span></a>
<a class="sourceLine" id="cb303-2" data-line-number="2">one.way &lt;-<span class="st"> </span><span class="kw">aov</span>(annual_comp <span class="op">~</span><span class="st"> </span>job_sat, <span class="dt">data =</span> employees)</a>
<a class="sourceLine" id="cb303-3" data-line-number="3"><span class="kw">summary</span>(one.way)</a></code></pre></div>
<pre><code>##               Df    Sum Sq   Mean Sq F value  Pr(&gt;F)   
## job_sat        1 1.337e+10 1.337e+10   7.508 0.00622 **
## Residuals   1468 2.613e+12 1.780e+09                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><strong>Factorial ANOVA</strong></p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb305-1" data-line-number="1"><span class="co"># Factorial ANOVA investigating mean differences in annual comp by job satisfaction and dept</span></a>
<a class="sourceLine" id="cb305-2" data-line-number="2">factorial &lt;-<span class="st"> </span><span class="kw">aov</span>(annual_comp <span class="op">~</span><span class="st"> </span>job_sat <span class="op">+</span><span class="st"> </span>dept, <span class="dt">data =</span> employees)</a>
<a class="sourceLine" id="cb305-3" data-line-number="3"><span class="kw">summary</span>(factorial)</a></code></pre></div>
<pre><code>##               Df    Sum Sq   Mean Sq F value  Pr(&gt;F)   
## job_sat        1 1.337e+10 1.337e+10   7.502 0.00624 **
## dept           2 1.375e+09 6.874e+08   0.386 0.67995   
## Residuals   1466 2.612e+12 1.782e+09                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb307-1" data-line-number="1"><span class="co"># ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept</span></a>
<a class="sourceLine" id="cb307-2" data-line-number="2">interaction &lt;-<span class="st"> </span><span class="kw">aov</span>(annual_comp <span class="op">~</span><span class="st"> </span>job_sat <span class="op">*</span><span class="st"> </span>dept, <span class="dt">data =</span> employees)</a>
<a class="sourceLine" id="cb307-3" data-line-number="3"><span class="kw">summary</span>(interaction)</a></code></pre></div>
<pre><code>##                Df    Sum Sq   Mean Sq F value Pr(&gt;F)   
## job_sat         1 1.337e+10 1.337e+10   7.514 0.0062 **
## dept            2 1.375e+09 6.874e+08   0.386 0.6795   
## job_sat:dept    2 7.762e+09 3.881e+09   2.182 0.1132   
## Residuals    1464 2.604e+12 1.779e+09                  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<div id="post-hoc-tests" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Post-Hoc Tests</h3>
<p><strong>Tukey’s Honest Significant Difference (HSD)</strong> test</p>
<p><strong>Scheffe</strong></p>
<p><strong>Bonferroni</strong></p>
</div>
</div>
<div id="exercises-6" class="section level2">
<h2><span class="header-section-number">10.4</span> Exercises</h2>
<ol style="list-style-type: decimal">
<li></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-wrang-prep.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["People_Analytics_Lifecycle.pdf", "People_Analytics_Lifecycle.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
