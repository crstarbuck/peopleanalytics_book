<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9 Analysis of Differences | An Introduction to People Analytics: With Applications in R</title>
  <meta name="description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="9 Analysis of Differences | An Introduction to People Analytics: With Applications in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="github-repo" content="crstarbuck/peopleanalytics-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9 Analysis of Differences | An Introduction to People Analytics: With Applications in R" />
  
  <meta name="twitter:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  

<meta name="author" content="Craig Starbuck" />


<meta name="date" content="2022-08-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inf-stats.html"/>
<link rel="next" href="lm.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<link href="libs/tabwid-1.0.0/tabwid.css" rel="stylesheet" />
<link href="libs/tabwid-1.0.0/scrool.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An Introduction to People Analytics: With Applications in R</a></li>

<li class="divider"></li>
<li><a href="dedication.html#dedication" id="toc-dedication">Dedication</a></li>
<li><a href="foreword.html#foreword" id="toc-foreword">Foreword</a></li>
<li><a href="preface.html#preface" id="toc-preface">Preface</a></li>
<li><a href="getting-started.html#getting-started" id="toc-getting-started"><span class="toc-section-number">1</span> Getting Started</a>
<ul>
<li><a href="getting-started.html#guiding-principles" id="toc-guiding-principles"><span class="toc-section-number">1.1</span> Guiding Principles</a>
<ul>
<li><a href="getting-started.html#pro-employee-thinking" id="toc-pro-employee-thinking"><span class="toc-section-number">1.1.1</span> Pro Employee Thinking</a></li>
<li><a href="getting-started.html#quality" id="toc-quality"><span class="toc-section-number">1.1.2</span> Quality</a></li>
<li><a href="getting-started.html#prioritization" id="toc-prioritization"><span class="toc-section-number">1.1.3</span> Prioritization</a></li>
</ul></li>
<li><a href="getting-started.html#tooling" id="toc-tooling"><span class="toc-section-number">1.2</span> Tooling</a></li>
<li><a href="getting-started.html#data" id="toc-data"><span class="toc-section-number">1.3</span> Data</a>
<ul>
<li><a href="getting-started.html#employees" id="toc-employees"><span class="toc-section-number">1.3.1</span> Employees</a></li>
<li><a href="getting-started.html#turnover-trends" id="toc-turnover-trends"><span class="toc-section-number">1.3.2</span> Turnover Trends</a></li>
<li><a href="getting-started.html#survey-responses" id="toc-survey-responses"><span class="toc-section-number">1.3.3</span> Survey Responses</a></li>
</ul></li>
<li><a href="getting-started.html#d-framework" id="toc-d-framework"><span class="toc-section-number">1.4</span> 4D Framework</a>
<ul>
<li><a href="getting-started.html#discover" id="toc-discover"><span class="toc-section-number">1.4.1</span> Discover</a></li>
<li><a href="getting-started.html#design" id="toc-design"><span class="toc-section-number">1.4.2</span> Design</a></li>
<li><a href="getting-started.html#develop" id="toc-develop"><span class="toc-section-number">1.4.3</span> Develop</a></li>
<li><a href="getting-started.html#deliver" id="toc-deliver"><span class="toc-section-number">1.4.4</span> Deliver</a></li>
</ul></li>
</ul></li>
<li><a href="r-intro.html#r-intro" id="toc-r-intro"><span class="toc-section-number">2</span> Introduction to R</a>
<ul>
<li><a href="r-intro.html#getting-started-1" id="toc-getting-started-1"><span class="toc-section-number">2.1</span> Getting Started</a></li>
<li><a href="r-intro.html#vectors" id="toc-vectors"><span class="toc-section-number">2.2</span> Vectors</a></li>
<li><a href="r-intro.html#matrices" id="toc-matrices"><span class="toc-section-number">2.3</span> Matrices</a></li>
<li><a href="r-intro.html#factors" id="toc-factors"><span class="toc-section-number">2.4</span> Factors</a></li>
<li><a href="r-intro.html#data-frames" id="toc-data-frames"><span class="toc-section-number">2.5</span> Data Frames</a></li>
<li><a href="r-intro.html#lists" id="toc-lists"><span class="toc-section-number">2.6</span> Lists</a></li>
<li><a href="r-intro.html#loops" id="toc-loops"><span class="toc-section-number">2.7</span> Loops</a></li>
<li><a href="r-intro.html#user-defined-functions-udfs" id="toc-user-defined-functions-udfs"><span class="toc-section-number">2.8</span> User-Defined Functions (UDFs)</a></li>
<li><a href="r-intro.html#graphics" id="toc-graphics"><span class="toc-section-number">2.9</span> Graphics</a></li>
<li><a href="r-intro.html#review-questions" id="toc-review-questions"><span class="toc-section-number">2.10</span> Review Questions</a></li>
</ul></li>
<li><a href="sql-intro.html#sql-intro" id="toc-sql-intro"><span class="toc-section-number">3</span> Introduction to SQL</a>
<ul>
<li><a href="sql-intro.html#basics" id="toc-basics"><span class="toc-section-number">3.1</span> Basics</a></li>
<li><a href="sql-intro.html#aggregate-functions" id="toc-aggregate-functions"><span class="toc-section-number">3.2</span> Aggregate Functions</a></li>
<li><a href="sql-intro.html#joins" id="toc-joins"><span class="toc-section-number">3.3</span> Joins</a></li>
<li><a href="sql-intro.html#subqueries" id="toc-subqueries"><span class="toc-section-number">3.4</span> Subqueries</a></li>
<li><a href="sql-intro.html#virtual-tables" id="toc-virtual-tables"><span class="toc-section-number">3.5</span> Virtual Tables</a></li>
<li><a href="sql-intro.html#window-functions" id="toc-window-functions"><span class="toc-section-number">3.6</span> Window Functions</a></li>
<li><a href="sql-intro.html#common-table-expressions-ctes" id="toc-common-table-expressions-ctes"><span class="toc-section-number">3.7</span> Common Table Expressions (CTEs)</a></li>
<li><a href="sql-intro.html#review-questions-1" id="toc-review-questions-1"><span class="toc-section-number">3.8</span> Review Questions</a></li>
</ul></li>
<li><a href="measure-sampl.html#measure-sampl" id="toc-measure-sampl"><span class="toc-section-number">4</span> Measurement &amp; Sampling</a>
<ul>
<li><a href="measure-sampl.html#variable-types" id="toc-variable-types"><span class="toc-section-number">4.1</span> Variable Types</a>
<ul>
<li><a href="measure-sampl.html#independent-variables-iv" id="toc-independent-variables-iv"><span class="toc-section-number">4.1.1</span> Independent Variables (IV)</a></li>
<li><a href="measure-sampl.html#dependent-variables-dv" id="toc-dependent-variables-dv"><span class="toc-section-number">4.1.2</span> Dependent Variables (DV)</a></li>
<li><a href="measure-sampl.html#control-variables-cv" id="toc-control-variables-cv"><span class="toc-section-number">4.1.3</span> Control Variables (CV)</a></li>
<li><a href="measure-sampl.html#moderating-variables" id="toc-moderating-variables"><span class="toc-section-number">4.1.4</span> Moderating Variables</a></li>
<li><a href="measure-sampl.html#mediating-variables" id="toc-mediating-variables"><span class="toc-section-number">4.1.5</span> Mediating Variables</a></li>
<li><a href="measure-sampl.html#endogenous-vs.-exogenous-variables" id="toc-endogenous-vs.-exogenous-variables"><span class="toc-section-number">4.1.6</span> Endogenous vs. Exogenous Variables</a></li>
</ul></li>
<li><a href="measure-sampl.html#measurement-scales" id="toc-measurement-scales"><span class="toc-section-number">4.2</span> Measurement Scales</a>
<ul>
<li><a href="measure-sampl.html#discrete-variables" id="toc-discrete-variables"><span class="toc-section-number">4.2.1</span> Discrete Variables</a></li>
<li><a href="measure-sampl.html#continuous-variables" id="toc-continuous-variables"><span class="toc-section-number">4.2.2</span> Continuous Variables</a></li>
</ul></li>
<li><a href="measure-sampl.html#sampling" id="toc-sampling"><span class="toc-section-number">4.3</span> Sampling</a>
<ul>
<li><a href="measure-sampl.html#sampling-nonsampling-error" id="toc-sampling-nonsampling-error"><span class="toc-section-number">4.3.1</span> Sampling &amp; Nonsampling Error</a></li>
</ul></li>
<li><a href="measure-sampl.html#review-questions-2" id="toc-review-questions-2"><span class="toc-section-number">4.4</span> Review Questions</a></li>
</ul></li>
<li><a href="research.html#research" id="toc-research"><span class="toc-section-number">5</span> Research Fundamentals</a>
<ul>
<li><a href="research.html#research-questions" id="toc-research-questions"><span class="toc-section-number">5.1</span> Research Questions</a></li>
<li><a href="research.html#research-hypotheses" id="toc-research-hypotheses"><span class="toc-section-number">5.2</span> Research Hypotheses</a></li>
<li><a href="research.html#internal-vs.-external-validity" id="toc-internal-vs.-external-validity"><span class="toc-section-number">5.3</span> Internal vs. External Validity</a></li>
<li><a href="research.html#research-methods" id="toc-research-methods"><span class="toc-section-number">5.4</span> Research Methods</a></li>
<li><a href="research.html#research-designs" id="toc-research-designs"><span class="toc-section-number">5.5</span> Research Designs</a></li>
<li><a href="research.html#review-questions-3" id="toc-review-questions-3"><span class="toc-section-number">5.6</span> Review Questions</a></li>
</ul></li>
<li><a href="data-mgmt-prep.html#data-mgmt-prep" id="toc-data-mgmt-prep"><span class="toc-section-number">6</span> Data Management &amp; Preparation</a>
<ul>
<li><a href="data-mgmt-prep.html#data-management" id="toc-data-management"><span class="toc-section-number">6.1</span> Data Management</a></li>
<li><a href="data-mgmt-prep.html#data-screening-cleaning" id="toc-data-screening-cleaning"><span class="toc-section-number">6.2</span> Data Screening &amp; Cleaning</a></li>
<li><a href="data-mgmt-prep.html#one-hot-encoding" id="toc-one-hot-encoding"><span class="toc-section-number">6.3</span> One-Hot Encoding</a></li>
<li><a href="data-mgmt-prep.html#feature-engineering" id="toc-feature-engineering"><span class="toc-section-number">6.4</span> Feature Engineering</a></li>
<li><a href="data-mgmt-prep.html#review-questions-4" id="toc-review-questions-4"><span class="toc-section-number">6.5</span> Review Questions</a></li>
</ul></li>
<li><a href="desc-stats.html#desc-stats" id="toc-desc-stats"><span class="toc-section-number">7</span> Descriptive Statistics</a>
<ul>
<li><a href="desc-stats.html#univariate-analysis" id="toc-univariate-analysis"><span class="toc-section-number">7.1</span> Univariate Analysis</a>
<ul>
<li><a href="desc-stats.html#measures-of-central-tendency" id="toc-measures-of-central-tendency"><span class="toc-section-number">7.1.1</span> Measures of Central Tendency</a></li>
<li><a href="desc-stats.html#measures-of-spread" id="toc-measures-of-spread"><span class="toc-section-number">7.1.2</span> Measures of Spread</a></li>
</ul></li>
<li><a href="desc-stats.html#bivariate-analysis" id="toc-bivariate-analysis"><span class="toc-section-number">7.2</span> Bivariate Analysis</a>
<ul>
<li><a href="desc-stats.html#covariance" id="toc-covariance"><span class="toc-section-number">7.2.1</span> Covariance</a></li>
<li><a href="desc-stats.html#correlation" id="toc-correlation"><span class="toc-section-number">7.2.2</span> Correlation</a></li>
</ul></li>
<li><a href="desc-stats.html#review-questions-5" id="toc-review-questions-5"><span class="toc-section-number">7.3</span> Review Questions</a></li>
</ul></li>
<li><a href="inf-stats.html#inf-stats" id="toc-inf-stats"><span class="toc-section-number">8</span> Statistical Inference</a>
<ul>
<li><a href="inf-stats.html#introduction-to-probability" id="toc-introduction-to-probability"><span class="toc-section-number">8.1</span> Introduction to Probability</a>
<ul>
<li><a href="inf-stats.html#probability-distributions" id="toc-probability-distributions"><span class="toc-section-number">8.1.1</span> Probability Distributions</a></li>
<li><a href="inf-stats.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">8.1.2</span> Conditional Probability</a></li>
</ul></li>
<li><a href="inf-stats.html#central-limit-theorem" id="toc-central-limit-theorem"><span class="toc-section-number">8.2</span> Central Limit Theorem</a></li>
<li><a href="inf-stats.html#confidence-intervals" id="toc-confidence-intervals"><span class="toc-section-number">8.3</span> Confidence Intervals</a>
<ul>
<li><a href="inf-stats.html#hypothesis-testing" id="toc-hypothesis-testing"><span class="toc-section-number">8.3.1</span> Hypothesis Testing</a></li>
<li><a href="inf-stats.html#alpha" id="toc-alpha"><span class="toc-section-number">8.3.2</span> Alpha</a></li>
<li><a href="inf-stats.html#type-i-ii-errors" id="toc-type-i-ii-errors"><span class="toc-section-number">8.3.3</span> Type I &amp; II Errors</a></li>
<li><a href="inf-stats.html#p-values" id="toc-p-values"><span class="toc-section-number">8.3.4</span> <span class="math inline">\(p\)</span>-Values</a></li>
<li><a href="inf-stats.html#bonferroni-correction" id="toc-bonferroni-correction"><span class="toc-section-number">8.3.5</span> Bonferroni Correction</a></li>
<li><a href="inf-stats.html#statistical-power" id="toc-statistical-power"><span class="toc-section-number">8.3.6</span> Statistical Power</a></li>
</ul></li>
<li><a href="inf-stats.html#review-questions-6" id="toc-review-questions-6"><span class="toc-section-number">8.4</span> Review Questions</a></li>
</ul></li>
<li><a href="aod.html#aod" id="toc-aod"><span class="toc-section-number">9</span> Analysis of Differences</a>
<ul>
<li><a href="aod.html#parametric-vs.-nonparametric-tests" id="toc-parametric-vs.-nonparametric-tests"><span class="toc-section-number">9.1</span> Parametric vs. Nonparametric Tests</a></li>
<li><a href="aod.html#differences-in-discrete-data" id="toc-differences-in-discrete-data"><span class="toc-section-number">9.2</span> Differences in Discrete Data</a></li>
<li><a href="aod.html#differences-in-continuous-data" id="toc-differences-in-continuous-data"><span class="toc-section-number">9.3</span> Differences in Continuous Data</a></li>
<li><a href="aod.html#review-questions-7" id="toc-review-questions-7"><span class="toc-section-number">9.4</span> Review Questions</a></li>
</ul></li>
<li><a href="lm.html#lm" id="toc-lm"><span class="toc-section-number">10</span> Linear Regression</a>
<ul>
<li><a href="lm.html#sample-size" id="toc-sample-size"><span class="toc-section-number">10.1</span> Sample Size</a></li>
<li><a href="lm.html#simple-linear-regression" id="toc-simple-linear-regression"><span class="toc-section-number">10.2</span> Simple Linear Regression</a></li>
<li><a href="lm.html#multiple-linear-regression" id="toc-multiple-linear-regression"><span class="toc-section-number">10.3</span> Multiple Linear Regression</a></li>
<li><a href="lm.html#moderation" id="toc-moderation"><span class="toc-section-number">10.4</span> Moderation</a></li>
<li><a href="lm.html#mediation" id="toc-mediation"><span class="toc-section-number">10.5</span> Mediation</a></li>
<li><a href="lm.html#review-questions-8" id="toc-review-questions-8"><span class="toc-section-number">10.6</span> Review Questions</a></li>
</ul></li>
<li><a href="lmv.html#lmv" id="toc-lmv"><span class="toc-section-number">11</span> Linear Model Variants</a>
<ul>
<li><a href="lmv.html#model-comparisons" id="toc-model-comparisons"><span class="toc-section-number">11.1</span> Model Comparisons</a></li>
<li><a href="lmv.html#hierarchical-regression" id="toc-hierarchical-regression"><span class="toc-section-number">11.2</span> Hierarchical Regression</a></li>
<li><a href="lmv.html#multilevel-models" id="toc-multilevel-models"><span class="toc-section-number">11.3</span> Multilevel Models</a></li>
<li><a href="lmv.html#polynomial-regression" id="toc-polynomial-regression"><span class="toc-section-number">11.4</span> Polynomial Regression</a></li>
<li><a href="lmv.html#review-questions-9" id="toc-review-questions-9"><span class="toc-section-number">11.5</span> Review Questions</a></li>
</ul></li>
<li><a href="log.html#log" id="toc-log"><span class="toc-section-number">12</span> Logistic Regression</a>
<ul>
<li><a href="log.html#binomial-logistic-regression" id="toc-binomial-logistic-regression"><span class="toc-section-number">12.0.1</span> Binomial Logistic Regression</a></li>
<li><a href="log.html#multinomial-logistic-regression" id="toc-multinomial-logistic-regression"><span class="toc-section-number">12.0.2</span> Multinomial Logistic Regression</a></li>
<li><a href="log.html#ordinal-logistic-regression" id="toc-ordinal-logistic-regression"><span class="toc-section-number">12.0.3</span> Ordinal Logistic Regression</a></li>
<li><a href="log.html#review-questions-10" id="toc-review-questions-10"><span class="toc-section-number">12.1</span> Review Questions</a></li>
</ul></li>
<li><a href="pred-mod.html#pred-mod" id="toc-pred-mod"><span class="toc-section-number">13</span> Predictive Modeling</a>
<ul>
<li><a href="pred-mod.html#cross-validation" id="toc-cross-validation"><span class="toc-section-number">13.1</span> Cross-Validation</a></li>
<li><a href="pred-mod.html#model-performance" id="toc-model-performance"><span class="toc-section-number">13.2</span> Model Performance</a></li>
<li><a href="pred-mod.html#bias-variance-tradeoff" id="toc-bias-variance-tradeoff"><span class="toc-section-number">13.3</span> Bias-Variance Tradeoff</a></li>
<li><a href="pred-mod.html#tree-based-algorithms" id="toc-tree-based-algorithms"><span class="toc-section-number">13.4</span> Tree-Based Algorithms</a></li>
<li><a href="pred-mod.html#predictive-modeling" id="toc-predictive-modeling"><span class="toc-section-number">13.5</span> Predictive Modeling</a></li>
<li><a href="pred-mod.html#review-questions-11" id="toc-review-questions-11"><span class="toc-section-number">13.6</span> Review Questions</a></li>
</ul></li>
<li><a href="unsup-lrn.html#unsup-lrn" id="toc-unsup-lrn"><span class="toc-section-number">14</span> Unsupervised Learning</a>
<ul>
<li><a href="unsup-lrn.html#factor-analysis" id="toc-factor-analysis"><span class="toc-section-number">14.1</span> Factor Analysis</a>
<ul>
<li><a href="unsup-lrn.html#exploratory-factor-analysis-efa" id="toc-exploratory-factor-analysis-efa"><span class="toc-section-number">14.1.1</span> Exploratory Factor Analysis (EFA)</a></li>
<li><a href="unsup-lrn.html#confirmatory-factor-analysis-cfa" id="toc-confirmatory-factor-analysis-cfa"><span class="toc-section-number">14.1.2</span> Confirmatory Factor Analysis (CFA)</a></li>
</ul></li>
<li><a href="unsup-lrn.html#clustering" id="toc-clustering"><span class="toc-section-number">14.2</span> Clustering</a>
<ul>
<li><a href="unsup-lrn.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">14.2.1</span> <span class="math inline">\(K\)</span>-Means Clustering</a></li>
<li><a href="unsup-lrn.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">14.2.2</span> Hierarchical Clustering</a></li>
</ul></li>
<li><a href="unsup-lrn.html#review-questions-12" id="toc-review-questions-12"><span class="toc-section-number">14.3</span> Review Questions</a></li>
</ul></li>
<li><a href="data-viz.html#data-viz" id="toc-data-viz"><span class="toc-section-number">15</span> Data Visualization</a>
<ul>
<li><a href="data-viz.html#design-guidelines" id="toc-design-guidelines"><span class="toc-section-number">15.1</span> Design Guidelines</a></li>
<li><a href="data-viz.html#visualization-types" id="toc-visualization-types"><span class="toc-section-number">15.2</span> Visualization Types</a>
<ul>
<li><a href="data-viz.html#tables" id="toc-tables"><span class="toc-section-number">15.2.1</span> Tables</a></li>
<li><a href="data-viz.html#heatmaps" id="toc-heatmaps"><span class="toc-section-number">15.2.2</span> Heatmaps</a></li>
<li><a href="data-viz.html#scatterplots" id="toc-scatterplots"><span class="toc-section-number">15.2.3</span> Scatterplots</a></li>
<li><a href="data-viz.html#line-graphs" id="toc-line-graphs"><span class="toc-section-number">15.2.4</span> Line Graphs</a></li>
<li><a href="data-viz.html#slopegraphs" id="toc-slopegraphs"><span class="toc-section-number">15.2.5</span> Slopegraphs</a></li>
<li><a href="data-viz.html#bar-charts" id="toc-bar-charts"><span class="toc-section-number">15.2.6</span> Bar Charts</a></li>
<li><a href="data-viz.html#waterfall-charts" id="toc-waterfall-charts"><span class="toc-section-number">15.2.7</span> Waterfall Charts</a></li>
<li><a href="data-viz.html#area-charts" id="toc-area-charts"><span class="toc-section-number">15.2.8</span> Area Charts</a></li>
<li><a href="data-viz.html#pie-charts" id="toc-pie-charts"><span class="toc-section-number">15.2.9</span> Pie Charts</a></li>
</ul></li>
<li><a href="data-viz.html#deceptive-visuals" id="toc-deceptive-visuals"><span class="toc-section-number">15.3</span> Deceptive Visuals</a></li>
<li><a href="data-viz.html#review-questions-13" id="toc-review-questions-13"><span class="toc-section-number">15.4</span> Review Questions</a></li>
</ul></li>
<li><a href="storytelling.html#storytelling" id="toc-storytelling"><span class="toc-section-number">16</span> Data Storytelling</a>
<ul>
<li><a href="storytelling.html#know-your-audience" id="toc-know-your-audience"><span class="toc-section-number">16.1</span> Know Your Audience</a></li>
<li><a href="storytelling.html#preattentive-attributes" id="toc-preattentive-attributes"><span class="toc-section-number">16.2</span> Preattentive Attributes</a></li>
<li><a href="storytelling.html#production-status" id="toc-production-status"><span class="toc-section-number">16.3</span> Production Status</a></li>
<li><a href="storytelling.html#structural-elements" id="toc-structural-elements"><span class="toc-section-number">16.4</span> Structural Elements</a>
<ul>
<li><a href="storytelling.html#tldr" id="toc-tldr"><span class="toc-section-number">16.4.1</span> TL;DR</a></li>
<li><a href="storytelling.html#problem-statement" id="toc-problem-statement"><span class="toc-section-number">16.4.2</span> Problem Statement</a></li>
<li><a href="storytelling.html#methodology" id="toc-methodology"><span class="toc-section-number">16.4.3</span> Methodology</a></li>
<li><a href="storytelling.html#results" id="toc-results"><span class="toc-section-number">16.4.4</span> Results</a></li>
<li><a href="storytelling.html#limitations" id="toc-limitations"><span class="toc-section-number">16.4.5</span> Limitations</a></li>
<li><a href="storytelling.html#next-steps" id="toc-next-steps"><span class="toc-section-number">16.4.6</span> Next Steps</a></li>
<li><a href="storytelling.html#appendix" id="toc-appendix"><span class="toc-section-number">16.4.7</span> Appendix</a></li>
</ul></li>
<li><a href="storytelling.html#qa" id="toc-qa"><span class="toc-section-number">16.5</span> Q&amp;A</a></li>
<li><a href="storytelling.html#review-questions-14" id="toc-review-questions-14"><span class="toc-section-number">16.6</span> Review Questions</a></li>
</ul></li>
<li><a href="bibli.html#bibli" id="toc-bibli"><span class="toc-section-number">17</span> Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to People Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="aod" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Analysis of Differences</h1>
<p>There are many statistical tests that can be used to test for differences within or between two or more groups. This chapter will cover common contexts for differences in people analytics and the tests applicable to each.</p>
<div id="parametric-vs.-nonparametric-tests" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Parametric vs. Nonparametric Tests</h2>
<p>In the context of data measured on a continuous scale (quantitative), we will cover parametric tests along with their nonparametric counterparts. When the hypothesis relates to average (mean) differences and <span class="math inline">\(n\)</span> is large, <strong>parametric tests</strong> are preferred as they generally have more statistical power. <strong>Nonparametric tests</strong> are <strong>distribution-free tests</strong> that do not require the population’s distribution to be characterized by certain parameters, such as a normal distribution defined by a mean and standard deviation. Nonparametric tests are great for qualitative data since the distribution of non-numeric data cannot be characterized by parameters.</p>
<p>Beyond ensuring the data were generated from a random and representative process as discussed in Chapter <a href="measure-sampl.html#measure-sampl">4</a>, as well as following the data screening procedures outlined in Chapter <a href="data-mgmt-prep.html#data-mgmt-prep">6</a> (e.g., addressing concerning outliers), parametric tests of differences <em>generally</em> feature three key assumptions:</p>
<ol style="list-style-type: decimal">
<li><strong>Independence</strong>: Observations within each group are independent of each other</li>
<li><strong>Homogeneity of Variance</strong>: Variances of populations from which samples were drawn are equal</li>
<li><strong>Normality</strong>: Residuals must be normally distributed (with mean of 0) within each group</li>
</ol>
<p>While homogeneity of variance assumes the variances across multiple groups are equal, parametric tests are generally robust to violations of equal variances when the sample sizes are large. Also, you may recall that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are sufficient to characterize a population distribution when data are situated symmetrically around the mean. However, the mean can be sensitive to outliers so if outliers are present in the data, the median may be a better way of representing the data’s center (i.e., nonparametric tests); just remember that the use of nonparametric tests requires hypotheses to be modified to adjust for <em>median</em> – rather than <em>mean</em> – centers.</p>
<p>You may be wondering whether the magical elixir that is the CLT, which we covered in Chapter <a href="inf-stats.html#inf-stats">8</a>, influences our ability to utilize parametric tests with respect to the assumption of normality. It’s important to remember that the normal distribution properties under the CLT relate to the <em>sampling distribution of means</em> – not to the distribution of the population or to the data for an individual sample. The CLT is important for estimating population parameters, but it does not transform a population distribution from nonnormal to normal. If we know the population distribution is nonnormal (e.g., ordinal, nominal, or skewed data), nonparametric tests should be considered. This is why we used Spearman’s correlation coefficient – a nonparametric test – in Chapter <a href="desc-stats.html#desc-stats">7</a> to evaluate the relationship between job level and education; these ordinal data are not normally distributed in the population.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="aod.html#cb263-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load employee data</span></span>
<span id="cb263-2"><a href="aod.html#cb263-2" aria-hidden="true" tabindex="-1"></a>employees <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_book/master/data/employees.csv&quot;</span>)</span></code></pre></div>
</div>
<div id="differences-in-discrete-data" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Differences in Discrete Data</h2>
<p>Nonparametric tests are generally best when working with data measured on a discrete scale since these data do not come from normally distributed populations. The two most commonly used tests to analyze variables measured on a discrete scale are the nonparametric <em>Chi-square test</em> and <em>Fisher’s exact test</em>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:discrete-tests"></span>
<img src="graphics/discrete_differences_test_table.png" alt="Chi-Square and Fisher Exact Test Criteria for Discrete Data" width="100%" />
<p class="caption">
Figure 9.1: Chi-Square and Fisher Exact Test Criteria for Discrete Data
</p>
</div>
<p>Both tests organize data within 2x2 <strong>contingency tables</strong> which enables us to understand interrelations between variables.</p>
<p><strong>Chi-Square Test</strong></p>
<p>The <strong>Chi-Square Test of Independence</strong> evaluates patterns of observations to determine if categories occur more frequently than we would expect by chance. The Chi-square statistic is defined by:</p>
<p><span class="math display">\[ {\chi}^2 = \sum\frac{(O_i - E_i)^2}{E_i}, \]</span></p>
<p>where <span class="math inline">\(O_i\)</span> is the observed value, and <span class="math inline">\(E_i\)</span> is the expected value.</p>
<p><span class="math inline">\(H_0\)</span> states that each variable is independent of one another (i.e., there is no relationship). In addition to the <span class="math inline">\({\chi}^2\)</span> test statistic, <span class="math inline">\(df\)</span> for the contingency table, defined by <span class="math inline">\(df = (rows - 1) * (columns - 1)\)</span>, is required to determine whether we reject or fail to reject <span class="math inline">\(H_0\)</span>.</p>
<p>While there is not consensus on the minimum sample size for this test, it is important to note that the <span class="math inline">\({\chi}^2\)</span> statistic follows a chi-square distribution <em>asymptotically</em>. This means we can only calculate accurate <span class="math inline">\(p\)</span>-values for larger samples, and a general rule of thumb is that the expected value for each cell needs to be at least 5. The challenge with small <span class="math inline">\(n\)</span>-counts is illustrated in Figure <a href="aod.html#fig:chisq-dist">9.2</a>; the chi-square distribution approaches a vertical line as <span class="math inline">\(df\)</span> drops below 5.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chisq-dist"></span>
<img src="Intro_People_Analytics_files/figure-html/chisq-dist-1.png" alt="Chi-Square Distributions by Degrees of Freedom (df)" width="100%" />
<p class="caption">
Figure 9.2: Chi-Square Distributions by Degrees of Freedom (df)
</p>
</div>
<p>We will demonstrate how to perform a chi-square test of independence by evaluating whether exit rates are independent of whether an employee works overtime. Let’s first construct a 2x2 contingency table using the <code>table()</code> function:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="aod.html#cb264-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create contingency table</span></span>
<span id="cb264-2"><a href="aod.html#cb264-2" aria-hidden="true" tabindex="-1"></a>cont_tbl <span class="ot">&lt;-</span> <span class="fu">table</span>(employees<span class="sc">$</span>active, employees<span class="sc">$</span>overtime)</span>
<span id="cb264-3"><a href="aod.html#cb264-3" aria-hidden="true" tabindex="-1"></a>cont_tbl</span></code></pre></div>
<pre><code>##      
##        No Yes
##   No  110 127
##   Yes 944 289</code></pre>
<p>A <strong>mosaic plot</strong> is a great way to visualize the delta between expected and observed frequencies for each cell. This can be produced using the <code>mosaicplot()</code> function from the <code>graphics</code> library:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mosaic-plot"></span>
<img src="Intro_People_Analytics_files/figure-html/mosaic-plot-1.png" alt="Mosaic Plot of Residuals for Overtime x Active Status" width="100%" />
<p class="caption">
Figure 9.3: Mosaic Plot of Residuals for Overtime x Active Status
</p>
</div>
<p>In Figure <a href="aod.html#fig:mosaic-plot">9.3</a>, blue indicates that the observed value is higher than the expected value, while red indicates that the observed value is lower than the expected value. Based on this plot, there appears to be some meaningful patterns and departures from expected values in both the high and low directions. There are more inactive employees than expected in the overtime group, and more active employees than expected in the group with no overtime. These large standardized residuals are indicative of meaningful relationships between the two categorical variables.</p>
<p>Let’s run the chi-square test of independence to determine whether these residuals are statistically significant. This test can be performed in R by passing the contingency table into the <code>chisq.test()</code> function:</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="aod.html#cb266-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform chi-square test of independence</span></span>
<span id="cb266-2"><a href="aod.html#cb266-2" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(cont_tbl)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  cont_tbl
## X-squared = 87.564, df = 1, p-value &lt; 2.2e-16</code></pre>
<p>Based on the results, exit rates are not independent of overtime (<span class="math inline">\({\chi}^2(1) = 87.56, p &lt; .05\)</span>). Therefore, there is a statistically significant relationship between an employee working overtime and the rate at which they change from an active to inactive status – confirming what was evident in the mosaic plot.</p>
<p><strong>Fisher’s Exact Test</strong></p>
<p>When the sample size is small, <strong>Fisher’s Exact Test</strong> can be used to calculate the <em>exact</em> <span class="math inline">\(p\)</span>-value rather than an approximation, which is the case with many statistical tests such as the chi-square test.</p>
<p><span class="math inline">\(H_0\)</span> for Fisher’s exact test is the same as <span class="math inline">\(H_0\)</span> for the chi-square test of independence: There is no relationship between the two categorical variables (i.e., they are independent). We can perform Fisher’s exact test using the <code>fisher.test()</code> function in R:</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="aod.html#cb268-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Fisher&#39;s exact test</span></span>
<span id="cb268-2"><a href="aod.html#cb268-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fisher.test</span>(cont_tbl)</span></code></pre></div>
<pre><code>## 
##  Fisher&#39;s Exact Test for Count Data
## 
## data:  cont_tbl
## p-value &lt; 2.2e-16
## alternative hypothesis: true odds ratio is not equal to 1
## 95 percent confidence interval:
##  0.1969101 0.3572582
## sample estimates:
## odds ratio 
##  0.2654384</code></pre>
<p>Note the <em>odds ratio</em> shown in this output. The <strong>odds ratio</strong> represents the ratio of positive to negative cases, which is the ratio of overtime for active and inactive workers in this example. The odds ratio is defined by:</p>
<p><span class="math display">\[OR = \frac{a*d}{b*c}\]</span></p>
<p>An odds ratio of 1 indicates no difference in overtime frequency between active and inactive workers. Figure <a href="aod.html#fig:contingency-tbl">9.4</a> illustrates the cells for the odds ratio calculation for the 2x2 contingency table of overtime for active and inactive (termed) workers.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:contingency-tbl"></span>
<img src="graphics/contingency_table.png" alt="2x2 Contingency Table of Overtime for Active and Inactive Workers" width="75%" />
<p class="caption">
Figure 9.4: 2x2 Contingency Table of Overtime for Active and Inactive Workers
</p>
</div>
<p>Since the 95% <span class="math inline">\(CI\)</span> for the odds ratio does not include 1, we reject the null hypothesis and conclude that exit rates are related to working overtime; this is consistent with results from the chi-square test of independence. Since overtime was indicated far more often for inactive workers than for active workers, it is no surprise that the denominator of our ratio is larger than the numerator (i.e., <span class="math inline">\(OR\)</span> &lt; 1).</p>
<p>As discussed in Chapter <a href="desc-stats.html#desc-stats">7</a>, we can produce a <span class="math inline">\(\phi\)</span> coefficient to understand the strength of the association by passing the contingency table into the <code>phi</code> function from the <code>psych</code> library:</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="aod.html#cb270-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the Phi Coefficient</span></span>
<span id="cb270-2"><a href="aod.html#cb270-2" aria-hidden="true" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">phi</span>(cont_tbl)</span></code></pre></div>
<pre><code>## [1] -0.25</code></pre>
<p>The relationship between active status and overtime is negative, and the strength of the relationship is weak (<span class="math inline">\(\phi\)</span> = -.25).</p>
<p>Another common method of measuring the strength of the association between two categorical variables is <strong>Cramer’s V</strong>, which ranges from 0 (no association) to 1 (strong association). In the interest of not muddying the waters with an exhaustive set of alernative methods, the implementation won’t be covered.</p>
</div>
<div id="differences-in-continuous-data" class="section level2" number="9.3">
<h2><span class="header-section-number">9.3</span> Differences in Continuous Data</h2>
<p>A variety of parametric and nonparametric tests are available for evaluating differences between variables measured on a continuous scale. Figure <a href="aod.html#fig:continuous-tests">9.5</a> provides a side-by-side of these parametric and corresponding nonparametric tests of differences.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:continuous-tests"></span>
<img src="graphics/continuous_differences_test_table.png" alt="Parametric and Nonparametric Tests of Differences for Continuous Data" width="100%" />
<p class="caption">
Figure 9.5: Parametric and Nonparametric Tests of Differences for Continuous Data
</p>
</div>
<p><strong>Independent Samples </strong><span class="math inline">\(\textbf t\)</span><strong>-Test</strong></p>
<p>When evaluating differences between two independent samples, social psychology researchers generally select from two tests: <em>Student’s</em> <span class="math inline">\(t\)</span><em>-test</em> and <em>Welch’s</em> <span class="math inline">\(t\)</span><em>-test</em>. There are other alternatives, such as <em>Yuen’s</em> <span class="math inline">\(t\)</span><em>-test</em> and a <em>bootstrapped</em> <span class="math inline">\(t\)</span><em>-test</em>, but these are less commonly reported in scholarly social science journals and will not be covered in this book.</p>
<p>The <strong>Student’s</strong> <span class="math inline">\(t\)</span><strong>-test</strong>, which was introduced in Chapter <a href="inf-stats.html#inf-stats">8</a>, is a parametric test whose assumptions of equal variances seldom hold in people analytics. <strong>Welch’s</strong> <span class="math inline">\(t\)</span><strong>-test</strong> is generally preferred to the Student’s <span class="math inline">\(t\)</span>-test because it has been shown to provide better control of Type 1 error rates when homogeneity of variance is not met, whilst losing little robustness (e.g., Delacre, Lakens, &amp; Leys, 2017). When <span class="math inline">\(n\)</span> is equal between groups, the Student’s <span class="math inline">\(t\)</span>-test is known to be robust to violations of the equal variance assumption, as long as <span class="math inline">\(n\)</span> is sufficiently high to accurately estimate parameters and the underlying distribution is not characterized by high skewness and kurtosis.</p>
<p>Let’s explore the mechanics of independent samples <span class="math inline">\(t\)</span>-tests. Figure <a href="aod.html#fig:mean-group-diff">9.6</a> illustrates mean differences (<span class="math inline">\(MD\)</span>) for nine Welch’s <span class="math inline">\(t\)</span>-tests based on random sample data generated from independent normal populations. Remember that statistical power increases with a large <span class="math inline">\(n\)</span>, as <span class="math inline">\(t\)</span> distributions approximate a standard normal distribution with larger <span class="math inline">\(df\)</span>. In the context of analysis of differences, this translates to an increase in the likelihood of detecting statistical differences in the means of two distributions. Note that for the two cases where both <span class="math inline">\(MD\)</span> and <span class="math inline">\(n\)</span> are relatively small, mean differences are not statistically significant (<span class="math inline">\(p &gt; .05\)</span>). You may also notice that as the <span class="math inline">\(t\)</span>-statistic approaches 0, statistical differences become less likely since a smaller <span class="math inline">\(t\)</span>-statistic indicates a smaller difference between mean values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mean-group-diff"></span>
<img src="Intro_People_Analytics_files/figure-html/mean-group-diff-1.png" alt="Density plots comparing distributions with a mean of 100 (blue) and 120 (grey) with sample sizes of 100, 1,000, and 10,000 (rows) and standard deviations of 25, 50, and 75 (columns). Dashed vertical lines represent distribution means." width="100%" />
<p class="caption">
Figure 9.6: Density plots comparing distributions with a mean of 100 (blue) and 120 (grey) with sample sizes of 100, 1,000, and 10,000 (rows) and standard deviations of 25, 50, and 75 (columns). Dashed vertical lines represent distribution means.
</p>
</div>
<p>Next, we will walk through the steps involved in performing Welch’s <span class="math inline">\(t\)</span>-test. Let’s first visualize the distribution of data for each group using boxplots:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:comp-job-boxplots"></span>
<img src="Intro_People_Analytics_files/figure-html/comp-job-boxplots-1.png" alt="Annual Compensation Distributions for Managers and Research Scientists" width="100%" />
<p class="caption">
Figure 9.7: Annual Compensation Distributions for Managers and Research Scientists
</p>
</div>
<p>While the median – rather than the mean which is being evaluated with Welch’s <span class="math inline">\(t\)</span>-test – is shown in these boxplots, this is a great way to visually inspect whether there are meaningful differences in the distribution of data between groups (in addition to identifying outliers). We could of course use density plots or histograms as an alternative. As we can see, annual compensation for employees with a Manager job title tends to be slightly higher than for those with a Research Scientist job title, and the variance in annual compensation appears to be fairly consistent between the groups.</p>
<p>There are several alternatives to a visual inspection of normality, such as the <span class="math inline">\({\chi}^2\)</span> <strong>Goodness-of-Fit test</strong> (Snedecor &amp; Cochran, 1980), <strong>Kolmogorov-Smirnov (K-S) test</strong> (Chakravarti, Laha, &amp; Roy, 1967), or <strong>Shapiro-Wilk test</strong> (Shapiro &amp; Wilk, 1965). The general idea is consistent for each of these tests: compare observed data to what would be expected if data are sampled from a normally distributed population. The <span class="math inline">\({\chi}^2\)</span> Goodness-of-Fit test compares the <em>count</em> of data points across the range of values relative to what would be expected in each for a sample with the same dimensions taken from a normal distribution. For example, if data are sampled from a normal population distribution, it follows that roughly half the values should exist below the mean and half above the mean. The K-S test evaluates how the observed <em>cumulative</em> distribution compares to the properties of a normal <em>cumulative</em> distribution. The Shapiro-Wilk test is based on <em>correlations</em> between observed and expected data.</p>
<p>We will test for normality using the Shapiro-Wilk test. The null hypothesis for the Shapiro-Wilk test is that the data are normally distributed, so a high <span class="math inline">\(p\)</span>-value indicates that the assumption of normality is satisfied (i.e., failure to reject the null hypothesis of normally distributed data). We can use the <code>with()</code> function together with the <code>shapiro.test()</code> function to run this test in R:</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="aod.html#cb272-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Shapiro-Wilk test of normality for each group</span></span>
<span id="cb272-2"><a href="aod.html#cb272-2" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(employees, <span class="fu">shapiro.test</span>(annual_comp[job_title <span class="sc">==</span> <span class="st">&#39;Manager&#39;</span>]))</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  annual_comp[job_title == &quot;Manager&quot;]
## W = 0.93546, p-value = 0.000103</code></pre>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="aod.html#cb274-1" aria-hidden="true" tabindex="-1"></a><span class="fu">with</span>(employees, <span class="fu">shapiro.test</span>(annual_comp[job_title <span class="sc">==</span> <span class="st">&#39;Research Scientist&#39;</span>]))</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  annual_comp[job_title == &quot;Research Scientist&quot;]
## W = 0.96002, p-value = 3.427e-07</code></pre>
<p>Based on these tests, the distribution of annual compensation for Managers and Research Scientists are significantly different from normally distributed data (<span class="math inline">\(p\)</span> &lt; .05).</p>
<p>While we should not proceed with performing Welch’s <span class="math inline">\(t\)</span>-test due to unequal variances, let’s do so merely to illustrate how the test is implemented in R. To perform Welch’s <span class="math inline">\(t\)</span>-test in R, we can simply pass into the <code>t.test()</code> function a numeric vector for each of the two groups.</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="aod.html#cb276-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create compensation vectors for two jobs</span></span>
<span id="cb276-2"><a href="aod.html#cb276-2" aria-hidden="true" tabindex="-1"></a>comp_mgr <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, job_title <span class="sc">==</span> <span class="st">&#39;Manager&#39;</span>, <span class="at">select =</span> annual_comp))</span>
<span id="cb276-3"><a href="aod.html#cb276-3" aria-hidden="true" tabindex="-1"></a>comp_rsci <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, job_title <span class="sc">==</span> <span class="st">&#39;Research Scientist&#39;</span>, <span class="at">select =</span> annual_comp))</span>
<span id="cb276-4"><a href="aod.html#cb276-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb276-5"><a href="aod.html#cb276-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Run Welch&#39;s t-test</span></span>
<span id="cb276-6"><a href="aod.html#cb276-6" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(comp_mgr, comp_rsci)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  comp_mgr and comp_rsci
## t = 0.22623, df = 159.55, p-value = 0.8213
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -8860.685 11153.244
## sample estimates:
## mean of x mean of y 
##  139900.8  138754.5</code></pre>
<p>If the data adhered to the assumptions of Welch’s <span class="math inline">\(t\)</span>-test, we would conclude that the mean difference between annual compensation for Managers (<span class="math inline">\(\bar{x}\)</span> = 139,901) and Research Scientists (<span class="math inline">\(\bar{x}\)</span> = 138,755) is not significant (<span class="math inline">\(t(159.55)\)</span> = .23, <span class="math inline">\(p\)</span> = .82).</p>
<p>Note that we can access specific metrics from this output by storing results to an object and then referencing specific elements by name or index:</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="aod.html#cb278-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This assigns each element of results from Welch&#39;s t-test to an indexed position in the object</span></span>
<span id="cb278-2"><a href="aod.html#cb278-2" aria-hidden="true" tabindex="-1"></a>t_rslts <span class="ot">&lt;-</span> <span class="fu">t.test</span>(comp_mgr, comp_rsci)</span>
<span id="cb278-3"><a href="aod.html#cb278-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb278-4"><a href="aod.html#cb278-4" aria-hidden="true" tabindex="-1"></a>t_rslts<span class="sc">$</span>statistic <span class="co"># t-statistic</span></span></code></pre></div>
<pre><code>##         t 
## 0.2262262</code></pre>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="aod.html#cb280-1" aria-hidden="true" tabindex="-1"></a>t_rslts<span class="sc">$</span>parameter <span class="co"># df</span></span></code></pre></div>
<pre><code>##       df 
## 159.5544</code></pre>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="aod.html#cb282-1" aria-hidden="true" tabindex="-1"></a>t_rslts<span class="sc">$</span>p.value <span class="co"># p-value</span></span></code></pre></div>
<pre><code>## [1] 0.8213151</code></pre>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="aod.html#cb284-1" aria-hidden="true" tabindex="-1"></a>t_rslts<span class="sc">$</span>method <span class="co"># type of t-test</span></span></code></pre></div>
<pre><code>## [1] &quot;Welch Two Sample t-test&quot;</code></pre>
<p>When object elements are referenced by index, the element name is displayed in the output to clarify what the metric represents:</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="aod.html#cb286-1" aria-hidden="true" tabindex="-1"></a>t_rslts[<span class="dv">1</span>] <span class="co"># t-statistic</span></span></code></pre></div>
<pre><code>## $statistic
##         t 
## 0.2262262</code></pre>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="aod.html#cb288-1" aria-hidden="true" tabindex="-1"></a>t_rslts[<span class="dv">2</span>] <span class="co"># df</span></span></code></pre></div>
<pre><code>## $parameter
##       df 
## 159.5544</code></pre>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="aod.html#cb290-1" aria-hidden="true" tabindex="-1"></a>t_rslts[<span class="dv">3</span>] <span class="co"># p-value</span></span></code></pre></div>
<pre><code>## $p.value
## [1] 0.8213151</code></pre>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="aod.html#cb292-1" aria-hidden="true" tabindex="-1"></a>t_rslts[<span class="dv">9</span>] <span class="co"># type of t-test</span></span></code></pre></div>
<pre><code>## $method
## [1] &quot;Welch Two Sample t-test&quot;</code></pre>
<p>Given <span class="math inline">\(df\)</span> = 159.55, you may be wondering how <span class="math inline">\(df\)</span> is calculated for Welch’s <span class="math inline">\(t\)</span>-test given that thus far, we have only discussed the basic <span class="math inline">\(df\)</span> calculation outlined in Chapter <a href="inf-stats.html#inf-stats">8</a>; namely, <span class="math inline">\(df = n - 1\)</span>. Welch’s <span class="math inline">\(t\)</span>-test uses the <strong>Welch-Satterthwaite equation</strong> for <span class="math inline">\(df\)</span> (Satterthwaite, 1946; Welch, 1947). This equation approximates <span class="math inline">\(df\)</span> for a linear combination of independent sample variances; which means that if samples are not independent, this approximation may not be valid. The Welch-Satterthwaite equation is defined by:</p>
<p><span class="math display">\[ df = \frac {(\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2})^2} {\frac{1}{n_1 - 1} (\frac{s^2_1}{n_1})^2 + \frac{1}{n_2 - 1} (\frac{s^2_2}{n_2})^2} \]</span></p>
<p><strong>Cohen’s</strong> <span class="math inline">\(\textbf d\)</span> is a standardized measure of the difference between two means that helps us understand the size (or <em>practical</em> significance) of observed mean differences. Cohen’s <span class="math inline">\(d\)</span> is defined by:</p>
<p><span class="math display">\[ d = \frac{\bar{x}_1 - \bar{x}_2} {s_p},  \]</span></p>
<p>where <span class="math inline">\(s_p\)</span> represents the pooled standard deviation defined by:</p>
<p><span class="math display">\[ s_p = \sqrt\frac{s^2_1 + s^2_2}{2} \]</span></p>
<p>Cohen’s <span class="math inline">\(d\)</span> can be produced using the <code>cohen.d()</code> function from the <code>effsize</code> package in R. The following thresholds can be referenced as a <em>general</em> rule of thumb for interpreting effect size:</p>
<ul>
<li><strong>Small</strong> = 0.2</li>
<li><strong>Medium</strong> = 0.5</li>
<li><strong>Large</strong> = 0.8</li>
</ul>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="aod.html#cb294-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library for effect size functions</span></span>
<span id="cb294-2"><a href="aod.html#cb294-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(effsize)</span>
<span id="cb294-3"><a href="aod.html#cb294-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb294-4"><a href="aod.html#cb294-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Cohen&#39;s d</span></span>
<span id="cb294-5"><a href="aod.html#cb294-5" aria-hidden="true" tabindex="-1"></a>effsize<span class="sc">::</span><span class="fu">cohen.d</span>(comp_mgr, comp_rsci)</span></code></pre></div>
<pre><code>## 
## Cohen&#39;s d
## 
## d estimate: 0.0273669 (negligible)
## 95 percent confidence interval:
##      lower      upper 
## -0.2004390  0.2551728</code></pre>
<p>Not only are the differences statistically insignificant, Cohen’s <span class="math inline">\(d\)</span> = .03 indicates a negligible difference. Therefore, there is nothing of interest based on these statistical and practical significance tests.</p>
<p><strong>Mann-Whitney U Test</strong></p>
<p>A popular nonparametric (distribution-free) alternative to Welch’s <span class="math inline">\(t\)</span>-test is the <strong>Mann-Whitney U Test</strong>, also referred to as the <strong>Wilcoxon Rank-Sum Test</strong>. Rather than comparing the mean between two groups, like the Student’s <span class="math inline">\(t\)</span>-test or Welch’s <span class="math inline">\(t\)</span>-test, the Mann-Whitney U test considers the entire distribution by evaluating the extent to which the <em>ranks</em> are consistent between groups (i.e., similarity in the proportion of records with each value). When distributions are similar, the medians of the two groups are compared.</p>
<p>The <code>wilcox.test()</code> function is used to run this test in R. Let’s illustrate by examining whether engagement (an ordinal variable in our dataset) is significantly different between those who have been promoted in the past year and those who have not:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb296-1"><a href="aod.html#cb296-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create dummy-coded promotion variable</span></span>
<span id="cb296-2"><a href="aod.html#cb296-2" aria-hidden="true" tabindex="-1"></a>employees<span class="sc">$</span>promo <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(employees<span class="sc">$</span>last_promo <span class="sc">==</span> <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb296-3"><a href="aod.html#cb296-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb296-4"><a href="aod.html#cb296-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create numeric engagement vectors for promo groups</span></span>
<span id="cb296-5"><a href="aod.html#cb296-5" aria-hidden="true" tabindex="-1"></a>no_promo <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, promo <span class="sc">==</span> <span class="dv">0</span>, <span class="at">select =</span> engagement))</span>
<span id="cb296-6"><a href="aod.html#cb296-6" aria-hidden="true" tabindex="-1"></a>promo <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, promo <span class="sc">==</span> <span class="dv">1</span>, <span class="at">select =</span> engagement))</span>
<span id="cb296-7"><a href="aod.html#cb296-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb296-8"><a href="aod.html#cb296-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform the Mann-Whitney U (aka Wilcoxon rank-sum) test</span></span>
<span id="cb296-9"><a href="aod.html#cb296-9" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(no_promo, promo)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test with continuity correction
## 
## data:  no_promo and promo
## W = 196056, p-value = 0.6707
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Based on these results, we fail to reject the null hypothesis, which states that there is no difference in engagement between those with and without promotions (<span class="math inline">\(W\)</span> = 196,056, <span class="math inline">\(p\)</span> = .67). Note the reference to continuity correction in the output. <strong>Continuity correction</strong> is applied when using a continuous distribution to approximate a discrete distribution. The Mann-Whitney U test we performed approximated a continuous distribution for testing differences between our ordinal (discrete) engagement data by applying this continuity correction.</p>
<p>Just as Cohen’s <span class="math inline">\(d\)</span> is used to measure the magnitude of difference between a pair of means, <strong>Cliff’s delta</strong> can be leveraged to evaluate the size of differences between ordinal variables. Cliff’s delta measures how often a value in one distribution is higher than values in another, and this is appropriate in situations in which a nonparametric test of differences is used. This statistic can be produced using the <code>cliff.delta()</code> function from the <code>effsize</code> package in R.</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="aod.html#cb298-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run Cliff&#39;s Delta</span></span>
<span id="cb298-2"><a href="aod.html#cb298-2" aria-hidden="true" tabindex="-1"></a>effsize<span class="sc">::</span><span class="fu">cliff.delta</span>(no_promo, promo)</span></code></pre></div>
<pre><code>## 
## Cliff&#39;s Delta
## 
## delta estimate: -0.0131625 (negligible)
## 95 percent confidence interval:
##       lower       upper 
## -0.07329491  0.04706528</code></pre>
<p>Some (e.g., Vargha &amp; Delaney, 2000) have endeavored to categorize the Cliff’s delta statistic, which ranges from -1 to 1, into effect size buckets. However, such categorizations are far more controversial than thresholds attributed to Cohen’s <span class="math inline">\(d\)</span>. Nevertheless, the near-zero delta estimate of -.01 indicates a negligible difference.</p>
<p><strong>Paired Samples</strong> <span class="math inline">\(\textbf t\)</span><strong>-Test</strong></p>
<p>A <strong>Paired Samples </strong><span class="math inline">\(\textbf t\)</span><strong>-Test</strong> is used to compare means between pairs of measurements. This test is known by many other names, such as a <strong>dependent samples</strong> <span class="math inline">\(\textbf t\)</span><strong>-test</strong>, <strong>paired-difference</strong> <span class="math inline">\(\textbf t\)</span><strong>-test</strong>, <strong>matched pairs</strong> <span class="math inline">\(\textbf t\)</span><strong>-test</strong>, and <strong>repeated-samples</strong> <span class="math inline">\(\textbf t\)</span><strong>-test</strong>.</p>
<p>The assumption of normality in the context of a paired samples <span class="math inline">\(t\)</span>-test relates to normally distributed paired differences. This is important, as the <span class="math inline">\(p\)</span>-value for the test statistic will not be valid if this assumption is violated.</p>
<p>To illustrate, let’s design an experiment. Let’s assume morale has declined for employees who travel frequently, and several actions have been proposed by a task force to help address this. The task force has decided to pilot a new flexible work benefit over a six-month period to determine if it has a meaningful effect on morale. This new benefit is piloted to a random sample of frequent travelers, and our task is to test whether the outcomes warrant a broader rollout to frequent travelers.</p>
<p>Our DV (happiness) will be measured using a composite index derived from individual engagement, environment satisfaction, job satisfaction, and relationship satisfaction scores. Our objective is to determine if there is a significant improvement in this happiness index for the treatment group (those who are part of the flexible work pilot) relative to the pre/post difference for the control group (those not selected for the flexible work pilot).</p>
<p>While we could simply look at the pre/post differences for the treatment group, we understand from Chapter <a href="research.html#research">5</a> that this would be a weak design that may lead to inaccurate conclusions. There could be alternative explanations for any observed increases in happiness that are unrelated to the intervention itself. For example, between time 1 and time 2, travel frequency may have decreased for everyone, which may contribute to overall happiness. By comparing pre/post differences between the treatment and control groups, we gain more confidence in isolating the effect of the flexible work benefit on happiness since alternative explanations should be reflected in any pre/post changes observed for the control group.</p>
<p><strong>Difference-in-differences (DiD)</strong> estimation is a quasi-experimental approach that originated from econometrics for this same purpose, but it is beyond the scope of this book. Angrist and Pischke (2009) is an excellent resource for learning about these methods.</p>
<p>Let’s prepare the data for this experiment. Since <code>employees</code> is a cross-sectional dataset (single point-in-time), we will generate simulated data for repeated measures (i.e., post-intervention scores).</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="aod.html#cb300-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducible results</span></span>
<span id="cb300-2"><a href="aod.html#cb300-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb300-3"><a href="aod.html#cb300-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-4"><a href="aod.html#cb300-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Derive happiness index from survey variables</span></span>
<span id="cb300-5"><a href="aod.html#cb300-5" aria-hidden="true" tabindex="-1"></a>employees<span class="sc">$</span>happiness_ind <span class="ot">&lt;-</span> (employees<span class="sc">$</span>engagement <span class="sc">+</span> employees<span class="sc">$</span>env_sat <span class="sc">+</span> employees<span class="sc">$</span>job_sat <span class="sc">+</span> employees<span class="sc">$</span>rel_sat) <span class="sc">/</span> <span class="dv">4</span></span>
<span id="cb300-6"><a href="aod.html#cb300-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-7"><a href="aod.html#cb300-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Sample size of frequent travelers</span></span>
<span id="cb300-8"><a href="aod.html#cb300-8" aria-hidden="true" tabindex="-1"></a>n <span class="ot">=</span> <span class="fu">nrow</span>(<span class="fu">subset</span>(employees, business_travel <span class="sc">==</span> <span class="st">&#39;Travel_Frequently&#39;</span>, <span class="at">select =</span> employee_id))</span>
<span id="cb300-9"><a href="aod.html#cb300-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-10"><a href="aod.html#cb300-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly assign half of frequent travelers to treatment and control groups</span></span>
<span id="cb300-11"><a href="aod.html#cb300-11" aria-hidden="true" tabindex="-1"></a>treat_ids <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">unlist</span>(<span class="fu">subset</span>(employees, business_travel <span class="sc">==</span> <span class="st">&#39;Travel_Frequently&#39;</span>, <span class="at">select =</span> employee_id)), <span class="fu">floor</span>(n <span class="sc">*</span> .<span class="dv">5</span>))</span>
<span id="cb300-12"><a href="aod.html#cb300-12" aria-hidden="true" tabindex="-1"></a>ctrl_ids <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, business_travel <span class="sc">==</span> <span class="st">&#39;Travel_Frequently&#39;</span> <span class="sc">&amp;</span> <span class="sc">!</span>employee_id <span class="sc">%in%</span> treat_ids, <span class="at">select =</span> employee_id))</span>
<span id="cb300-13"><a href="aod.html#cb300-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-14"><a href="aod.html#cb300-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize dfs for pre/post metrics</span></span>
<span id="cb300-15"><a href="aod.html#cb300-15" aria-hidden="true" tabindex="-1"></a>treat_metrics <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">pre_ind =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(treat_ids)),</span>
<span id="cb300-16"><a href="aod.html#cb300-16" aria-hidden="true" tabindex="-1"></a>                           <span class="at">rand_num =</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(treat_ids), <span class="at">mean =</span> <span class="dv">15</span>, <span class="at">sd =</span> <span class="dv">5</span>) <span class="sc">*</span> .<span class="dv">001</span>,</span>
<span id="cb300-17"><a href="aod.html#cb300-17" aria-hidden="true" tabindex="-1"></a>                           <span class="at">post_ind =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(treat_ids)), </span>
<span id="cb300-18"><a href="aod.html#cb300-18" aria-hidden="true" tabindex="-1"></a>                           <span class="at">diff =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(treat_ids)))</span>
<span id="cb300-19"><a href="aod.html#cb300-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-20"><a href="aod.html#cb300-20" aria-hidden="true" tabindex="-1"></a>ctrl_metrics <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">pre_ind =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(ctrl_ids)), </span>
<span id="cb300-21"><a href="aod.html#cb300-21" aria-hidden="true" tabindex="-1"></a>                          <span class="at">rand_num =</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(ctrl_ids), <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>) <span class="sc">*</span> .<span class="dv">001</span>, </span>
<span id="cb300-22"><a href="aod.html#cb300-22" aria-hidden="true" tabindex="-1"></a>                          <span class="at">post_ind =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(ctrl_ids)),</span>
<span id="cb300-23"><a href="aod.html#cb300-23" aria-hidden="true" tabindex="-1"></a>                          <span class="at">diff =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(ctrl_ids)))</span>
<span id="cb300-24"><a href="aod.html#cb300-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-25"><a href="aod.html#cb300-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Store happiness indices for treatment and control groups</span></span>
<span id="cb300-26"><a href="aod.html#cb300-26" aria-hidden="true" tabindex="-1"></a>treat_metrics<span class="sc">$</span>pre_ind <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, employee_id <span class="sc">%in%</span> treat_ids, <span class="at">select =</span> happiness_ind))</span>
<span id="cb300-27"><a href="aod.html#cb300-27" aria-hidden="true" tabindex="-1"></a>ctrl_metrics<span class="sc">$</span>pre_ind <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, employee_id <span class="sc">%in%</span> ctrl_ids, <span class="at">select =</span> happiness_ind))</span>
<span id="cb300-28"><a href="aod.html#cb300-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-29"><a href="aod.html#cb300-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Create vectors with artificially inflated post-intervention happiness indices</span></span>
<span id="cb300-30"><a href="aod.html#cb300-30" aria-hidden="true" tabindex="-1"></a>treat_metrics<span class="sc">$</span>post_ind <span class="ot">&lt;-</span> treat_metrics<span class="sc">$</span>pre_ind <span class="sc">+</span> treat_metrics<span class="sc">$</span>rand_num</span>
<span id="cb300-31"><a href="aod.html#cb300-31" aria-hidden="true" tabindex="-1"></a>ctrl_metrics<span class="sc">$</span>post_ind <span class="ot">&lt;-</span> ctrl_metrics<span class="sc">$</span>pre_ind <span class="sc">+</span> ctrl_metrics<span class="sc">$</span>rand_num</span>
<span id="cb300-32"><a href="aod.html#cb300-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb300-33"><a href="aod.html#cb300-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Force an upper bound of 4 to adjusted index scores (variables were measured using a 4-point Likert scale)</span></span>
<span id="cb300-34"><a href="aod.html#cb300-34" aria-hidden="true" tabindex="-1"></a>treat_metrics<span class="sc">$</span>post_ind <span class="ot">&lt;-</span> <span class="cf">if</span>(treat_metrics<span class="sc">$</span>post_ind <span class="sc">&gt;</span> <span class="dv">4</span>) {<span class="dv">4</span>} <span class="cf">else</span> {treat_metrics<span class="sc">$</span>post_ind}</span>
<span id="cb300-35"><a href="aod.html#cb300-35" aria-hidden="true" tabindex="-1"></a>ctrl_metrics<span class="sc">$</span>post_ind <span class="ot">&lt;-</span> <span class="cf">if</span>(ctrl_metrics<span class="sc">$</span>post_ind <span class="sc">&gt;</span> <span class="dv">4</span>) {<span class="dv">4</span>} <span class="cf">else</span> {ctrl_metrics<span class="sc">$</span>post_ind}</span></code></pre></div>
<p>It’s important to remember that a paired samples <span class="math inline">\(t\)</span>-test requires that each of the paired measurements be obtained from the same subject. Therefore, if an employee terms between time 1 and time 2, or does not provide the survey responses needed to calculate the happiness index at both time 1 and time 2, the employee should be removed from the data since paired measurements will not be available.</p>
<p>The variance is not assumed to be equal for a paired test; therefore, the homogeneity of variance assumption is not applicable in this context.</p>
<p>Next, we will evaluate whether paired differences are normally distributed using the Shapiro Wilk test. While individual survey items are measured on an ordinal scale, our derived happiness index is the average of multiple ordinal items and can be considered an <em>approximately</em> continuous variable. There are <span class="math inline">\(2^p - p - 1\)</span> combinations of scores, where <span class="math inline">\(p\)</span> is the number of variables. For our happiness index, there are <span class="math inline">\(2^4 - 4 - 1 = 11\)</span> combinations.</p>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="aod.html#cb301-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb301-2"><a href="aod.html#cb301-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggpubr)</span>
<span id="cb301-3"><a href="aod.html#cb301-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-4"><a href="aod.html#cb301-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate pre/post differences</span></span>
<span id="cb301-5"><a href="aod.html#cb301-5" aria-hidden="true" tabindex="-1"></a>treat_metrics<span class="sc">$</span>diff <span class="ot">&lt;-</span> treat_metrics<span class="sc">$</span>post_ind <span class="sc">-</span> treat_metrics<span class="sc">$</span>pre_ind</span>
<span id="cb301-6"><a href="aod.html#cb301-6" aria-hidden="true" tabindex="-1"></a>ctrl_metrics<span class="sc">$</span>diff <span class="ot">&lt;-</span> ctrl_metrics<span class="sc">$</span>post_ind <span class="sc">-</span> ctrl_metrics<span class="sc">$</span>pre_ind</span>
<span id="cb301-7"><a href="aod.html#cb301-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-8"><a href="aod.html#cb301-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Histogram for distribution of pre/post treatment group differences</span></span>
<span id="cb301-9"><a href="aod.html#cb301-9" aria-hidden="true" tabindex="-1"></a>p_treat <span class="ot">&lt;-</span> ggplot2<span class="sc">::</span><span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb301-10"><a href="aod.html#cb301-10" aria-hidden="true" tabindex="-1"></a>           ggplot2<span class="sc">::</span><span class="fu">aes</span>(treat_metrics<span class="sc">$</span>diff) <span class="sc">+</span> </span>
<span id="cb301-11"><a href="aod.html#cb301-11" aria-hidden="true" tabindex="-1"></a>           ggplot2<span class="sc">::</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Treatment Group&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Happiness Index Differences&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span>) <span class="sc">+</span> </span>
<span id="cb301-12"><a href="aod.html#cb301-12" aria-hidden="true" tabindex="-1"></a>           ggplot2<span class="sc">::</span><span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="sc">+</span></span>
<span id="cb301-13"><a href="aod.html#cb301-13" aria-hidden="true" tabindex="-1"></a>           ggplot2<span class="sc">::</span><span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb301-14"><a href="aod.html#cb301-14" aria-hidden="true" tabindex="-1"></a>           ggplot2<span class="sc">::</span><span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span>
<span id="cb301-15"><a href="aod.html#cb301-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-16"><a href="aod.html#cb301-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Histogram for distribution of pre/post control group differences</span></span>
<span id="cb301-17"><a href="aod.html#cb301-17" aria-hidden="true" tabindex="-1"></a>p_ctrl <span class="ot">&lt;-</span> ggplot2<span class="sc">::</span><span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb301-18"><a href="aod.html#cb301-18" aria-hidden="true" tabindex="-1"></a>          ggplot2<span class="sc">::</span><span class="fu">aes</span>(ctrl_metrics<span class="sc">$</span>diff) <span class="sc">+</span> </span>
<span id="cb301-19"><a href="aod.html#cb301-19" aria-hidden="true" tabindex="-1"></a>          ggplot2<span class="sc">::</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Control Group&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Happiness Index Differences&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span>) <span class="sc">+</span> </span>
<span id="cb301-20"><a href="aod.html#cb301-20" aria-hidden="true" tabindex="-1"></a>          ggplot2<span class="sc">::</span><span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="sc">+</span></span>
<span id="cb301-21"><a href="aod.html#cb301-21" aria-hidden="true" tabindex="-1"></a>          ggplot2<span class="sc">::</span><span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb301-22"><a href="aod.html#cb301-22" aria-hidden="true" tabindex="-1"></a>          ggplot2<span class="sc">::</span><span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span>
<span id="cb301-23"><a href="aod.html#cb301-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb301-24"><a href="aod.html#cb301-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Display histograms side-by-side</span></span>
<span id="cb301-25"><a href="aod.html#cb301-25" aria-hidden="true" tabindex="-1"></a>ggpubr<span class="sc">::</span><span class="fu">ggarrange</span>(p_treat, p_ctrl, <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pre-post-diff"></span>
<img src="Intro_People_Analytics_files/figure-html/pre-post-diff-1.png" alt="Pre/Post Differences for Treatment and Control Groups" width="672" />
<p class="caption">
Figure 9.8: Pre/Post Differences for Treatment and Control Groups
</p>
</div>
<p>Based on a visual inspection, the distributions of differences appear to be roughly normal. This should not be surprising given random values were sampled from normal distributions to derive artificial post-intervention happiness indices.</p>
<p>Let’s test for normality by performing the Shapiro-Wilk test on vectors of differences:</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="aod.html#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Shapiro-Wilk test of normality</span></span>
<span id="cb302-2"><a href="aod.html#cb302-2" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(treat_metrics<span class="sc">$</span>diff)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  treat_metrics$diff
## W = 0.98936, p-value = 0.3738</code></pre>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="aod.html#cb304-1" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(ctrl_metrics<span class="sc">$</span>diff)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  ctrl_metrics$diff
## W = 0.99096, p-value = 0.5134</code></pre>
<p>Since <span class="math inline">\(p\)</span> &gt; .05 for both tests, the assumption of normally distributed differences is met. Given the data generative process implemented for this example, differences would become increasingly normal as the sample size increases. We now have the greenlight to perform the paired samples <span class="math inline">\(t\)</span>-test.</p>
<p>We can run a paired samples <span class="math inline">\(t\)</span>-test in R by passing <code>paired = TRUE</code> as an argument to the same <code>t.test()</code> function used for the independent samples <span class="math inline">\(t\)</span>-test. Since we are investigating whether the average post-intervention happiness index is significantly <em>greater</em> than the average pre-intervention happiness index, we also need the <code>alternative = "greater"</code> argument since the default two-tailed test only evaluates whether the average indices are significantly different (regardless of whether the pre- or post-intervention index is larger).</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="aod.html#cb306-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform one-tailed paired samples t-test for treatment group</span></span>
<span id="cb306-2"><a href="aod.html#cb306-2" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(treat_metrics<span class="sc">$</span>post_ind, treat_metrics<span class="sc">$</span>pre_ind, <span class="at">paired =</span> <span class="cn">TRUE</span>, <span class="at">alternative =</span> <span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Paired t-test
## 
## data:  treat_metrics$post_ind and treat_metrics$pre_ind
## t = 35.906, df = 137, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  0.01490482        Inf
## sample estimates:
## mean of the differences 
##              0.01562551</code></pre>
<p>These results indicate that the post-intervention happiness index is significantly larger than the pre-intervention happiness index. This is encouraging with respect to the potential efficacy of the flexible work pilot, but the question about whether the control group experienced a commensurate improvement over the observation period remains unanswered.</p>
<p>Let’s run the same paired samples <span class="math inline">\(t\)</span>-test using the control group indices:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="aod.html#cb308-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform one-tailed paired samples t-test for control group</span></span>
<span id="cb308-2"><a href="aod.html#cb308-2" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(ctrl_metrics<span class="sc">$</span>post_ind, ctrl_metrics<span class="sc">$</span>pre_ind, <span class="at">paired =</span> <span class="cn">TRUE</span>, <span class="at">alternative =</span> <span class="st">&quot;greater&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Paired t-test
## 
## data:  ctrl_metrics$post_ind and ctrl_metrics$pre_ind
## t = 0.59995, df = 138, p-value = 0.2748
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -8.719262e-05           Inf
## sample estimates:
## mean of the differences 
##            4.953658e-05</code></pre>
<p>Since <span class="math inline">\(p\)</span> &gt; .05, we can conclude that there was not a significant increase in happiness indices for the control group, which provides additional – but not conclusive – support for the effectiveness of the flexible work benefit. Chapter <a href="lm.html#lm">10</a> will introduce linear regression, which is a powerful modeling tool for people analytics that helps control for multiple alternative explanations of associations with the DV in order to isolate the unique effects of each IV.</p>
<p>We can evaluate the magnitude of mean differences for these paired samples by passing the <code>paired = TRUE</code> argument to the same <code>cohen.d()</code> function used for independent samples:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="aod.html#cb310-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Cohen&#39;s d</span></span>
<span id="cb310-2"><a href="aod.html#cb310-2" aria-hidden="true" tabindex="-1"></a>effsize<span class="sc">::</span><span class="fu">cohen.d</span>(treat_metrics<span class="sc">$</span>post_ind, treat_metrics<span class="sc">$</span>pre_ind, <span class="at">paired =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## Cohen&#39;s d
## 
## d estimate: 0.03132117 (negligible)
## 95 percent confidence interval:
##      lower      upper 
## 0.02960345 0.03303890</code></pre>
<p>Though pre/post indices are statistically significant for the treatment group, the size of the difference is negligible (<span class="math inline">\(d\)</span> = .03).</p>
<p><strong>Wilcoxon Signed-Rank Test</strong></p>
<p>The <strong>Wilcoxon Signed-Rank Test</strong> is the nonparametric alternative to the paired samples <span class="math inline">\(t\)</span>-test. This distribution-free test does not require normally distributed differences.</p>
<p>The matched Wilcoxon Signed-Rank test is performed in R using the same <code>wilcox.test()</code> function used to perform the unmatched Wilcoxon Rank-Sum test. Though we can use a paired samples <span class="math inline">\(t\)</span>-test to test differences for our flexible work benefit study since the assumption of normally distributed differences is met, let’s run a Wilcoxon Signed-Rank test for demonstrative purposes:</p>
<div class="sourceCode" id="cb312"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb312-1"><a href="aod.html#cb312-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Wilcoxon Signed-Rank test</span></span>
<span id="cb312-2"><a href="aod.html#cb312-2" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(treat_metrics<span class="sc">$</span>post_ind, treat_metrics<span class="sc">$</span>pre_ind, <span class="at">paired =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  treat_metrics$post_ind and treat_metrics$pre_ind
## V = 9591, p-value &lt; 2.2e-16
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="aod.html#cb314-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Wilcoxon Signed-Rank test</span></span>
<span id="cb314-2"><a href="aod.html#cb314-2" aria-hidden="true" tabindex="-1"></a><span class="fu">wilcox.test</span>(ctrl_metrics<span class="sc">$</span>post_ind, ctrl_metrics<span class="sc">$</span>pre_ind, <span class="at">paired =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test with continuity correction
## 
## data:  ctrl_metrics$post_ind and ctrl_metrics$pre_ind
## V = 5166, p-value = 0.5275
## alternative hypothesis: true location shift is not equal to 0</code></pre>
<p>Consistent with results from the paired samples <span class="math inline">\(t\)</span>-tests, significantly higher post-intervention happiness indices were observed for the treatment group but not for the control group.</p>
<p>We can evaluate the magnitude of differences for these paired samples by passing the <code>paired = TRUE</code> argument to the same <code>cliff.delta()</code> function used for independent samples:</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="aod.html#cb316-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run Cliff&#39;s Delta</span></span>
<span id="cb316-2"><a href="aod.html#cb316-2" aria-hidden="true" tabindex="-1"></a>effsize<span class="sc">::</span><span class="fu">cliff.delta</span>(treat_metrics<span class="sc">$</span>post_ind, treat_metrics<span class="sc">$</span>pre_ind, <span class="at">paired =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## 
## Cliff&#39;s Delta
## 
## delta estimate: 0.1544844 (small)
## 95 percent confidence interval:
##      lower      upper 
## 0.01652557 0.28667094</code></pre>
<p>With Cliff’s delta, we observe a small difference between pre/post indices for the treatment group (delta estimate = .15).</p>
<p><strong>Analysis of Variance (ANOVA)</strong></p>
<p><strong>Analysis of Variance (ANOVA)</strong> is used to determine whether the means of scale-level DVs are equal across nominal-level variables with three or more independent categories.</p>
<p>It is important to understand that <span class="math inline">\(H_0\)</span> in ANOVA not only requires all group means to be equal but their complex contrasts as well. For example, if we have four groups named A, B, C, and D, <span class="math inline">\(H_0\)</span> requires that <span class="math inline">\(\mu_A = \mu_B = \mu_C = \mu_D\)</span> is true as well as the various complex contrasts such as <span class="math inline">\(\mu_{A,B} = \mu_{C,D}\)</span> and <span class="math inline">\(\mu_A = \mu_{B,C,D}\)</span> and <span class="math inline">\(\mu_D = \mu_{B,C}\)</span>. Therefore, a difference between one or more of these contrasts results in a decision to reject <span class="math inline">\(H_0\)</span> in ANOVA. As a result, we may find a significant <span class="math inline">\(F\)</span>-statistic but no significant differences between pairwise means.</p>
<p>In addition, it is possible to find a significant pairwise mean difference but a non-significant result from ANOVA. As you may recall from Chapter <a href="inf-stats.html#inf-stats">8</a>, multiple comparisons reduce the power of statistical tests. Since multiple tests of mean differences are performed with ANOVA, the family-wise error rate is used to adjust for the increased probability of a Type I error across the set of analyses. Since the power of a single pairwise test is greater relative to the power of familywise comparisons, we may find a significant result for the former but not the latter.</p>
<p>ANOVA requires IVs to be categorical (nominal or ordinal) and the DV to be continuous (interval or ratio). A <strong>one-way ANOVA</strong> is used to determine how one categorical IV influences a continuous DV. A <strong>two-way ANOVA</strong> is used to determine how two categorical IVs influence a continuous DV. A <strong>three-way ANOVA</strong> is used to evaluate how three categorical IVs influence a continuous DV. An ANOVA that uses two or more categorical IVs is often referred to as a <strong>factorial ANOVA</strong>. As discussed in Chapter <a href="getting-started.html#getting-started">1</a>, it is important to remain grounded in specific hypotheses, as a significant ANOVA may not actually test what is being hypothesized.</p>
<p>ANOVA is not a test, per se, but a <span class="math inline">\(F\)</span>-test underpins it. The mathematical procedure behind the <span class="math inline">\(F\)</span>-test is relatively straightforward:</p>
<ol style="list-style-type: decimal">
<li>Compute the <strong>within-group variance</strong>, which is also known as residual variance. Simply put, this tells us how different each member of the group is from the average.</li>
<li>Compute the <strong>between-group variance</strong>. This represents how different the group means are from one another.</li>
<li>Produce the <span class="math inline">\(F\)</span>-statistic, which is the ratio of <em>within-group variance</em> to <em>between-group variance</em>.</li>
</ol>
<p>More formally, the <span class="math inline">\(F\)</span>-statistic is defined by:</p>
<p><span class="math display">\[ F = \frac{MS_{between}}{MS_{within}}, \]</span></p>
<p>where:</p>
<p><span class="math display">\[ MS_{between} = \frac{SS_{between}}{df_{between}}, \]</span></p>
<p><span class="math display">\[ MS_{within} = \frac{SS_{within}}{df_{within}}, \]</span></p>
<p><span class="math display">\[ SS_{between} = \displaystyle\sum_{j=1}^{p} n_j(\bar{x}_j-\bar{x})^2, \]</span></p>
<p><span class="math display">\[ SS_{within} = \displaystyle\sum_{j=1}^{p} \displaystyle\sum_{i=1}^{n_j} (x_{ij}-\bar{x_j})^2 \]</span></p>
<p><strong>One-Way ANOVA</strong></p>
<p>To illustrate how to perform a one-way ANOVA, we will test the hypothesis that mean annual compensation is equal across job satisfaction levels.</p>
<p>Each observation in <code>employees</code> represents a unique employee, and a given employee can only have one job satisfaction score and one annual compensation value. The assumption of independence is met since each record exists independent of one another and each job satisfaction group is comprised of different employees.</p>
<p>Levene’s test (Levene, 1960) can be used to test the homogeneity of variance assumption – even with non-normal distributions. This can be performed in R using the <code>leveneTest()</code> function from the <code>car</code> package:</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="aod.html#cb318-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform Levene&#39;s test for homogeneity of variance</span></span>
<span id="cb318-2"><a href="aod.html#cb318-2" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">leveneTest</span>(annual_comp <span class="sc">~</span> <span class="fu">as.factor</span>(job_sat), <span class="at">data =</span> employees)</span></code></pre></div>
<pre><code>## Levene&#39;s Test for Homogeneity of Variance (center = median)
##         Df F value Pr(&gt;F)
## group    3  0.3293 0.8042
##       1466</code></pre>
<p>The test statistic associated with Levene’s test relates to the null hypothesis that there are no significant differences in variances across the job satisfaction levels. Since <span class="math inline">\(p &gt; .05\)</span>, we fail to reject this null hypothesis and can assume equal variances.</p>
<p>Next, let’s test the assumption of normality. It is important to note that the assumption of normality <em>does not</em> apply to the distribution of the DV but to the distribution of residuals for each group of the IV. Residuals in the context of ANOVA represent the difference between the actual values of the continuous DV relative to its mean value for each level of the categorical IV (e.g., <span class="math inline">\(y - \bar{y}_A\)</span>, <span class="math inline">\(y - \bar{y}_B\)</span>, <span class="math inline">\(y - \bar{y}_C\)</span>). In ANOVA, we expect the residuals to be normally distributed around a mean of 0 (the balance point) when the data are normally distributed within each IV category; the more skewed the data, the larger the average distance of each DV value from the mean.</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="aod.html#cb320-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data viz library</span></span>
<span id="cb320-2"><a href="aod.html#cb320-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb320-3"><a href="aod.html#cb320-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggpubr)</span>
<span id="cb320-4"><a href="aod.html#cb320-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-5"><a href="aod.html#cb320-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create function to visualize distribution</span></span>
<span id="cb320-6"><a href="aod.html#cb320-6" aria-hidden="true" tabindex="-1"></a>dist.viz <span class="ot">&lt;-</span> <span class="cf">function</span>(data, x) {</span>
<span id="cb320-7"><a href="aod.html#cb320-7" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb320-8"><a href="aod.html#cb320-8" aria-hidden="true" tabindex="-1"></a>viz <span class="ot">&lt;-</span> ggplot2<span class="sc">::</span><span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb320-9"><a href="aod.html#cb320-9" aria-hidden="true" tabindex="-1"></a>       ggplot2<span class="sc">::</span><span class="fu">aes</span>(data) <span class="sc">+</span> </span>
<span id="cb320-10"><a href="aod.html#cb320-10" aria-hidden="true" tabindex="-1"></a>       ggplot2<span class="sc">::</span><span class="fu">labs</span>(<span class="at">title =</span> <span class="fu">paste</span>(<span class="st">&quot;Job Sat = &quot;</span>, x), <span class="at">x =</span> <span class="st">&quot;Annual Compensation&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span>) <span class="sc">+</span> </span>
<span id="cb320-11"><a href="aod.html#cb320-11" aria-hidden="true" tabindex="-1"></a>       ggplot2<span class="sc">::</span><span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="sc">+</span></span>
<span id="cb320-12"><a href="aod.html#cb320-12" aria-hidden="true" tabindex="-1"></a>       ggplot2<span class="sc">::</span><span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb320-13"><a href="aod.html#cb320-13" aria-hidden="true" tabindex="-1"></a>       ggplot2<span class="sc">::</span><span class="fu">theme</span>(<span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">hjust =</span> <span class="fl">0.5</span>))</span>
<span id="cb320-14"><a href="aod.html#cb320-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-15"><a href="aod.html#cb320-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(viz)</span>
<span id="cb320-16"><a href="aod.html#cb320-16" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb320-17"><a href="aod.html#cb320-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-18"><a href="aod.html#cb320-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce annual compensation vectors for each job satisfaction level</span></span>
<span id="cb320-19"><a href="aod.html#cb320-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Unlist() is needed to convert the default object from subset() into a numeric vector</span></span>
<span id="cb320-20"><a href="aod.html#cb320-20" aria-hidden="true" tabindex="-1"></a>group_1 <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, job_sat <span class="sc">==</span> <span class="dv">1</span>, <span class="at">select =</span> annual_comp))</span>
<span id="cb320-21"><a href="aod.html#cb320-21" aria-hidden="true" tabindex="-1"></a>group_2 <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, job_sat <span class="sc">==</span> <span class="dv">2</span>, <span class="at">select =</span> annual_comp))</span>
<span id="cb320-22"><a href="aod.html#cb320-22" aria-hidden="true" tabindex="-1"></a>group_3 <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, job_sat <span class="sc">==</span> <span class="dv">3</span>, <span class="at">select =</span> annual_comp))</span>
<span id="cb320-23"><a href="aod.html#cb320-23" aria-hidden="true" tabindex="-1"></a>group_4 <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">subset</span>(employees, job_sat <span class="sc">==</span> <span class="dv">4</span>, <span class="at">select =</span> annual_comp))</span>
<span id="cb320-24"><a href="aod.html#cb320-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-25"><a href="aod.html#cb320-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Call UDF to build annual comp histogram for each job satisfaction level</span></span>
<span id="cb320-26"><a href="aod.html#cb320-26" aria-hidden="true" tabindex="-1"></a>viz_1 <span class="ot">&lt;-</span> <span class="fu">dist.viz</span>(<span class="at">data =</span> group_1, <span class="at">x =</span> <span class="dv">1</span>)</span>
<span id="cb320-27"><a href="aod.html#cb320-27" aria-hidden="true" tabindex="-1"></a>viz_2 <span class="ot">&lt;-</span> <span class="fu">dist.viz</span>(<span class="at">data =</span> group_2, <span class="at">x =</span> <span class="dv">2</span>)</span>
<span id="cb320-28"><a href="aod.html#cb320-28" aria-hidden="true" tabindex="-1"></a>viz_3 <span class="ot">&lt;-</span> <span class="fu">dist.viz</span>(<span class="at">data =</span> group_3, <span class="at">x =</span> <span class="dv">3</span>)</span>
<span id="cb320-29"><a href="aod.html#cb320-29" aria-hidden="true" tabindex="-1"></a>viz_4 <span class="ot">&lt;-</span> <span class="fu">dist.viz</span>(<span class="at">data =</span> group_4, <span class="at">x =</span> <span class="dv">4</span>)</span>
<span id="cb320-30"><a href="aod.html#cb320-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb320-31"><a href="aod.html#cb320-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Display distribution visualizations</span></span>
<span id="cb320-32"><a href="aod.html#cb320-32" aria-hidden="true" tabindex="-1"></a>ggpubr<span class="sc">::</span><span class="fu">ggarrange</span>(viz_1, viz_2, viz_3, viz_4,</span>
<span id="cb320-33"><a href="aod.html#cb320-33" aria-hidden="true" tabindex="-1"></a>          <span class="at">ncol =</span> <span class="dv">2</span>, <span class="at">nrow =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:comp-dist"></span>
<img src="Intro_People_Analytics_files/figure-html/comp-dist-1.png" alt="Annual Compensation Distribution by Job Satisfaction Level" width="672" />
<p class="caption">
Figure 9.9: Annual Compensation Distribution by Job Satisfaction Level
</p>
</div>
<p>As we can see, annual compensation data are not normally distributed within job satisfaction groups. Therefore, we would not expect the distribution of residuals to be normally distributed within these groups either.</p>
<p>To test whether the assumption of normality is met, we will first produce and review a <strong>quantile-quantile (Q-Q) plot</strong>. A Q-Q plot compares two probability distributions by plotting their quantiles (data partitioned into equal-sized groups) against each other. After partitioning annual compensation into groups differentiated by job satisfaction level, we can use the <code>ggqqplot()</code> function from the <code>ggpubr</code> library to build a Q-Q plot and evaluate the distribution of residuals.</p>
<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb321-1"><a href="aod.html#cb321-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate residuals for each group</span></span>
<span id="cb321-2"><a href="aod.html#cb321-2" aria-hidden="true" tabindex="-1"></a>residuals <span class="ot">&lt;-</span> <span class="fu">c</span>(group_1 <span class="sc">-</span> <span class="fu">mean</span>(group_1), group_2 <span class="sc">-</span> <span class="fu">mean</span>(group_2), group_3 <span class="sc">-</span> <span class="fu">mean</span>(group_3), group_4 <span class="sc">-</span> <span class="fu">mean</span>(group_4))</span>
<span id="cb321-3"><a href="aod.html#cb321-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb321-4"><a href="aod.html#cb321-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a Q-Q plot of residuals</span></span>
<span id="cb321-5"><a href="aod.html#cb321-5" aria-hidden="true" tabindex="-1"></a>ggpubr<span class="sc">::</span><span class="fu">ggqqplot</span>(residuals)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qq-plot"></span>
<img src="Intro_People_Analytics_files/figure-html/qq-plot-1.png" alt="Q-Q Plot of Annual Compensation Residuals" width="672" />
<p class="caption">
Figure 9.10: Q-Q Plot of Annual Compensation Residuals
</p>
</div>
<p>To satisfy the assumption of normality, residuals must lie along the linear line. Based on the Q-Q plot in Figure <a href="aod.html#fig:qq-plot">9.10</a>, there is a clear departure from normality at both ends of the theoretical range.</p>
<p>Let’s test for normality using the Shapiro-Wilk test:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="aod.html#cb322-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Shapiro-Wilk test of normality</span></span>
<span id="cb322-2"><a href="aod.html#cb322-2" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(residuals)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals
## W = 0.95874, p-value &lt; 2.2e-16</code></pre>
<p>Since <span class="math inline">\(p &lt; .05\)</span>, we reject the null hypothesis of normally distributed data, which indicates that the assumption of normality is violated. This should not be surprising based on the deviation from normality we observed in Figure <a href="aod.html#fig:qq-plot">9.10</a>.</p>
<p>Because the assumption of normality is violated, we have two options. First, we can attempt to transform the data so that the residuals using the transformed values are normally distributed. If the data are resistant to transformation, we can leverage a nonparametric alternative to ANOVA.</p>
<p>Let’s first try several common data transformations and then examine the resulting Q-Q plots:</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="aod.html#cb324-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a linear model using the natural logarithm of annual comp</span></span>
<span id="cb324-2"><a href="aod.html#cb324-2" aria-hidden="true" tabindex="-1"></a>ln.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(annual_comp) <span class="sc">~</span> job_sat, <span class="at">data =</span> employees)</span>
<span id="cb324-3"><a href="aod.html#cb324-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb324-4"><a href="aod.html#cb324-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a linear model using the log base 10 of annual comp</span></span>
<span id="cb324-5"><a href="aod.html#cb324-5" aria-hidden="true" tabindex="-1"></a>log10.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log10</span>(annual_comp) <span class="sc">~</span> job_sat, <span class="at">data =</span> employees)</span>
<span id="cb324-6"><a href="aod.html#cb324-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb324-7"><a href="aod.html#cb324-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Build a linear model using the square root of annual comp</span></span>
<span id="cb324-8"><a href="aod.html#cb324-8" aria-hidden="true" tabindex="-1"></a>sqrt.model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(annual_comp) <span class="sc">~</span> job_sat, <span class="at">data =</span> employees)</span>
<span id="cb324-9"><a href="aod.html#cb324-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb324-10"><a href="aod.html#cb324-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Store Q-Q plots to viz objects</span></span>
<span id="cb324-11"><a href="aod.html#cb324-11" aria-hidden="true" tabindex="-1"></a>ln.viz <span class="ot">&lt;-</span> ggpubr<span class="sc">::</span><span class="fu">ggqqplot</span>(<span class="fu">residuals</span>(ln.model)) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Natural Log&quot;</span>)</span>
<span id="cb324-12"><a href="aod.html#cb324-12" aria-hidden="true" tabindex="-1"></a>log10.viz <span class="ot">&lt;-</span> ggpubr<span class="sc">::</span><span class="fu">ggqqplot</span>(<span class="fu">residuals</span>(log10.model)) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Log Base 10&quot;</span>)</span>
<span id="cb324-13"><a href="aod.html#cb324-13" aria-hidden="true" tabindex="-1"></a>sqrt.viz <span class="ot">&lt;-</span> ggpubr<span class="sc">::</span><span class="fu">ggqqplot</span>(<span class="fu">residuals</span>(sqrt.model)) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">&quot;Square Root&quot;</span>)</span>
<span id="cb324-14"><a href="aod.html#cb324-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb324-15"><a href="aod.html#cb324-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Display Q-Q plots of residuals</span></span>
<span id="cb324-16"><a href="aod.html#cb324-16" aria-hidden="true" tabindex="-1"></a>ggpubr<span class="sc">::</span><span class="fu">ggarrange</span>(ln.viz, log10.viz, sqrt.viz,</span>
<span id="cb324-17"><a href="aod.html#cb324-17" aria-hidden="true" tabindex="-1"></a>          <span class="at">ncol =</span> <span class="dv">3</span>, <span class="at">nrow =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qq-plots-trans"></span>
<img src="Intro_People_Analytics_files/figure-html/qq-plots-trans-1.png" alt="Q-Q Plots of Transformed Annual Compensation Residuals" width="672" />
<p class="caption">
Figure 9.11: Q-Q Plots of Transformed Annual Compensation Residuals
</p>
</div>
<p>Even with these transformations, there is still a clear S-shaped curve about the residuals. Though we cannot proceed with ANOVA due to violated assumptions, the implementation of ANOVA is demonstrated below. Performing ANOVA involves the <code>aov()</code> function along with the <code>summary()</code> function to display model output:</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="aod.html#cb325-1" aria-hidden="true" tabindex="-1"></a><span class="co"># One-way ANOVA investigating mean differences in annual comp by job satisfaction</span></span>
<span id="cb325-2"><a href="aod.html#cb325-2" aria-hidden="true" tabindex="-1"></a>one.way <span class="ot">&lt;-</span> <span class="fu">aov</span>(annual_comp <span class="sc">~</span> job_sat, <span class="at">data =</span> employees)</span>
<span id="cb325-3"><a href="aod.html#cb325-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(one.way)</span></code></pre></div>
<pre><code>##               Df    Sum Sq   Mean Sq F value  Pr(&gt;F)   
## job_sat        1 1.337e+10 1.337e+10   7.508 0.00622 **
## Residuals   1468 2.613e+12 1.780e+09                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <strong>Kruskal Wallis H Test</strong> is the nonparametric alternative to a one-way ANOVA (Daniel, 1990) and an appropriate alternative for investigating median differences in annual comp by job satisfaction in our data. This test can be performed using the <code>kruskal.test()</code> function in R:</p>
<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb327-1"><a href="aod.html#cb327-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Nonparametric Kruskal one-way ANOVA</span></span>
<span id="cb327-2"><a href="aod.html#cb327-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kruskal.test</span>(annual_comp <span class="sc">~</span> job_sat, <span class="at">data =</span> employees)</span></code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  annual_comp by job_sat
## Kruskal-Wallis chi-squared = 8.3242, df = 3, p-value = 0.03977</code></pre>
<p>Since <span class="math inline">\(p &lt; .05\)</span>, we can conclude that there are significant differences in median compensation across the groups. However, this test does not indicate which groups are different. We can utilize the <code>pairwise.wilcox.test()</code> function to compute pairwise Wilcoxon rank-sum tests to identify where differences exist:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="aod.html#cb329-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pairwise.wilcox.test</span>(employees<span class="sc">$</span>annual_comp, employees<span class="sc">$</span>job_sat, <span class="at">p.adjust.method =</span> <span class="st">&quot;BH&quot;</span>)</span></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using Wilcoxon rank sum test with continuity correction 
## 
## data:  employees$annual_comp and employees$job_sat 
## 
##   1     2     3    
## 2 0.298 -     -    
## 3 0.041 0.298 -    
## 4 0.041 0.298 0.879
## 
## P value adjustment method: BH</code></pre>
<p>Based on the results, there are significant pairwise differences in median annual compensation for job satisfaction levels 3 and 4 relative to level 1.</p>
<p><strong>Factorial ANOVA</strong></p>
<p><strong>Factorial ANOVA</strong> is any ANOVA which uses two or more categorical IVs, such as a two-way or three-way ANOVA. It is important to understand the hypothesis a factorial design tests. The following output reflects the cross-tabulation of average annual compensation for each combination of two factors – job satisfaction and stock option level.</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="aod.html#cb331-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate mean for each IV pair</span></span>
<span id="cb331-2"><a href="aod.html#cb331-2" aria-hidden="true" tabindex="-1"></a>combos <span class="ot">&lt;-</span> <span class="fu">aggregate</span>(annual_comp <span class="sc">~</span> job_sat <span class="sc">+</span> stock_opt_lvl, employees, mean)</span>
<span id="cb331-3"><a href="aod.html#cb331-3" aria-hidden="true" tabindex="-1"></a>combos</span></code></pre></div>
<pre><code>##    job_sat stock_opt_lvl annual_comp
## 1        1             0    141254.0
## 2        2             0    138753.3
## 3        3             0    132159.2
## 4        4             0    132227.0
## 5        1             1    141763.9
## 6        2             1    135494.5
## 7        3             1    135235.7
## 8        4             1    135569.0
## 9        1             2    146240.0
## 10       2             2    146432.0
## 11       3             2    145080.0
## 12       4             2    143019.3
## 13       1             3    154844.4
## 14       2             3    144254.1
## 15       3             3    135672.7
## 16       4             3    127102.9</code></pre>
<p>As we have already discussed, a difference between one or more of these contrasts may result in a decision to reject <span class="math inline">\(H_0\)</span> in ANOVA. We may also find a significant pairwise difference but a non-significant result from ANOVA since the family-wise error rate adjustment is applied in the context of multiple comparisons which reduces statistical power.</p>
<p>Factorial ANOVA can be performed by adding variables with <code>+</code> within the same <code>aov()</code> function used for one-way ANOVA:</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="aod.html#cb333-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Factorial ANOVA investigating mean differences in annual comp by job satisfaction and stock option level</span></span>
<span id="cb333-2"><a href="aod.html#cb333-2" aria-hidden="true" tabindex="-1"></a>factorial <span class="ot">&lt;-</span> <span class="fu">aov</span>(annual_comp <span class="sc">~</span> job_sat <span class="sc">+</span> stock_opt_lvl, <span class="at">data =</span> employees)</span>
<span id="cb333-3"><a href="aod.html#cb333-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(factorial)</span></code></pre></div>
<pre><code>##                 Df    Sum Sq   Mean Sq F value  Pr(&gt;F)   
## job_sat          1 1.337e+10 1.337e+10   7.523 0.00617 **
## stock_opt_lvl    1 6.840e+09 6.840e+09   3.850 0.04995 * 
## Residuals     1467 2.606e+12 1.777e+09                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>While mean annual compensation is significantly different by both job satisfaction and stock option level factors, this output alone is not too helpful in understanding the nature of the differences. These statistical significance markers indicate that there are meaningful differences that warrant a deeper understanding. Relationships of job satisfaction and stock option level with annual compensation are illustrated more effectively in Figure <a href="aod.html#fig:two-way-factorial">9.12</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:two-way-factorial"></span>
<img src="Intro_People_Analytics_files/figure-html/two-way-factorial-1.png" alt="Relationships of Job Satisfaction and Stock Option Level with Annual Compensation" width="100%" />
<p class="caption">
Figure 9.12: Relationships of Job Satisfaction and Stock Option Level with Annual Compensation
</p>
</div>
<p>As we can see, there is a strong negative relationship between job satisfaction and average annual compensation among employees with the highest stock option level (3). The relationship between job satisfaction and average annual compensation appears to be negative for employees with other stock option levels as well, albeit much weaker.</p>
<p>These relationships may initially seem counterintuitive, as one might expect higher levels of job satisfaction to contribute to higher performance and consequently, higher compensation. There may be other variables that happen to be correlated with job satisfaction and/or stock option level that are the actual determinants of annual compensation. For example, there may be a relationship between jobs that each feature a stock option level of 3 but for which there are markedly different average job satisfaction scores and annual compensation among workers in said jobs. Without accounting for additional variables that may explain why employees vary in the amount of annual compensation they earn, the limited set of relationships shown in Figure <a href="aod.html#fig:two-way-factorial">9.12</a> may lead to a misleading understanding and inaccurate conclusions.</p>
<p>Three-way factorials (and beyond) become difficult to visualize and understand in the way one-way ANOVA and two-way factorials have been explained in this chapter. In Chapter <a href="lm.html#lm">10</a>, we will discuss how to create linear combinations of many IVs and parse the output to understand how they independently and jointly explain variation in the DV.</p>
</div>
<div id="review-questions-7" class="section level2" number="9.4">
<h2><span class="header-section-number">9.4</span> Review Questions</h2>
<ol style="list-style-type: decimal">
<li><p>What are the main differences between a Chi-square test and Fisher’s exact test?</p></li>
<li><p>Why is it problematic to test for significant differences using the <span class="math inline">\({\chi}^2\)</span> statistic with extremely small samples (e.g., <span class="math inline">\(n\)</span> &lt; 5)?</p></li>
<li><p>What are the general assumptions of parametric tests?</p></li>
<li><p>What is a benefit of Welch’s <span class="math inline">\(t\)</span>-test over the Student’s <span class="math inline">\(t\)</span>-test?</p></li>
<li><p>How does a paired-samples <span class="math inline">\(t\)</span>-test differ from an independent samples <span class="math inline">\(t\)</span>-test?</p></li>
<li><p>In what ways does the Wilcoxon signed-rank test differ from the paired samples <span class="math inline">\(t\)</span>-test?</p></li>
<li><p>How can the magnitude of differences (i.e., practical significance) be quantified when working with data measured on a continuous scale?</p></li>
<li><p>How can the magnitude of differences (i.e., practical significance) be quantified when working with data measured on an ordinal scale?</p></li>
<li><p>What null hypothesis does ANOVA test?</p></li>
<li><p>What are some ways to better understand the nature of statistical differences indicated in the output of ANOVA?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inf-stats.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Intro_People_Analytics.pdf", "Intro_People_Analytics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
