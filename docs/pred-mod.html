<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13 Predictive Modeling | The Fundamentals of People Analytics: With Applications in R</title>
  <meta name="description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="13 Predictive Modeling | The Fundamentals of People Analytics: With Applications in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="github-repo" content="crstarbuck/peopleanalytics-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13 Predictive Modeling | The Fundamentals of People Analytics: With Applications in R" />
  
  <meta name="twitter:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  

<meta name="author" content="Craig Starbuck" />


<meta name="date" content="2022-09-21" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="log.html"/>
<link rel="next" href="unsup-lrn.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Fundamentals of People Analytics: With Applications in R</a></li>

<li class="divider"></li>
<li><a href="dedication.html#dedication" id="toc-dedication">Dedication</a></li>
<li><a href="foreword.html#foreword" id="toc-foreword">Foreword</a></li>
<li><a href="preface.html#preface" id="toc-preface">Preface</a></li>
<li><a href="getting-started.html#getting-started" id="toc-getting-started"><span class="toc-section-number">1</span> Getting Started</a>
<ul>
<li><a href="getting-started.html#guiding-principles" id="toc-guiding-principles"><span class="toc-section-number">1.1</span> Guiding Principles</a>
<ul>
<li><a href="getting-started.html#pro-employee-thinking" id="toc-pro-employee-thinking"><span class="toc-section-number">1.1.1</span> Pro-Employee Thinking</a></li>
<li><a href="getting-started.html#quality" id="toc-quality"><span class="toc-section-number">1.1.2</span> Quality</a></li>
<li><a href="getting-started.html#prioritization" id="toc-prioritization"><span class="toc-section-number">1.1.3</span> Prioritization</a></li>
</ul></li>
<li><a href="getting-started.html#tooling" id="toc-tooling"><span class="toc-section-number">1.2</span> Tooling</a></li>
<li><a href="getting-started.html#data-sets" id="toc-data-sets"><span class="toc-section-number">1.3</span> Data Sets</a>
<ul>
<li><a href="getting-started.html#employees" id="toc-employees"><span class="toc-section-number">1.3.1</span> Employees</a></li>
<li><a href="getting-started.html#turnover-trends" id="toc-turnover-trends"><span class="toc-section-number">1.3.2</span> Turnover Trends</a></li>
<li><a href="getting-started.html#survey-responses" id="toc-survey-responses"><span class="toc-section-number">1.3.3</span> Survey Responses</a></li>
</ul></li>
<li><a href="getting-started.html#d-framework" id="toc-d-framework"><span class="toc-section-number">1.4</span> 4D Framework</a></li>
</ul></li>
<li><a href="r-intro.html#r-intro" id="toc-r-intro"><span class="toc-section-number">2</span> Introduction to R</a>
<ul>
<li><a href="r-intro.html#getting-started-1" id="toc-getting-started-1"><span class="toc-section-number">2.1</span> Getting Started</a>
<ul>
<li><a href="r-intro.html#installing-r" id="toc-installing-r"><span class="toc-section-number">2.1.1</span> Installing R</a></li>
<li><a href="r-intro.html#installing-r-studio" id="toc-installing-r-studio"><span class="toc-section-number">2.1.2</span> Installing R Studio</a></li>
<li><a href="r-intro.html#installing-packages" id="toc-installing-packages"><span class="toc-section-number">2.1.3</span> Installing Packages</a></li>
<li><a href="r-intro.html#case-sensitivity" id="toc-case-sensitivity"><span class="toc-section-number">2.1.4</span> Case Sensitivity</a></li>
<li><a href="r-intro.html#help" id="toc-help"><span class="toc-section-number">2.1.5</span> Help</a></li>
<li><a href="r-intro.html#objects" id="toc-objects"><span class="toc-section-number">2.1.6</span> Objects</a></li>
<li><a href="r-intro.html#comments" id="toc-comments"><span class="toc-section-number">2.1.7</span> Comments</a></li>
<li><a href="r-intro.html#testing-early-and-often" id="toc-testing-early-and-often"><span class="toc-section-number">2.1.8</span> Testing Early and Often</a></li>
</ul></li>
<li><a href="r-intro.html#vectors" id="toc-vectors"><span class="toc-section-number">2.2</span> Vectors</a></li>
<li><a href="r-intro.html#matrices" id="toc-matrices"><span class="toc-section-number">2.3</span> Matrices</a></li>
<li><a href="r-intro.html#factors" id="toc-factors"><span class="toc-section-number">2.4</span> Factors</a></li>
<li><a href="r-intro.html#data-frames" id="toc-data-frames"><span class="toc-section-number">2.5</span> Data Frames</a></li>
<li><a href="r-intro.html#lists" id="toc-lists"><span class="toc-section-number">2.6</span> Lists</a></li>
<li><a href="r-intro.html#loops" id="toc-loops"><span class="toc-section-number">2.7</span> Loops</a></li>
<li><a href="r-intro.html#user-defined-functions-udfs" id="toc-user-defined-functions-udfs"><span class="toc-section-number">2.8</span> User-Defined Functions (UDFs)</a></li>
<li><a href="r-intro.html#graphics" id="toc-graphics"><span class="toc-section-number">2.9</span> Graphics</a></li>
<li><a href="r-intro.html#review-questions" id="toc-review-questions"><span class="toc-section-number">2.10</span> Review Questions</a></li>
</ul></li>
<li><a href="sql-intro.html#sql-intro" id="toc-sql-intro"><span class="toc-section-number">3</span> Introduction to SQL</a>
<ul>
<li><a href="sql-intro.html#basics" id="toc-basics"><span class="toc-section-number">3.1</span> Basics</a></li>
<li><a href="sql-intro.html#aggregate-functions" id="toc-aggregate-functions"><span class="toc-section-number">3.2</span> Aggregate Functions</a></li>
<li><a href="sql-intro.html#joins" id="toc-joins"><span class="toc-section-number">3.3</span> Joins</a></li>
<li><a href="sql-intro.html#subqueries" id="toc-subqueries"><span class="toc-section-number">3.4</span> Subqueries</a></li>
<li><a href="sql-intro.html#virtual-tables" id="toc-virtual-tables"><span class="toc-section-number">3.5</span> Virtual Tables</a></li>
<li><a href="sql-intro.html#window-functions" id="toc-window-functions"><span class="toc-section-number">3.6</span> Window Functions</a></li>
<li><a href="sql-intro.html#common-table-expressions-ctes" id="toc-common-table-expressions-ctes"><span class="toc-section-number">3.7</span> Common Table Expressions (CTEs)</a></li>
<li><a href="sql-intro.html#review-questions-1" id="toc-review-questions-1"><span class="toc-section-number">3.8</span> Review Questions</a></li>
</ul></li>
<li><a href="research.html#research" id="toc-research"><span class="toc-section-number">4</span> Research Design</a>
<ul>
<li><a href="research.html#research-questions" id="toc-research-questions"><span class="toc-section-number">4.1</span> Research Questions</a></li>
<li><a href="research.html#research-hypotheses" id="toc-research-hypotheses"><span class="toc-section-number">4.2</span> Research Hypotheses</a></li>
<li><a href="research.html#internal-and-external-validity" id="toc-internal-and-external-validity"><span class="toc-section-number">4.3</span> Internal and External Validity</a></li>
<li><a href="research.html#research-methods" id="toc-research-methods"><span class="toc-section-number">4.4</span> Research Methods</a></li>
<li><a href="research.html#research-designs" id="toc-research-designs"><span class="toc-section-number">4.5</span> Research Designs</a></li>
<li><a href="research.html#review-questions-2" id="toc-review-questions-2"><span class="toc-section-number">4.6</span> Review Questions</a></li>
</ul></li>
<li><a href="measure-sampl.html#measure-sampl" id="toc-measure-sampl"><span class="toc-section-number">5</span> Measurement &amp; Sampling</a>
<ul>
<li><a href="measure-sampl.html#variable-types" id="toc-variable-types"><span class="toc-section-number">5.1</span> Variable Types</a>
<ul>
<li><a href="measure-sampl.html#independent-variables-iv" id="toc-independent-variables-iv"><span class="toc-section-number">5.1.1</span> Independent Variables (IV)</a></li>
<li><a href="measure-sampl.html#dependent-variables-dv" id="toc-dependent-variables-dv"><span class="toc-section-number">5.1.2</span> Dependent Variables (DV)</a></li>
<li><a href="measure-sampl.html#control-variables-cv" id="toc-control-variables-cv"><span class="toc-section-number">5.1.3</span> Control Variables (CV)</a></li>
<li><a href="measure-sampl.html#moderating-variables" id="toc-moderating-variables"><span class="toc-section-number">5.1.4</span> Moderating Variables</a></li>
<li><a href="measure-sampl.html#mediating-variables" id="toc-mediating-variables"><span class="toc-section-number">5.1.5</span> Mediating Variables</a></li>
<li><a href="measure-sampl.html#endogenous-vs.-exogenous-variables" id="toc-endogenous-vs.-exogenous-variables"><span class="toc-section-number">5.1.6</span> Endogenous vs. Exogenous Variables</a></li>
</ul></li>
<li><a href="measure-sampl.html#measurement-scales" id="toc-measurement-scales"><span class="toc-section-number">5.2</span> Measurement Scales</a>
<ul>
<li><a href="measure-sampl.html#discrete-variables" id="toc-discrete-variables"><span class="toc-section-number">5.2.1</span> Discrete Variables</a></li>
<li><a href="measure-sampl.html#continuous-variables" id="toc-continuous-variables"><span class="toc-section-number">5.2.2</span> Continuous Variables</a></li>
</ul></li>
<li><a href="measure-sampl.html#sampling-methods" id="toc-sampling-methods"><span class="toc-section-number">5.3</span> Sampling Methods</a>
<ul>
<li><a href="measure-sampl.html#probability-sampling" id="toc-probability-sampling"><span class="toc-section-number">5.3.1</span> Probability Sampling</a></li>
<li><a href="measure-sampl.html#non-probability-sampling" id="toc-non-probability-sampling"><span class="toc-section-number">5.3.2</span> Non-Probability Sampling</a></li>
</ul></li>
<li><a href="measure-sampl.html#sampling-nonsampling-error" id="toc-sampling-nonsampling-error"><span class="toc-section-number">5.4</span> Sampling &amp; Nonsampling Error</a>
<ul>
<li><a href="measure-sampl.html#sampling-error" id="toc-sampling-error"><span class="toc-section-number">5.4.1</span> Sampling Error</a></li>
<li><a href="measure-sampl.html#nonsampling-error" id="toc-nonsampling-error"><span class="toc-section-number">5.4.2</span> Nonsampling Error</a></li>
</ul></li>
<li><a href="measure-sampl.html#scale-reliability-and-validity" id="toc-scale-reliability-and-validity"><span class="toc-section-number">5.5</span> Scale Reliability and Validity</a>
<ul>
<li><a href="measure-sampl.html#reliability" id="toc-reliability"><span class="toc-section-number">5.5.1</span> Reliability</a></li>
<li><a href="measure-sampl.html#validity" id="toc-validity"><span class="toc-section-number">5.5.2</span> Validity</a></li>
</ul></li>
<li><a href="measure-sampl.html#review-questions-3" id="toc-review-questions-3"><span class="toc-section-number">5.6</span> Review Questions</a></li>
</ul></li>
<li><a href="data-prep.html#data-prep" id="toc-data-prep"><span class="toc-section-number">6</span> Data Preparation</a>
<ul>
<li><a href="data-prep.html#data-extraction" id="toc-data-extraction"><span class="toc-section-number">6.1</span> Data Extraction</a>
<ul>
<li><a href="data-prep.html#data-architecture" id="toc-data-architecture"><span class="toc-section-number">6.1.1</span> Data Architecture</a></li>
</ul></li>
<li><a href="data-prep.html#data-screening-cleaning" id="toc-data-screening-cleaning"><span class="toc-section-number">6.2</span> Data Screening &amp; Cleaning</a>
<ul>
<li><a href="data-prep.html#missingness" id="toc-missingness"><span class="toc-section-number">6.2.1</span> Missingness</a></li>
<li><a href="data-prep.html#outliers" id="toc-outliers"><span class="toc-section-number">6.2.2</span> Outliers</a></li>
<li><a href="data-prep.html#low-variability" id="toc-low-variability"><span class="toc-section-number">6.2.3</span> Low Variability</a></li>
<li><a href="data-prep.html#inconsistent-categories" id="toc-inconsistent-categories"><span class="toc-section-number">6.2.4</span> Inconsistent Categories</a></li>
<li><a href="data-prep.html#data-binning" id="toc-data-binning"><span class="toc-section-number">6.2.5</span> Data Binning</a></li>
</ul></li>
<li><a href="data-prep.html#one-hot-encoding" id="toc-one-hot-encoding"><span class="toc-section-number">6.3</span> One-Hot Encoding</a></li>
<li><a href="data-prep.html#feature-engineering" id="toc-feature-engineering"><span class="toc-section-number">6.4</span> Feature Engineering</a></li>
<li><a href="data-prep.html#review-questions-4" id="toc-review-questions-4"><span class="toc-section-number">6.5</span> Review Questions</a></li>
</ul></li>
<li><a href="desc-stats.html#desc-stats" id="toc-desc-stats"><span class="toc-section-number">7</span> Descriptive Statistics</a>
<ul>
<li><a href="desc-stats.html#univariate-analysis" id="toc-univariate-analysis"><span class="toc-section-number">7.1</span> Univariate Analysis</a>
<ul>
<li><a href="desc-stats.html#measures-of-central-tendency" id="toc-measures-of-central-tendency"><span class="toc-section-number">7.1.1</span> Measures of Central Tendency</a></li>
<li><a href="desc-stats.html#measures-of-spread" id="toc-measures-of-spread"><span class="toc-section-number">7.1.2</span> Measures of Spread</a></li>
</ul></li>
<li><a href="desc-stats.html#bivariate-analysis" id="toc-bivariate-analysis"><span class="toc-section-number">7.2</span> Bivariate Analysis</a>
<ul>
<li><a href="desc-stats.html#covariance" id="toc-covariance"><span class="toc-section-number">7.2.1</span> Covariance</a></li>
<li><a href="desc-stats.html#correlation" id="toc-correlation"><span class="toc-section-number">7.2.2</span> Correlation</a></li>
</ul></li>
<li><a href="desc-stats.html#review-questions-5" id="toc-review-questions-5"><span class="toc-section-number">7.3</span> Review Questions</a></li>
</ul></li>
<li><a href="inf-stats.html#inf-stats" id="toc-inf-stats"><span class="toc-section-number">8</span> Statistical Inference</a>
<ul>
<li><a href="inf-stats.html#introduction-to-probability" id="toc-introduction-to-probability"><span class="toc-section-number">8.1</span> Introduction to Probability</a>
<ul>
<li><a href="inf-stats.html#probability-distributions" id="toc-probability-distributions"><span class="toc-section-number">8.1.1</span> Probability Distributions</a></li>
<li><a href="inf-stats.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">8.1.2</span> Conditional Probability</a></li>
</ul></li>
<li><a href="inf-stats.html#central-limit-theorem" id="toc-central-limit-theorem"><span class="toc-section-number">8.2</span> Central Limit Theorem</a></li>
<li><a href="inf-stats.html#confidence-intervals" id="toc-confidence-intervals"><span class="toc-section-number">8.3</span> Confidence Intervals</a>
<ul>
<li><a href="inf-stats.html#hypothesis-testing" id="toc-hypothesis-testing"><span class="toc-section-number">8.3.1</span> Hypothesis Testing</a></li>
<li><a href="inf-stats.html#alpha" id="toc-alpha"><span class="toc-section-number">8.3.2</span> Alpha</a></li>
<li><a href="inf-stats.html#type-i-ii-errors" id="toc-type-i-ii-errors"><span class="toc-section-number">8.3.3</span> Type I &amp; II Errors</a></li>
<li><a href="inf-stats.html#p-values" id="toc-p-values"><span class="toc-section-number">8.3.4</span> <span class="math inline">\(p\)</span>-Values</a></li>
<li><a href="inf-stats.html#bonferroni-correction" id="toc-bonferroni-correction"><span class="toc-section-number">8.3.5</span> Bonferroni Correction</a></li>
<li><a href="inf-stats.html#statistical-power" id="toc-statistical-power"><span class="toc-section-number">8.3.6</span> Statistical Power</a></li>
</ul></li>
<li><a href="inf-stats.html#review-questions-6" id="toc-review-questions-6"><span class="toc-section-number">8.4</span> Review Questions</a></li>
</ul></li>
<li><a href="aod.html#aod" id="toc-aod"><span class="toc-section-number">9</span> Analysis of Differences</a>
<ul>
<li><a href="aod.html#parametric-vs.-nonparametric-tests" id="toc-parametric-vs.-nonparametric-tests"><span class="toc-section-number">9.1</span> Parametric vs. Nonparametric Tests</a></li>
<li><a href="aod.html#differences-in-discrete-data" id="toc-differences-in-discrete-data"><span class="toc-section-number">9.2</span> Differences in Discrete Data</a></li>
<li><a href="aod.html#differences-in-continuous-data" id="toc-differences-in-continuous-data"><span class="toc-section-number">9.3</span> Differences in Continuous Data</a></li>
<li><a href="aod.html#review-questions-7" id="toc-review-questions-7"><span class="toc-section-number">9.4</span> Review Questions</a></li>
</ul></li>
<li><a href="lm.html#lm" id="toc-lm"><span class="toc-section-number">10</span> Linear Regression</a>
<ul>
<li><a href="lm.html#sample-size" id="toc-sample-size"><span class="toc-section-number">10.1</span> Sample Size</a></li>
<li><a href="lm.html#simple-linear-regression" id="toc-simple-linear-regression"><span class="toc-section-number">10.2</span> Simple Linear Regression</a></li>
<li><a href="lm.html#multiple-linear-regression" id="toc-multiple-linear-regression"><span class="toc-section-number">10.3</span> Multiple Linear Regression</a></li>
<li><a href="lm.html#moderation" id="toc-moderation"><span class="toc-section-number">10.4</span> Moderation</a></li>
<li><a href="lm.html#mediation" id="toc-mediation"><span class="toc-section-number">10.5</span> Mediation</a></li>
<li><a href="lm.html#review-questions-8" id="toc-review-questions-8"><span class="toc-section-number">10.6</span> Review Questions</a></li>
</ul></li>
<li><a href="lme.html#lme" id="toc-lme"><span class="toc-section-number">11</span> Linear Model Extensions</a>
<ul>
<li><a href="lme.html#model-comparisons" id="toc-model-comparisons"><span class="toc-section-number">11.1</span> Model Comparisons</a></li>
<li><a href="lme.html#hierarchical-regression" id="toc-hierarchical-regression"><span class="toc-section-number">11.2</span> Hierarchical Regression</a></li>
<li><a href="lme.html#multilevel-models" id="toc-multilevel-models"><span class="toc-section-number">11.3</span> Multilevel Models</a></li>
<li><a href="lme.html#polynomial-regression" id="toc-polynomial-regression"><span class="toc-section-number">11.4</span> Polynomial Regression</a></li>
<li><a href="lme.html#review-questions-9" id="toc-review-questions-9"><span class="toc-section-number">11.5</span> Review Questions</a></li>
</ul></li>
<li><a href="log.html#log" id="toc-log"><span class="toc-section-number">12</span> Logistic Regression</a>
<ul>
<li><a href="log.html#binomial-logistic-regression" id="toc-binomial-logistic-regression"><span class="toc-section-number">12.1</span> Binomial Logistic Regression</a></li>
<li><a href="log.html#multinomial-logistic-regression" id="toc-multinomial-logistic-regression"><span class="toc-section-number">12.2</span> Multinomial Logistic Regression</a></li>
<li><a href="log.html#ordinal-logistic-regression" id="toc-ordinal-logistic-regression"><span class="toc-section-number">12.3</span> Ordinal Logistic Regression</a></li>
<li><a href="log.html#review-questions-10" id="toc-review-questions-10"><span class="toc-section-number">12.4</span> Review Questions</a></li>
</ul></li>
<li><a href="pred-mod.html#pred-mod" id="toc-pred-mod"><span class="toc-section-number">13</span> Predictive Modeling</a>
<ul>
<li><a href="pred-mod.html#cross-validation" id="toc-cross-validation"><span class="toc-section-number">13.1</span> Cross-Validation</a></li>
<li><a href="pred-mod.html#model-performance" id="toc-model-performance"><span class="toc-section-number">13.2</span> Model Performance</a></li>
<li><a href="pred-mod.html#bias-variance-tradeoff" id="toc-bias-variance-tradeoff"><span class="toc-section-number">13.3</span> Bias-Variance Tradeoff</a></li>
<li><a href="pred-mod.html#tree-based-algorithms" id="toc-tree-based-algorithms"><span class="toc-section-number">13.4</span> Tree-Based Algorithms</a></li>
<li><a href="pred-mod.html#predictive-modeling" id="toc-predictive-modeling"><span class="toc-section-number">13.5</span> Predictive Modeling</a></li>
<li><a href="pred-mod.html#review-questions-11" id="toc-review-questions-11"><span class="toc-section-number">13.6</span> Review Questions</a></li>
</ul></li>
<li><a href="unsup-lrn.html#unsup-lrn" id="toc-unsup-lrn"><span class="toc-section-number">14</span> Unsupervised Learning</a>
<ul>
<li><a href="unsup-lrn.html#factor-analysis" id="toc-factor-analysis"><span class="toc-section-number">14.1</span> Factor Analysis</a>
<ul>
<li><a href="unsup-lrn.html#exploratory-factor-analysis-efa" id="toc-exploratory-factor-analysis-efa"><span class="toc-section-number">14.1.1</span> Exploratory Factor Analysis (EFA)</a></li>
<li><a href="unsup-lrn.html#confirmatory-factor-analysis-cfa" id="toc-confirmatory-factor-analysis-cfa"><span class="toc-section-number">14.1.2</span> Confirmatory Factor Analysis (CFA)</a></li>
</ul></li>
<li><a href="unsup-lrn.html#clustering" id="toc-clustering"><span class="toc-section-number">14.2</span> Clustering</a>
<ul>
<li><a href="unsup-lrn.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">14.2.1</span> <span class="math inline">\(K\)</span>-Means Clustering</a></li>
<li><a href="unsup-lrn.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">14.2.2</span> Hierarchical Clustering</a></li>
</ul></li>
<li><a href="unsup-lrn.html#review-questions-12" id="toc-review-questions-12"><span class="toc-section-number">14.3</span> Review Questions</a></li>
</ul></li>
<li><a href="data-viz.html#data-viz" id="toc-data-viz"><span class="toc-section-number">15</span> Data Visualization</a>
<ul>
<li><a href="data-viz.html#best-practices" id="toc-best-practices"><span class="toc-section-number">15.1</span> Best Practices</a>
<ul>
<li><a href="data-viz.html#color-palette" id="toc-color-palette"><span class="toc-section-number">15.1.1</span> Color Palette</a></li>
<li><a href="data-viz.html#chart-borders" id="toc-chart-borders"><span class="toc-section-number">15.1.2</span> Chart Borders</a></li>
<li><a href="data-viz.html#zero-baseline" id="toc-zero-baseline"><span class="toc-section-number">15.1.3</span> Zero Baseline</a></li>
<li><a href="data-viz.html#intuitive-layout" id="toc-intuitive-layout"><span class="toc-section-number">15.1.4</span> Intuitive Layout</a></li>
<li><a href="data-viz.html#preattentive-attributes" id="toc-preattentive-attributes"><span class="toc-section-number">15.1.5</span> Preattentive Attributes</a></li>
</ul></li>
<li><a href="data-viz.html#step-by-step-visual-upgrade" id="toc-step-by-step-visual-upgrade"><span class="toc-section-number">15.2</span> Step-by-Step Visual Upgrade</a>
<ul>
<li><a href="data-viz.html#step-1-build-bar-chart-with-defaults" id="toc-step-1-build-bar-chart-with-defaults"><span class="toc-section-number">15.2.1</span> Step 1: Build Bar Chart with Defaults</a></li>
<li><a href="data-viz.html#step-2-remove-legend" id="toc-step-2-remove-legend"><span class="toc-section-number">15.2.2</span> Step 2: Remove Legend</a></li>
<li><a href="data-viz.html#step-3-assign-colors-strategically" id="toc-step-3-assign-colors-strategically"><span class="toc-section-number">15.2.3</span> Step 3: Assign Colors Strategically</a></li>
<li><a href="data-viz.html#step-4-add-axis-titles-and-margins" id="toc-step-4-add-axis-titles-and-margins"><span class="toc-section-number">15.2.4</span> Step 4: Add Axis Titles and Margins</a></li>
<li><a href="data-viz.html#step-5-add-left-justified-title" id="toc-step-5-add-left-justified-title"><span class="toc-section-number">15.2.5</span> Step 5: Add Left-Justified Title</a></li>
<li><a href="data-viz.html#step-6-remove-background" id="toc-step-6-remove-background"><span class="toc-section-number">15.2.6</span> Step 6: Remove Background</a></li>
<li><a href="data-viz.html#step-7-remove-axis-ticks" id="toc-step-7-remove-axis-ticks"><span class="toc-section-number">15.2.7</span> Step 7: Remove Axis Ticks</a></li>
<li><a href="data-viz.html#step-8-mute-titles" id="toc-step-8-mute-titles"><span class="toc-section-number">15.2.8</span> Step 8: Mute Titles</a></li>
<li><a href="data-viz.html#step-9-flip-axes" id="toc-step-9-flip-axes"><span class="toc-section-number">15.2.9</span> Step 9: Flip Axes</a></li>
<li><a href="data-viz.html#step-10-sort-data" id="toc-step-10-sort-data"><span class="toc-section-number">15.2.10</span> Step 10: Sort Data</a></li>
</ul></li>
<li><a href="data-viz.html#visualization-types" id="toc-visualization-types"><span class="toc-section-number">15.3</span> Visualization Types</a>
<ul>
<li><a href="data-viz.html#tables" id="toc-tables"><span class="toc-section-number">15.3.1</span> Tables</a></li>
<li><a href="data-viz.html#heatmaps" id="toc-heatmaps"><span class="toc-section-number">15.3.2</span> Heatmaps</a></li>
<li><a href="data-viz.html#scatterplots" id="toc-scatterplots"><span class="toc-section-number">15.3.3</span> Scatterplots</a></li>
<li><a href="data-viz.html#line-graphs" id="toc-line-graphs"><span class="toc-section-number">15.3.4</span> Line Graphs</a></li>
<li><a href="data-viz.html#slopegraphs" id="toc-slopegraphs"><span class="toc-section-number">15.3.5</span> Slopegraphs</a></li>
<li><a href="data-viz.html#bar-charts" id="toc-bar-charts"><span class="toc-section-number">15.3.6</span> Bar Charts</a></li>
<li><a href="data-viz.html#combination-charts" id="toc-combination-charts"><span class="toc-section-number">15.3.7</span> Combination Charts</a></li>
<li><a href="data-viz.html#waterfall-charts" id="toc-waterfall-charts"><span class="toc-section-number">15.3.8</span> Waterfall Charts</a></li>
<li><a href="data-viz.html#waffle-charts" id="toc-waffle-charts"><span class="toc-section-number">15.3.9</span> Waffle Charts</a></li>
<li><a href="data-viz.html#sankey-diagrams" id="toc-sankey-diagrams"><span class="toc-section-number">15.3.10</span> Sankey Diagrams</a></li>
<li><a href="data-viz.html#pie-charts" id="toc-pie-charts"><span class="toc-section-number">15.3.11</span> Pie Charts</a></li>
<li><a href="data-viz.html#d-visuals" id="toc-d-visuals"><span class="toc-section-number">15.3.12</span> 3D Visuals</a></li>
</ul></li>
<li><a href="data-viz.html#elegant-data-visualization" id="toc-elegant-data-visualization"><span class="toc-section-number">15.4</span> Elegant Data Visualization</a></li>
<li><a href="data-viz.html#review-questions-13" id="toc-review-questions-13"><span class="toc-section-number">15.5</span> Review Questions</a></li>
</ul></li>
<li><a href="storytelling.html#storytelling" id="toc-storytelling"><span class="toc-section-number">16</span> Data Storytelling</a>
<ul>
<li><a href="storytelling.html#know-your-audience" id="toc-know-your-audience"><span class="toc-section-number">16.1</span> Know Your Audience</a></li>
<li><a href="storytelling.html#production-status" id="toc-production-status"><span class="toc-section-number">16.2</span> Production Status</a></li>
<li><a href="storytelling.html#structural-elements" id="toc-structural-elements"><span class="toc-section-number">16.3</span> Structural Elements</a>
<ul>
<li><a href="storytelling.html#tldr" id="toc-tldr"><span class="toc-section-number">16.3.1</span> TL;DR</a></li>
<li><a href="storytelling.html#purpose" id="toc-purpose"><span class="toc-section-number">16.3.2</span> Purpose</a></li>
<li><a href="storytelling.html#methodology" id="toc-methodology"><span class="toc-section-number">16.3.3</span> Methodology</a></li>
<li><a href="storytelling.html#results" id="toc-results"><span class="toc-section-number">16.3.4</span> Results</a></li>
<li><a href="storytelling.html#limitations" id="toc-limitations"><span class="toc-section-number">16.3.5</span> Limitations</a></li>
<li><a href="storytelling.html#next-steps" id="toc-next-steps"><span class="toc-section-number">16.3.6</span> Next Steps</a></li>
<li><a href="storytelling.html#appendix" id="toc-appendix"><span class="toc-section-number">16.3.7</span> Appendix</a></li>
</ul></li>
<li><a href="storytelling.html#qa" id="toc-qa"><span class="toc-section-number">16.4</span> Q&amp;A</a></li>
<li><a href="storytelling.html#review-questions-14" id="toc-review-questions-14"><span class="toc-section-number">16.5</span> Review Questions</a></li>
</ul></li>
<li><a href="bibli.html#bibli" id="toc-bibli"><span class="toc-section-number">17</span> Bibliography</a></li>
<li><a href="storytelling.html#appendix" id="toc-appendix"><span class="toc-section-number">18</span> Appendix</a>
<ul>
<li><a href="appendix.html#d-framework-1" id="toc-d-framework-1"><span class="toc-section-number">18.1</span> 4D Framework</a>
<ul>
<li><a href="appendix.html#discover" id="toc-discover"><span class="toc-section-number">18.1.1</span> Discover</a></li>
<li><a href="appendix.html#design" id="toc-design"><span class="toc-section-number">18.1.2</span> Design</a></li>
<li><a href="appendix.html#develop" id="toc-develop"><span class="toc-section-number">18.1.3</span> Develop</a></li>
<li><a href="appendix.html#deliver" id="toc-deliver"><span class="toc-section-number">18.1.4</span> Deliver</a></li>
</ul></li>
<li><a href="appendix.html#data-visualization" id="toc-data-visualization"><span class="toc-section-number">18.2</span> Data Visualization</a>
<ul>
<li><a href="appendix.html#step-by-step-visual-upgrade-1" id="toc-step-by-step-visual-upgrade-1"><span class="toc-section-number">18.2.1</span> Step-by-Step Visual Upgrade</a></li>
<li><a href="appendix.html#tables-1" id="toc-tables-1"><span class="toc-section-number">18.2.2</span> Tables</a></li>
<li><a href="appendix.html#heatmaps-1" id="toc-heatmaps-1"><span class="toc-section-number">18.2.3</span> Heatmaps</a></li>
<li><a href="appendix.html#scatterplots-1" id="toc-scatterplots-1"><span class="toc-section-number">18.2.4</span> Scatterplots</a></li>
<li><a href="appendix.html#line-charts" id="toc-line-charts"><span class="toc-section-number">18.2.5</span> Line Charts</a></li>
<li><a href="appendix.html#slopegraphs-1" id="toc-slopegraphs-1"><span class="toc-section-number">18.2.6</span> Slopegraphs</a></li>
<li><a href="appendix.html#bar-charts-1" id="toc-bar-charts-1"><span class="toc-section-number">18.2.7</span> Bar Charts</a></li>
<li><a href="appendix.html#combination-charts-1" id="toc-combination-charts-1"><span class="toc-section-number">18.2.8</span> Combination Charts</a></li>
<li><a href="appendix.html#waterfall-charts-1" id="toc-waterfall-charts-1"><span class="toc-section-number">18.2.9</span> Waterfall Charts</a></li>
<li><a href="appendix.html#waffle-charts-1" id="toc-waffle-charts-1"><span class="toc-section-number">18.2.10</span> Waffle Charts</a></li>
<li><a href="appendix.html#sankey-diagrams-1" id="toc-sankey-diagrams-1"><span class="toc-section-number">18.2.11</span> Sankey Diagrams</a></li>
<li><a href="appendix.html#pie-charts-1" id="toc-pie-charts-1"><span class="toc-section-number">18.2.12</span> Pie Charts</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Fundamentals of People Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pred-mod" class="section level1" number="13">
<h1><span class="header-section-number">13</span> Predictive Modeling</h1>
<p>In people analytics, inferential models like those covered in Chapters <a href="lm.html#lm">10</a> and <a href="lme.html#lme">11</a> are generally warranted by the research objectives. However, there are times when we need to go beyond interpreting coefficients to understand relative influences of predictors on an outcome and leverage the models to estimate or <em>predict</em> the most likely future values. This type of modeling is often referred to as <strong>predictive analytics</strong> and is the subject of this chapter.</p>
<p>A branch of <strong>Artificial Intelligence (AI)</strong> known as <strong>Machine Learning (ML)</strong> is often associated with predictive modeling. ML is a set of methods which aim to improve performance on a set of tasks by learning from data (Mitchell, 1997). ML applications can be found in medical diagnostics, autonomous vehicles, speech recognition, automated securities trading, lending decisions, marketing, and many other domains. The difference between statistics and ML is largely philosophical. Logistic regression, for example, is covered in both statistics and ML textbooks. While statistics focuses more on modeling, and ML is more algorithmic, both can be used for prediction. The broader field of <strong>data science</strong> often further confounds distinctions between these disciplines, though data science represents the entire end-to-end process – from data extraction and engineering to modeling and analysis.</p>
<p>A good use case for a predictive model in people analytics is data restatement to adjust for reorganizations over time. In this case, accuracy may be more important than the explainability of the model. A predictive model may be used to predict and assign a current functional executive to historical records to support leader-wise trending analyses. For example, consider a scenario in which the current VP of Product Marketing was hired six months ago to replace the former VP of Product Marketing who was in the role for the prior five year period. If the new VP wants to see monthly termination counts for their organization over the past three years, term records prior to the VP’s start date need to be associated with the <em>current</em> – rather than former – VP of Product Marketing to accomplish this. A model can be trained to learn from patterns in the combinations of current workers’ department, leader, and job attributes that can be used to assign current executives to past data such as historical termination events or month-end worker snapshots.</p>
<p>It is important to note that while there are AI/ML applications for people analytics, feeding data to black box models without an understanding of how the underlying algorithms work is generally a bad idea. Despite the glamour predictive analytics has seen in recent years due to the allure of a magical elixir that can portend the future, more times than not inferential statistical approaches are more appropriate in a people analytics setting. Unfortunately, the hype has given rise to AI snake oil to justify a premium price point for modern HR tech solutions, which are often little more than the descriptive statistics covered in Chapter <a href="desc-stats.html#desc-stats">7</a>.</p>
<p>It is both a blessing and a curse that a predictive model can be built with a single line of code in R, and it is dangerous to blindly make recommendations on the basis of the output. A simpler model that you can understand and explain is generally a better option than a more complex one that you cannot. There is a high probability that stakeholders will ask deeper questions about <em>why</em> a particular segment of the workforce is predicted to exit at higher rates, for example, and answering these questions requires a deeper understanding of the factors that led to them being classified as such.</p>
<p>People data are messy, and understanding why people vary in attitudes, perceptions, and behaviors is an inherently difficult endeavor. Spoiler alert: There is no crystal ball that will soften this reality.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stats-ml-satire"></span>
<img src="graphics/stats_ml_satire.png" alt="Satirical illustration based on the viral meme of 2019 which depicts the enduring popularity and utility of linear regression, even in light of the billions companies invest in ML each year." width="75%" />
<p class="caption">
Figure 13.1: Satirical illustration based on the viral meme of 2019 which depicts the enduring popularity and utility of linear regression, even in light of the billions companies invest in ML each year.
</p>
</div>
<div id="cross-validation" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> Cross-Validation</h2>
<p>Predictive modeling involves training a model on a data set referred to as a <strong>training set</strong> and using the model to make predictions for observations on a separate data set known as the <strong>test set</strong> or <strong>validation set</strong> to evaluate model performance.</p>
<p>A model is blind to data in the test set, since only data in the training set are used to <em>train</em> the model. Therefore, the test set provides a convenient way to compare the actual known values to the predicted values and estimate how well the model will generalize to other data. Evaluating model performance on the basis of training data would be akin to having students take an exam after providing them the answers. Strong performance on the training data is almost a certainty, so the value of a model rests on its performance on data not used to build it.</p>
<p>This partitioning procedure is known as <strong>cross-validation (CV)</strong>. While there are many methods of splitting data into training and test sets, a common feature among all is that the partitioning strategy is random. Without randomization, the model may learn patterns characteristic of the training set that result in inaccurate predictions for other data which do not feature such patterning.</p>
<p>This section will explore a few of the most common CV methods.</p>
<p><strong>Validation Set Approach</strong></p>
<p>The <strong>validation set approach</strong> is the most basic form of CV. This approach involves defining proportions by which to partition data into training and test sets – usually 2/3 and 1/3, respectively. The model is trained on the training set and then differences between actual <span class="math inline">\(y\)</span> and predicted <span class="math inline">\(\hat y\)</span> values are calculated on the test set to evaluate model performance.</p>
<p><strong>Leave-One-Out</strong></p>
<p><strong>Leave-one out</strong> CV fits a model using <span class="math inline">\(n-1\)</span> observations <span class="math inline">\(n\)</span> times. The test error is then evaluated by calculating differences between actual <span class="math inline">\(y\)</span> and predicted <span class="math inline">\(\hat y\)</span> values for all omitted observations.</p>
<p><span class="math inline">\(\textbf k\)</span><strong>-Fold</strong></p>
<p><span class="math inline">\(\textbf k\)</span><strong>-fold</strong> CV randomly partitions observations into <span class="math inline">\(k\)</span> groups, or <em>folds</em>, that are approximately equal in size. The first fold is treated as the test set, and the model is trained on the remaining <span class="math inline">\(k-1\)</span> folds. This procedure is repeated <span class="math inline">\(k\)</span> times, each with a different set of observations (<em>fold</em>) as the test set. The test error is then evaluated by calculating differences between actual <span class="math inline">\(y\)</span> and predicted <span class="math inline">\(\hat y\)</span> values across all test sets.</p>
</div>
<div id="model-performance" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Model Performance</h2>
<p>There are several methods of quantifying how well models perform on test data in order to assess the extent to which the model will generalize. Predicting modeling applications will be categorized as either <em>classification</em> or <em>forecasting</em> to reflect the families of use cases germane to people analytics.</p>
<p><strong>Classification</strong></p>
<p>In a classification setting, predictions are either right or wrong. Therefore, calculating the overall error rate across test data is straightforward:</p>
<p><span class="math display">\[ \frac{1}{n} \displaystyle\sum_{i=1}^{n} I(y_i \ne \hat y_i), \]</span></p>
<p>where <span class="math inline">\(I\)</span> is an indicator variable equal to <span class="math inline">\(1\)</span> if <span class="math inline">\(y_i \ne \hat y_i\)</span> and <span class="math inline">\(0\)</span> if <span class="math inline">\(y_i = \hat y_i\)</span>.</p>
<p>A <strong>confusion matrix</strong> is often used in classification to parse the overall model accuracy rate into component parts and understand whether the model performs at a level appropriate to a defined tolerance level per the research objective. In an attrition project, it may be more important to correctly predict high performers who leave than to correctly predict those who stay, as prediction errors for the former are likely far more costly. Several performance metrics are provided by the confusion matrix to aid in a more granular understanding of model performance, which are outlined in Figure <a href="pred-mod.html#fig:confusion-mtx">13.2</a>:</p>
<ul>
<li><strong>True Positive</strong>: Number of correct true predictions</li>
<li><strong>True Negative</strong>: Number of correct false predictions</li>
<li><strong>False Positive</strong>: Number of incorrect true predictions (type 1 error)</li>
<li><strong>False Negative</strong>: Number of incorrect false predictions (type 2 error)</li>
<li><strong>Accuracy</strong>: Rate of correct predictions overall</li>
<li><strong>Sensitivity</strong>: Rate of actual true cases predicted correctly (also known as <strong>Recall</strong>)</li>
<li><strong>Specificity</strong>: Rate of actual false cases predicted correctly</li>
<li><strong>Precision</strong>: Rate of correct predictions among all cases predicted true</li>
<li><strong>Negative Predictive Value</strong>: Rate of correct predictions among all cases predicted false</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:confusion-mtx"></span>
<img src="graphics/confusion_matrix.png" alt="Confusion matrix" width="100%" />
<p class="caption">
Figure 13.2: Confusion matrix
</p>
</div>
<p><strong>Forecasting</strong></p>
<p>While predictions are either right or wrong in a classification context, evaluating model performance in a forecasting context involves assessing the <em>magnitude</em> of differences between actual <span class="math inline">\(y\)</span> and predicted <span class="math inline">\(\hat y\)</span> values – usually across time periods. There are many methods for assessing forecasting model performance, and we will focus on some of the most common.</p>
<ul>
<li>Mean absolute deviation (MAD): Average absolute difference between actual <span class="math inline">\(y\)</span> and predicted <span class="math inline">\(\hat y\)</span> values</li>
</ul>
<p><span class="math display">\[ \frac{\sum|y_i - \hat y_i|}{n} \]</span></p>
<ul>
<li>Mean square error (MSE): Average squared difference between actual <span class="math inline">\(y\)</span> and predicted <span class="math inline">\(\hat y\)</span> values</li>
</ul>
<p><span class="math display">\[ \frac{\sum(y_i - \hat y_i)^2}{n} \]</span></p>
<p>Squaring differences accomplishes two key objectives: (1) converts negative differences to positive consistent with the MAD approach, and (2) imposes a greater penalty on larger differences, which causes error rates to increase at an exponential rather than linear rate (e.g., <span class="math inline">\(1^2 = 1\)</span>, <span class="math inline">\(2^2 = 4\)</span>, <span class="math inline">\(3^2 = 9\)</span>, <span class="math inline">\(4^2 = 16\)</span>). MSE is perhaps the most pervasive model performance measure in predictive modeling.</p>
<ul>
<li>Mean absolute percentage error (MAPE): Average absolute difference expressed as a percentage</li>
</ul>
<p><span class="math display">\[ (\frac{100}{n})\sum|\frac{y_i - \hat y_i}{y_i}| \]</span></p>
</div>
<div id="bias-variance-tradeoff" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Bias-Variance Tradeoff</h2>
<p><strong>Bias-variance tradeoff</strong> refers to the important endeavor of minimizing two sources of error that prevent models from generalizing beyond their training data: <em>bias</em> and <em>variance</em>.</p>
<ul>
<li><strong>Bias</strong>: Error from erroneous assumptions in the model. High bias results from models that are too simplistic to accurately capture the relationships between predictors and the outcome; this is known as <strong>underfitting</strong>.</li>
<li><strong>Variance</strong>: Error from sensitivity to small fluctuations in training data. High variance results from models that capture random noise rather than the significant patterns in the training data; this is known as <strong>overfitting</strong>.</li>
</ul>
<p>As a general rule, the more flexible the model, the more variance and less bias. As shown in Figure <a href="pred-mod.html#fig:bias-var-tradeoff">13.3</a>, minimizing test error by achieving a model with optimal fit to the data requires limiting both bias and variance.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bias-var-tradeoff"></span>
<img src="graphics/bias_variance_tradeoff.png" alt="Bias-variance tradeoff. Dashed line represents optimal model performance." width="75%" />
<p class="caption">
Figure 13.3: Bias-variance tradeoff. Dashed line represents optimal model performance.
</p>
</div>
</div>
<div id="tree-based-algorithms" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Tree-Based Algorithms</h2>
<p>While there are many flexible ML algorithms, such as <strong>Extreme Gradient Boosting (XGBoost)</strong>, <strong>Artificial Neural Networks (ANN)</strong>, and <strong>Support Vector Machines (SVM)</strong>, that tend to perform well across a range of prediction problems, these will not be covered as we will focus on more <em>interpretable</em> tree-based algorithms that are more suitable for people analytics.</p>
<p><strong>Decision Trees</strong></p>
<p>In addition to the inferential models covered in previous chapters, <strong>decision trees</strong> are also excellent tools that lend to simple and effective narratives about factors influencing outcomes in either a regression or classification setting. As illustrated in Figure <a href="pred-mod.html#fig:decision-tree">13.4</a>, decision trees resemble a tree that depicts a set of decisions as well as consequences of those decisions. The top-level <code>Department</code> variable is known as a root node, and the remaining <code>Tenure</code>, <code>Performance Rating</code>, and <code>Remote</code> nodes are known as interior nodes. Decisions represented in <code>Active</code> and <code>Inactive</code> boxes are referred to as leaf, terminal, or end nodes.</p>
<p>As evidenced by the inactive status prediction in the leaf node, this decision tree shows that employees in the Engineering department are unlikely to stick around for two or more years. In addition, employees in other departments terminate if they are low performers or if they are high performers who do not work remotely. It’s important to note that in practice, it is rare to achieve complete purity in leaf nodes, as there is usually a mix of results in a given node – though a more frequent class or range of values is expected. If leaf nodes for a classification problem, for example, are comprised of a single class, it may be evidence of overfitting (especially if the <span class="math inline">\(n\)</span>-count is small).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:decision-tree"></span>
<img src="graphics/decision_tree.png" alt="Conceptual decision tree for employee attrition prediction" width="75%" />
<p class="caption">
Figure 13.4: Conceptual decision tree for employee attrition prediction
</p>
</div>
<p>The accuracy of predictions based on a deep tree with an excessive number of partitions and few observations will likely be unacceptable beyond the training data. The goal of decision trees is to arrive at a set of decisions that best delineate one class or range of values from others by identifying patterns and natural cutpoints among a reasonably large subset of the data at each level. There must be signal in the features used to partition the data such that predictions are an improvement on random guessing (e.g., 50/50 chance in a binary classification setting).</p>
<p><strong>Random Forests</strong></p>
<p>A <strong>random forest (RF)</strong> is a natural extension of the decision tree. As the name implies, a random forest is a large number (forest) of individual decision trees that operate as an ensemble. The process of fitting multiple models on different subsets of training data and then combining predictions across all models is referred to as <strong>bagging</strong>. This is a case of <em>wisdom of the crowd</em> decision-making in which a large number of uncorrelated trees (models) functioning as a committee should outperform individual trees.</p>
<p>To understand the mechanics of a random forest, consider an investment strategy in which you diversify an investment portfolio by spreading investments across different assets. By investing in assets that are uncorrelated, there is a lower likelihood that the portfolio’s value will be negatively impacted by a negative event impacting a single holding. In the same way, a random forest constructs an ensemble of trees that are each based on different randomized subsets of data and combinations of features to amalgamate the information and arrive at more accurate predictions. The potential for poor performance from a single tree is mitigated by the many trees working in concert with one another.</p>
<p>Though random forests combine information from many decision trees, there are still intuitive ways of understanding which features are most important in segmenting employees to understand drivers of various outcomes.</p>
</div>
<div id="predictive-modeling" class="section level2" number="13.5">
<h2><span class="header-section-number">13.5</span> Predictive Modeling</h2>
<p>We will now integrate these concepts into attrition classification and forecasting examples. The high-level prediction workflow will follow four steps:</p>
<ul>
<li><strong>Step 1</strong>: Partition data into training and test sets for cross-validation.</li>
<li><strong>Step 2</strong>: Build models using training data.</li>
<li><strong>Step 3</strong>: Use models to make predictions on test data.</li>
<li><strong>Step 4</strong>: Evaluate model performance.</li>
</ul>
<p><strong>Classification</strong></p>
<p>To demonstrate the prediction workflow steps for classification, we will leverage our <code>employees</code> data set:</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="pred-mod.html#cb413-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb413-2"><a href="pred-mod.html#cb413-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb413-3"><a href="pred-mod.html#cb413-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb413-4"><a href="pred-mod.html#cb413-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load employee data</span></span>
<span id="cb413-5"><a href="pred-mod.html#cb413-5" aria-hidden="true" tabindex="-1"></a>prediction_dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_book/master/data/employees.csv&quot;</span>)</span>
<span id="cb413-6"><a href="pred-mod.html#cb413-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb413-7"><a href="pred-mod.html#cb413-7" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode active outcome variable, setting inactives to 1 and actives to 0</span></span>
<span id="cb413-8"><a href="pred-mod.html#cb413-8" aria-hidden="true" tabindex="-1"></a>prediction_dat<span class="sc">$</span>active <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(prediction_dat<span class="sc">$</span>active <span class="sc">==</span> <span class="st">&#39;No&#39;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span></code></pre></div>
<ul>
<li><strong>Step 1</strong>: Partition data into training and test sets for cross-validation.</li>
</ul>
<p>For this example, we will implement the validation set approach for CV.</p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="pred-mod.html#cb414-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducible training and test sets</span></span>
<span id="cb414-2"><a href="pred-mod.html#cb414-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">9876</span>)</span>
<span id="cb414-3"><a href="pred-mod.html#cb414-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-4"><a href="pred-mod.html#cb414-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Randomly select 2/3 of employees for the training set</span></span>
<span id="cb414-5"><a href="pred-mod.html#cb414-5" aria-hidden="true" tabindex="-1"></a>training_ids <span class="ot">&lt;-</span> <span class="fu">sample</span>(prediction_dat<span class="sc">$</span>employee_id, <span class="at">size =</span> <span class="fu">nrow</span>(prediction_dat) <span class="sc">*</span> <span class="dv">2</span><span class="sc">/</span><span class="dv">3</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb414-6"><a href="pred-mod.html#cb414-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-7"><a href="pred-mod.html#cb414-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create training data</span></span>
<span id="cb414-8"><a href="pred-mod.html#cb414-8" aria-hidden="true" tabindex="-1"></a>training_dat <span class="ot">&lt;-</span> prediction_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(employee_id <span class="sc">%in%</span> training_ids)</span>
<span id="cb414-9"><a href="pred-mod.html#cb414-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-10"><a href="pred-mod.html#cb414-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create test data using remaining 1/3 of observations</span></span>
<span id="cb414-11"><a href="pred-mod.html#cb414-11" aria-hidden="true" tabindex="-1"></a>test_dat <span class="ot">&lt;-</span> prediction_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(<span class="sc">!</span>employee_id <span class="sc">%in%</span> training_ids)</span>
<span id="cb414-12"><a href="pred-mod.html#cb414-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb414-13"><a href="pred-mod.html#cb414-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Return Boolean to validate that all observations are accounted for</span></span>
<span id="cb414-14"><a href="pred-mod.html#cb414-14" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(training_dat) <span class="sc">+</span> <span class="fu">nrow</span>(test_dat) <span class="sc">==</span> <span class="fu">nrow</span>(prediction_dat)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<ul>
<li><strong>Step 2</strong>: Build models using training data.</li>
</ul>
<p>Machine learning (ML) algorithms are sensitive to imbalanced classes. That is, when there is not an equal representation of each class we wish to predict in the data (e.g., employees who left and employees who did not), it can adversely impact our results. In the case of our <code>employees</code> data set, there are 1,233 observations for active employees but only 237 observations for inactive employees. Therefore, we will introduce a popular technique to address this known as <strong>Synthetic Minority Oversampling Technique (SMOTE)</strong>, which will be important for the RF model we will train. This technique takes random samples with replacement (i.e., each observation may be chosen more than once) from the minority class to augment the class’s representation in the dataset and achieve balanced classes.</p>
<p>While functions exist for implementing SMOTE, such as the <code>SMOTE()</code> function from the <code>DMwR</code> library, in the spirit of demystifying black box ML approaches we will step through this procedure without the use of an available function:</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="pred-mod.html#cb416-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate class representation delta in training data</span></span>
<span id="cb416-2"><a href="pred-mod.html#cb416-2" aria-hidden="true" tabindex="-1"></a>training_class_delta <span class="ot">&lt;-</span> <span class="fu">nrow</span>(training_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(active <span class="sc">==</span> <span class="dv">0</span>)) <span class="sc">-</span> <span class="fu">nrow</span>(training_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(active <span class="sc">==</span> <span class="dv">1</span>))</span>
<span id="cb416-3"><a href="pred-mod.html#cb416-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb416-4"><a href="pred-mod.html#cb416-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Copy training data to separate data frame for oversampling</span></span>
<span id="cb416-5"><a href="pred-mod.html#cb416-5" aria-hidden="true" tabindex="-1"></a>training_dat_os <span class="ot">&lt;-</span> training_dat</span>
<span id="cb416-6"><a href="pred-mod.html#cb416-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb416-7"><a href="pred-mod.html#cb416-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for reproducible results</span></span>
<span id="cb416-8"><a href="pred-mod.html#cb416-8" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">9876</span>)</span>
<span id="cb416-9"><a href="pred-mod.html#cb416-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb416-10"><a href="pred-mod.html#cb416-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Oversample the underrepresented inactive class by training_class_delta to align observation counts with active class</span></span>
<span id="cb416-11"><a href="pred-mod.html#cb416-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: A loop is not the most efficient -- especially with large data sets -- but it is leveraged here to simplify instruction on SMOTE mechanics</span></span>
<span id="cb416-12"><a href="pred-mod.html#cb416-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>training_class_delta){</span>
<span id="cb416-13"><a href="pred-mod.html#cb416-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb416-14"><a href="pred-mod.html#cb416-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Sample employee id from underrepresented class</span></span>
<span id="cb416-15"><a href="pred-mod.html#cb416-15" aria-hidden="true" tabindex="-1"></a>  oversampled_id <span class="ot">&lt;-</span> <span class="fu">sample</span>(training_dat_os[training_dat_os<span class="sc">$</span>active <span class="sc">==</span> <span class="dv">1</span>, <span class="st">&#39;employee_id&#39;</span>], <span class="at">size =</span> <span class="dv">1</span>, <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb416-16"><a href="pred-mod.html#cb416-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb416-17"><a href="pred-mod.html#cb416-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Store observation for sampled employee id</span></span>
<span id="cb416-18"><a href="pred-mod.html#cb416-18" aria-hidden="true" tabindex="-1"></a>  new_obs <span class="ot">&lt;-</span> <span class="fu">unique</span>(training_dat_os <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(employee_id <span class="sc">==</span> oversampled_id))</span>
<span id="cb416-19"><a href="pred-mod.html#cb416-19" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb416-20"><a href="pred-mod.html#cb416-20" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Append sampled observation to training data frame</span></span>
<span id="cb416-21"><a href="pred-mod.html#cb416-21" aria-hidden="true" tabindex="-1"></a>  training_dat_os <span class="ot">&lt;-</span> <span class="fu">rbind</span>(training_dat_os, new_obs)</span>
<span id="cb416-22"><a href="pred-mod.html#cb416-22" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb416-23"><a href="pred-mod.html#cb416-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb416-24"><a href="pred-mod.html#cb416-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Return Boolean to validate that classes are equal in the training data</span></span>
<span id="cb416-25"><a href="pred-mod.html#cb416-25" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(training_dat_os <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(active <span class="sc">==</span> <span class="dv">0</span>)) <span class="sc">==</span> <span class="fu">nrow</span>(training_dat_os <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(active <span class="sc">==</span> <span class="dv">1</span>))</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Next, we will fit a binomial logistic regression model using a subset of predictors from the oversampled training data. For comparison, let’s summarize a model using original and oversampled data.</p>
<pre><code>## 
## Call:
## glm(formula = active ~ overtime + job_lvl + engagement + interview_rating, 
##     family = binomial, data = training_dat)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.78408  -0.00157   0.00000   0.00000   2.54959  
## 
## Coefficients:
##                  Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)       85.7594    16.4594   5.210 1.88e-07 ***
## overtimeYes        2.5156     0.8766   2.870  0.00411 ** 
## job_lvl           -0.1372     0.4805  -0.285  0.77528    
## engagement        -0.6845     0.6304  -1.086  0.27755    
## interview_rating -24.9181     4.7673  -5.227 1.72e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 869.013  on 979  degrees of freedom
## Residual deviance:  44.466  on 975  degrees of freedom
## AIC: 54.466
## 
## Number of Fisher Scoring iterations: 11</code></pre>
<pre><code>## 
## Call:
## glm(formula = active ~ overtime + job_lvl + engagement + interview_rating, 
##     family = binomial, data = training_dat_os)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -3.801   0.000   0.000   0.000   2.704  
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)      131.12905   19.56650   6.702 2.06e-11 ***
## overtimeYes        2.27109    0.67614   3.359 0.000782 ***
## job_lvl            0.04793    0.36730   0.130 0.896172    
## engagement        -0.41182    0.42738  -0.964 0.335252    
## interview_rating -37.87539    5.64636  -6.708 1.97e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2276.295  on 1641  degrees of freedom
## Residual deviance:   88.655  on 1637  degrees of freedom
## AIC: 98.655
## 
## Number of Fisher Scoring iterations: 12</code></pre>
<p>As we can see, results of the two binomial logistic regression models are consistent in the sense that only <code>overtime</code> and <code>interview_rating</code> emerge as significant in classifying employees into active and inactive classes. Since the oversampled data has a larger <span class="math inline">\(n\)</span>-count, there is greater power to detect effects, and we see this reflected in the larger coefficients and lower standard errors.</p>
<p>Using a similar syntax, we can fit a RF using the <code>randomForest()</code> function from the package by the same name:</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="pred-mod.html#cb420-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb420-2"><a href="pred-mod.html#cb420-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb420-3"><a href="pred-mod.html#cb420-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb420-4"><a href="pred-mod.html#cb420-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Train RF model using original data</span></span>
<span id="cb420-5"><a href="pred-mod.html#cb420-5" aria-hidden="true" tabindex="-1"></a>rf.fit <span class="ot">&lt;-</span> randomForest<span class="sc">::</span><span class="fu">randomForest</span>(active <span class="sc">~</span> overtime <span class="sc">+</span> job_lvl <span class="sc">+</span> engagement <span class="sc">+</span> interview_rating, <span class="at">data =</span> training_dat)</span>
<span id="cb420-6"><a href="pred-mod.html#cb420-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb420-7"><a href="pred-mod.html#cb420-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train RF model using oversampled data</span></span>
<span id="cb420-8"><a href="pred-mod.html#cb420-8" aria-hidden="true" tabindex="-1"></a>rf.os.fit <span class="ot">&lt;-</span> randomForest<span class="sc">::</span><span class="fu">randomForest</span>(active <span class="sc">~</span> overtime <span class="sc">+</span> job_lvl <span class="sc">+</span> engagement <span class="sc">+</span> interview_rating, <span class="at">data =</span> training_dat_os)</span></code></pre></div>
<p>While RFs generally offer a significant lift in performance beyond a single decision tree, we could also apply <em>tuning wrappers</em> around the <code>randomForest()</code> function to tune the model’s hyperparameters. Experimenting with a range of values for parameters, such as <code>mtry</code> for the number of variables to randomly sample and <code>ntree</code> for the number of trees to grow, may further improve model performance. Hyperparameter tuning is beyond the scope of this book, but Kuhn and Johnson (2013) is an excellent resource for a more exhaustive treatment on ML models.</p>
<ul>
<li><strong>Step 3</strong>: Use models to make predictions on test data.</li>
</ul>
<p>While there are packages in R which provide performance metrics for predictive models, we will create a function for greater visibility into how each metric is calculated:</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="pred-mod.html#cb421-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Develop function that returns a data frame of classification model performance statistics</span></span>
<span id="cb421-2"><a href="pred-mod.html#cb421-2" aria-hidden="true" tabindex="-1"></a>classifier.perf <span class="ot">&lt;-</span> <span class="cf">function</span>(actual, predicted){</span>
<span id="cb421-3"><a href="pred-mod.html#cb421-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb421-4"><a href="pred-mod.html#cb421-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check for missing values; metrics will be computed on non-missing values only</span></span>
<span id="cb421-5"><a href="pred-mod.html#cb421-5" aria-hidden="true" tabindex="-1"></a>  predicted <span class="ot">&lt;-</span> predicted[<span class="sc">!</span><span class="fu">is.na</span>(actual)]</span>
<span id="cb421-6"><a href="pred-mod.html#cb421-6" aria-hidden="true" tabindex="-1"></a>  actual <span class="ot">&lt;-</span> actual[<span class="sc">!</span><span class="fu">is.na</span>(actual)]</span>
<span id="cb421-7"><a href="pred-mod.html#cb421-7" aria-hidden="true" tabindex="-1"></a>  actual <span class="ot">&lt;-</span> actual[<span class="sc">!</span><span class="fu">is.na</span>(predicted)]</span>
<span id="cb421-8"><a href="pred-mod.html#cb421-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb421-9"><a href="pred-mod.html#cb421-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Produce counts for model performance metrics</span></span>
<span id="cb421-10"><a href="pred-mod.html#cb421-10" aria-hidden="true" tabindex="-1"></a>  TP <span class="ot">&lt;-</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> predicted <span class="sc">==</span> <span class="dv">1</span>) <span class="co"># true positives</span></span>
<span id="cb421-11"><a href="pred-mod.html#cb421-11" aria-hidden="true" tabindex="-1"></a>  TN <span class="ot">&lt;-</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">0</span> <span class="sc">&amp;</span> predicted <span class="sc">==</span> <span class="dv">0</span>) <span class="co"># true negatives</span></span>
<span id="cb421-12"><a href="pred-mod.html#cb421-12" aria-hidden="true" tabindex="-1"></a>  FP <span class="ot">&lt;-</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">0</span> <span class="sc">&amp;</span> predicted <span class="sc">==</span> <span class="dv">1</span>) <span class="co"># false positives</span></span>
<span id="cb421-13"><a href="pred-mod.html#cb421-13" aria-hidden="true" tabindex="-1"></a>  FN <span class="ot">&lt;-</span> <span class="fu">sum</span>(actual <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> predicted <span class="sc">==</span> <span class="dv">0</span>) <span class="co"># false negatives</span></span>
<span id="cb421-14"><a href="pred-mod.html#cb421-14" aria-hidden="true" tabindex="-1"></a>  P <span class="ot">&lt;-</span> TP <span class="sc">+</span> FN <span class="co"># total positives</span></span>
<span id="cb421-15"><a href="pred-mod.html#cb421-15" aria-hidden="true" tabindex="-1"></a>  N <span class="ot">&lt;-</span> FP <span class="sc">+</span> TN <span class="co"># total negatives</span></span>
<span id="cb421-16"><a href="pred-mod.html#cb421-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb421-17"><a href="pred-mod.html#cb421-17" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Store rates to variables</span></span>
<span id="cb421-18"><a href="pred-mod.html#cb421-18" aria-hidden="true" tabindex="-1"></a>  accuracy <span class="ot">&lt;-</span> <span class="fu">signif</span>(<span class="dv">100</span> <span class="sc">*</span> (<span class="fu">sum</span>(actual <span class="sc">==</span> predicted) <span class="sc">/</span> <span class="fu">length</span>(actual)), <span class="dv">3</span>)</span>
<span id="cb421-19"><a href="pred-mod.html#cb421-19" aria-hidden="true" tabindex="-1"></a>  sensitivity <span class="ot">&lt;-</span> <span class="fu">signif</span>(<span class="dv">100</span> <span class="sc">*</span> (TP <span class="sc">/</span> (TP <span class="sc">+</span> FN)), <span class="dv">3</span>)</span>
<span id="cb421-20"><a href="pred-mod.html#cb421-20" aria-hidden="true" tabindex="-1"></a>  specificity <span class="ot">&lt;-</span> <span class="fu">signif</span>(<span class="dv">100</span> <span class="sc">*</span> (TN <span class="sc">/</span> (TN <span class="sc">+</span> FP)), <span class="dv">3</span>)</span>
<span id="cb421-21"><a href="pred-mod.html#cb421-21" aria-hidden="true" tabindex="-1"></a>  precision <span class="ot">&lt;-</span> <span class="fu">signif</span>(<span class="dv">100</span> <span class="sc">*</span> (TP <span class="sc">/</span> (TP <span class="sc">+</span> FP)), <span class="dv">3</span>)</span>
<span id="cb421-22"><a href="pred-mod.html#cb421-22" aria-hidden="true" tabindex="-1"></a>  neg_pred_val <span class="ot">&lt;-</span> <span class="fu">signif</span>(<span class="dv">100</span> <span class="sc">*</span> (TN <span class="sc">/</span> (TN <span class="sc">+</span> FN)), <span class="dv">3</span>)</span>
<span id="cb421-23"><a href="pred-mod.html#cb421-23" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb421-24"><a href="pred-mod.html#cb421-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Format output</span></span>
<span id="cb421-25"><a href="pred-mod.html#cb421-25" aria-hidden="true" tabindex="-1"></a>  stat_nm <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;accuracy&quot;</span>, <span class="st">&quot;sensitivity&quot;</span>, <span class="st">&quot;specificity&quot;</span>, <span class="st">&quot;precision&quot;</span>, <span class="st">&quot;neg_pred_val&quot;</span>)</span>
<span id="cb421-26"><a href="pred-mod.html#cb421-26" aria-hidden="true" tabindex="-1"></a>  stat_vl <span class="ot">&lt;-</span> <span class="fu">c</span>(accuracy, sensitivity, specificity, precision, neg_pred_val)</span>
<span id="cb421-27"><a href="pred-mod.html#cb421-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb421-28"><a href="pred-mod.html#cb421-28" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Return model performance statistics in a data frame</span></span>
<span id="cb421-29"><a href="pred-mod.html#cb421-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">data.frame</span>(stat_nm, stat_vl))</span>
<span id="cb421-30"><a href="pred-mod.html#cb421-30" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb421-31"><a href="pred-mod.html#cb421-31" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>We can use the <code>predict()</code> function in conjunction with the object holding the trained model to predict class values for our test data. For classification, we need to define a probability threshold for classifying observations into one class versus the other. This is an important consideration since we want to avoid investing in retention strategies for employees who are not actually going to leave (minimizing false positives), while ensuring employees who are truly at risk are flagged as such (maximizing true positives).</p>
<p>We will predict the class using both binomial logistic regression and RF models, trained on both balanced (SMOTE) and imbalanced class data, and store performance metrics in a single data frame for easy comparison:</p>
<div class="sourceCode" id="cb422"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb422-1"><a href="pred-mod.html#cb422-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize empty data frame for model performance stats</span></span>
<span id="cb422-2"><a href="pred-mod.html#cb422-2" aria-hidden="true" tabindex="-1"></a>class.perf.metrics <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb422-3"><a href="pred-mod.html#cb422-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb422-4"><a href="pred-mod.html#cb422-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Set probability threshold for classification</span></span>
<span id="cb422-5"><a href="pred-mod.html#cb422-5" aria-hidden="true" tabindex="-1"></a>prob_threshold <span class="ot">&lt;-</span> .<span class="dv">7</span></span>
<span id="cb422-6"><a href="pred-mod.html#cb422-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb422-7"><a href="pred-mod.html#cb422-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict with logistic regression model</span></span>
<span id="cb422-8"><a href="pred-mod.html#cb422-8" aria-hidden="true" tabindex="-1"></a>class.perf.metrics <span class="ot">&lt;-</span> <span class="fu">rbind</span>(class.perf.metrics, <span class="fu">cbind.data.frame</span>(</span>
<span id="cb422-9"><a href="pred-mod.html#cb422-9" aria-hidden="true" tabindex="-1"></a>                      <span class="at">model =</span> <span class="fu">rep</span>(<span class="st">&quot;GLM&quot;</span>, <span class="fu">nrow</span>(test_dat)),</span>
<span id="cb422-10"><a href="pred-mod.html#cb422-10" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">classifier.perf</span>(</span>
<span id="cb422-11"><a href="pred-mod.html#cb422-11" aria-hidden="true" tabindex="-1"></a>                      <span class="at">actual =</span> test_dat<span class="sc">$</span>active,</span>
<span id="cb422-12"><a href="pred-mod.html#cb422-12" aria-hidden="true" tabindex="-1"></a>                      <span class="at">predicted =</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(glm.fit, test_dat, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;=</span> prob_threshold, <span class="dv">1</span>, <span class="dv">0</span>))))</span>
<span id="cb422-13"><a href="pred-mod.html#cb422-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb422-14"><a href="pred-mod.html#cb422-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict with logistic regression model (SMOTE)</span></span>
<span id="cb422-15"><a href="pred-mod.html#cb422-15" aria-hidden="true" tabindex="-1"></a>class.perf.metrics <span class="ot">&lt;-</span> <span class="fu">rbind</span>(class.perf.metrics, <span class="fu">cbind.data.frame</span>(</span>
<span id="cb422-16"><a href="pred-mod.html#cb422-16" aria-hidden="true" tabindex="-1"></a>                      <span class="at">model =</span> <span class="fu">rep</span>(<span class="st">&quot;GLM (SMOTE)&quot;</span>, <span class="fu">nrow</span>(test_dat)),</span>
<span id="cb422-17"><a href="pred-mod.html#cb422-17" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">classifier.perf</span>(</span>
<span id="cb422-18"><a href="pred-mod.html#cb422-18" aria-hidden="true" tabindex="-1"></a>                      <span class="at">actual =</span> test_dat<span class="sc">$</span>active,</span>
<span id="cb422-19"><a href="pred-mod.html#cb422-19" aria-hidden="true" tabindex="-1"></a>                      <span class="at">predicted =</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(glm.os.fit, test_dat, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;=</span> prob_threshold, <span class="dv">1</span>, <span class="dv">0</span>))))</span>
<span id="cb422-20"><a href="pred-mod.html#cb422-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb422-21"><a href="pred-mod.html#cb422-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict with RF model</span></span>
<span id="cb422-22"><a href="pred-mod.html#cb422-22" aria-hidden="true" tabindex="-1"></a>class.perf.metrics <span class="ot">&lt;-</span> <span class="fu">rbind</span>(class.perf.metrics, <span class="fu">cbind.data.frame</span>(</span>
<span id="cb422-23"><a href="pred-mod.html#cb422-23" aria-hidden="true" tabindex="-1"></a>                      <span class="at">model =</span> <span class="fu">rep</span>(<span class="st">&quot;RF&quot;</span>, <span class="fu">nrow</span>(test_dat)),</span>
<span id="cb422-24"><a href="pred-mod.html#cb422-24" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">classifier.perf</span>(</span>
<span id="cb422-25"><a href="pred-mod.html#cb422-25" aria-hidden="true" tabindex="-1"></a>                      <span class="at">actual =</span> test_dat<span class="sc">$</span>active,</span>
<span id="cb422-26"><a href="pred-mod.html#cb422-26" aria-hidden="true" tabindex="-1"></a>                      <span class="at">predicted =</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(rf.fit, test_dat, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;=</span> prob_threshold, <span class="dv">1</span>, <span class="dv">0</span>))))</span>
<span id="cb422-27"><a href="pred-mod.html#cb422-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb422-28"><a href="pred-mod.html#cb422-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict with RF model (SMOTE)</span></span>
<span id="cb422-29"><a href="pred-mod.html#cb422-29" aria-hidden="true" tabindex="-1"></a>class.perf.metrics <span class="ot">&lt;-</span> <span class="fu">rbind</span>(class.perf.metrics, <span class="fu">cbind.data.frame</span>(</span>
<span id="cb422-30"><a href="pred-mod.html#cb422-30" aria-hidden="true" tabindex="-1"></a>                      <span class="at">model =</span> <span class="fu">rep</span>(<span class="st">&quot;RF (SMOTE)&quot;</span>, <span class="fu">nrow</span>(test_dat)),</span>
<span id="cb422-31"><a href="pred-mod.html#cb422-31" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">classifier.perf</span>(</span>
<span id="cb422-32"><a href="pred-mod.html#cb422-32" aria-hidden="true" tabindex="-1"></a>                      <span class="at">actual =</span> test_dat<span class="sc">$</span>active,</span>
<span id="cb422-33"><a href="pred-mod.html#cb422-33" aria-hidden="true" tabindex="-1"></a>                      <span class="at">predicted =</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(rf.os.fit, test_dat, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;=</span> prob_threshold, <span class="dv">1</span>, <span class="dv">0</span>))))</span></code></pre></div>
<ul>
<li><strong>Step 4</strong>: Evaluate model performance.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:class-mdl-perf"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/class-mdl-perf-1.png" alt="Classification performance for binomial logistic regression and RF models using balanced (SMOTE) and imbalanced classes" width="100%" />
<p class="caption">
Figure 13.5: Classification performance for binomial logistic regression and RF models using balanced (SMOTE) and imbalanced classes
</p>
</div>
<p>As we can see in Figure <a href="pred-mod.html#fig:class-mdl-perf">13.5</a>, with our stringent probability threshold set at <code>.7</code> for class delineation, all models had perfect specificity (correctly predicting those who stay) and precision (all employees who were predicted to attrit did). However, we see notable differences in sensitivity across the model types, and this is generally a very important performance measure in a predictive attrition project since the cost of not flagging employees who attrit can be costly. Sensitivity for the RF model trained on balanced classes (SMOTE) performed much better than its imbalanced RF counterpart (<code>95.6%</code> vs. <code>71.6%</code>), reinforcing that ML models are sensitive (no pun intended) to imbalanced classes.</p>
<p>The results also show that there is likely no benefit to compromising model interpretability by using a flexible ML model like RF since our trusty binomial logistic regression model performs just as well on these data. Nevertheless, we can construct what is known as a <strong>Variable Importance Plot</strong> on RF output using the <code>varImpPlot()</code> function from the <code>randomForest</code> library to understand the relative importance of each predictor in the model.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:var-imp-plot"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/var-imp-plot-1.png" alt="Variable importance plot for Random Forest model" width="100%" />
<p class="caption">
Figure 13.6: Variable importance plot for Random Forest model
</p>
</div>
<p>Variable importance plots are based on the mean decrease in <strong>Gini importance</strong>. Gini importance measures the average gain of purity by splits of a given variable. In other words, if a variable is useful it tends to split mixed labeled nodes (nodes with both employees who separated and employees who stayed) into pure single class nodes. The most important variables are at the top of the variable importance plot, indicating that without these variables nodes will not be as pure and as a result, classification performance will not be as strong.</p>
<p>Figure <a href="pred-mod.html#fig:var-imp-plot">13.6</a> shows that <code>interview_rating</code> is far more important than the second most important predictor, <code>overtime</code>. This is consistent with what we observed in the results of the binomial logistic regression models too.</p>
<p>Let’s compare the information from the RF’s Variable Importance Plot to a single decision tree built on training data with balanced classes. We can build and visualize a decision tree in R using the <code>rpart()</code> and <code>rpart.plot()</code> functions. <code>rpart</code> is an acronym for <em>recursive partitioning and regression trees</em>:</p>
<pre><code>## Warning: package &#39;rpart&#39; was built under R version 4.0.5</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rpart-tree"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/rpart-tree-1.png" alt="Decision tree for employee attrition prediction using training data with balanced classes (SMOTE)" width="75%" />
<p class="caption">
Figure 13.7: Decision tree for employee attrition prediction using training data with balanced classes (SMOTE)
</p>
</div>
<p>As shown in Figure <a href="pred-mod.html#fig:rpart-tree">13.7</a>, the most important predictor for splitting the data is <code>interview_rating</code> when using a single decision tree. At the root node, there is a 50 percent chance of leaving and staying, which is expected given we balanced the classes using SMOTE. A prediction that is no better than a fair coin toss is of course not helpful. Walking down to the leaf nodes, we can see that for the 50% of employees with <code>interview_rating &gt;= 3.5</code>, the algorithm predicts they will stay (<code>status = 0</code>); for the 50% of employees with <code>interview_rating &lt; 3.5</code>, the algorithm predicts they will leave (<code>status = 1</code>). Given what we observed in the results of the binomial logistic regression output, additional partitioning by <code>overtime</code> may further increase node purity on the test data since classes are mixed for those with interview ratings between <code>3.3</code> and <code>3.6</code>. However, given the strong performance splitting data only on <code>interview_rating</code>, further partitioning will likely result in modeling noise and overfitting.</p>
<p><strong>Forecasting</strong></p>
<p>To demonstrate the prediction workflow steps for forecasting, we will leverage our <code>turnover_trends</code> data set:</p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="pred-mod.html#cb424-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb424-2"><a href="pred-mod.html#cb424-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb424-3"><a href="pred-mod.html#cb424-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb424-4"><a href="pred-mod.html#cb424-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load employee data</span></span>
<span id="cb424-5"><a href="pred-mod.html#cb424-5" aria-hidden="true" tabindex="-1"></a>forecasting_dat <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_book/master/data/turnover_trends.csv&quot;</span>)</span></code></pre></div>
<ul>
<li><strong>Step 1</strong>: Partition data into training and test sets for cross-validation.</li>
</ul>
<p>Since we have 60 months of data for each combination of <code>job</code>, <code>level</code>, and <code>remote</code> in the data, we will select a combination for which <code>turnover_rate</code> can be projected for future months. To simplify, we will train a model using data for the first 48 months and test using the final 12 months.</p>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="pred-mod.html#cb425-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create training data</span></span>
<span id="cb425-2"><a href="pred-mod.html#cb425-2" aria-hidden="true" tabindex="-1"></a>train_dat <span class="ot">&lt;-</span> forecasting_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(job <span class="sc">==</span> <span class="st">&#39;People Scientist&#39;</span> <span class="sc">&amp;</span> level <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> year <span class="sc">%in%</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb425-3"><a href="pred-mod.html#cb425-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb425-4"><a href="pred-mod.html#cb425-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create test data</span></span>
<span id="cb425-5"><a href="pred-mod.html#cb425-5" aria-hidden="true" tabindex="-1"></a>test_dat <span class="ot">&lt;-</span> forecasting_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(job <span class="sc">==</span> <span class="st">&#39;People Scientist&#39;</span> <span class="sc">&amp;</span> level <span class="sc">==</span> <span class="dv">1</span> <span class="sc">&amp;</span> remote <span class="sc">==</span> <span class="st">&#39;Yes&#39;</span> <span class="sc">&amp;</span> year <span class="sc">==</span> <span class="dv">5</span>)</span></code></pre></div>
<ul>
<li><strong>Step 2</strong>: Build models using training data.</li>
</ul>
<p>Given the significant quadratic and cubic terms identified in Chapter <a href="lme.html#lme">11</a>, we will fit a cubic regression model on the training data.</p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="pred-mod.html#cb426-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit cubic model</span></span>
<span id="cb426-2"><a href="pred-mod.html#cb426-2" aria-hidden="true" tabindex="-1"></a>train.cube.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(turnover_rate <span class="sc">~</span> year <span class="sc">+</span> month <span class="sc">+</span> <span class="fu">I</span>(month<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(month<span class="sc">^</span><span class="dv">3</span>) <span class="sc">+</span> remote, <span class="at">data =</span> train_dat)</span>
<span id="cb426-3"><a href="pred-mod.html#cb426-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb426-4"><a href="pred-mod.html#cb426-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce model summary</span></span>
<span id="cb426-5"><a href="pred-mod.html#cb426-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(train.cube.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = turnover_rate ~ year + month + I(month^2) + I(month^3) + 
##     remote, data = train_dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.3360 -0.1605  0.0075  0.1680  0.3210 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.7650000  0.1594281  17.343  &lt; 2e-16 ***
## year        -0.1130000  0.0230955  -4.893 4.33e-06 ***
## month        2.4100000  0.0935806  25.753  &lt; 2e-16 ***
## I(month^2)  -0.4100000  0.0163907 -25.014  &lt; 2e-16 ***
## I(month^3)   0.0200000  0.0008311  24.064  &lt; 2e-16 ***
## remoteYes   -1.6400000  0.0516430 -31.756  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.253 on 90 degrees of freedom
## Multiple R-squared:  0.9499, Adjusted R-squared:  0.9471 
## F-statistic: 341.4 on 5 and 90 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>All linear, quadratic, and cubic terms on <code>month</code> are statistically significant at the <span class="math inline">\(p &lt; .001\)</span> level.</p>
<ul>
<li><strong>Step 3</strong>: Use models to make predictions on test data.</li>
</ul>
<p>We will again build a function to evaluate the performance of the fitted models applied to test data rather than using delivered functions. The following function will return <em>mean absolute deviation (MAD)</em>, <em>mean squared error (MSE)</em>, and <em>mean absolute percentage error (MAPE)</em>:</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="pred-mod.html#cb428-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Develop function that returns a data frame of forecasting model performance statistics</span></span>
<span id="cb428-2"><a href="pred-mod.html#cb428-2" aria-hidden="true" tabindex="-1"></a>forecast.perf <span class="ot">&lt;-</span> <span class="cf">function</span>(actual, predicted){</span>
<span id="cb428-3"><a href="pred-mod.html#cb428-3" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb428-4"><a href="pred-mod.html#cb428-4" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Check for missing values; metrics will be computed on non-missing values only</span></span>
<span id="cb428-5"><a href="pred-mod.html#cb428-5" aria-hidden="true" tabindex="-1"></a>  predicted <span class="ot">&lt;-</span> predicted[<span class="sc">!</span><span class="fu">is.na</span>(actual)]</span>
<span id="cb428-6"><a href="pred-mod.html#cb428-6" aria-hidden="true" tabindex="-1"></a>  actual <span class="ot">&lt;-</span> actual[<span class="sc">!</span><span class="fu">is.na</span>(actual)]</span>
<span id="cb428-7"><a href="pred-mod.html#cb428-7" aria-hidden="true" tabindex="-1"></a>  actual <span class="ot">&lt;-</span> actual[<span class="sc">!</span><span class="fu">is.na</span>(predicted)]</span>
<span id="cb428-8"><a href="pred-mod.html#cb428-8" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb428-9"><a href="pred-mod.html#cb428-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Store rates to variables</span></span>
<span id="cb428-10"><a href="pred-mod.html#cb428-10" aria-hidden="true" tabindex="-1"></a>  mad <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">mean</span>(<span class="fu">abs</span>(actual <span class="sc">-</span> predicted)), <span class="dv">2</span>)</span>
<span id="cb428-11"><a href="pred-mod.html#cb428-11" aria-hidden="true" tabindex="-1"></a>  mse <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">mean</span>((actual <span class="sc">-</span> predicted)<span class="sc">^</span><span class="dv">2</span>), <span class="dv">2</span>)</span>
<span id="cb428-12"><a href="pred-mod.html#cb428-12" aria-hidden="true" tabindex="-1"></a>  mape <span class="ot">&lt;-</span> <span class="fu">round</span>(<span class="fu">mean</span>(<span class="fu">abs</span>((actual <span class="sc">-</span> predicted) <span class="sc">/</span> actual)) <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">2</span>)</span>
<span id="cb428-13"><a href="pred-mod.html#cb428-13" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb428-14"><a href="pred-mod.html#cb428-14" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Return model performance statistics in a data frame</span></span>
<span id="cb428-15"><a href="pred-mod.html#cb428-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">data.frame</span>(mad, mse, mape))</span>
<span id="cb428-16"><a href="pred-mod.html#cb428-16" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb428-17"><a href="pred-mod.html#cb428-17" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>For this forecast, we will also produce a prediction interval. A <strong>prediction interval</strong> is a range of values that is likely to contain the outcome value for a single new observation given a set of predictors. For example, a 95% prediction interval of <code>[10 15]</code> indicates that we can be 95% confident that the observation for which a prediction is being made will have an actual outcome value between <code>10</code> and <code>15</code>. Note that this is very different from a confidence interval in inferential statistics, which is a range of values that likely contains the value of an unknown population parameter.</p>
<p>We can produce a prediction interval by passing an additional <code>interval = 'predict'</code> argument into the <code>predict()</code> function:</p>
<div class="sourceCode" id="cb429"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb429-1"><a href="pred-mod.html#cb429-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize empty data frames for model predictions and performance stats</span></span>
<span id="cb429-2"><a href="pred-mod.html#cb429-2" aria-hidden="true" tabindex="-1"></a>forecast.metrics <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb429-3"><a href="pred-mod.html#cb429-3" aria-hidden="true" tabindex="-1"></a>forecast.err.rates <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb429-4"><a href="pred-mod.html#cb429-4" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb429-5"><a href="pred-mod.html#cb429-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict on test_dat</span></span>
<span id="cb429-6"><a href="pred-mod.html#cb429-6" aria-hidden="true" tabindex="-1"></a>forecast.metrics <span class="ot">&lt;-</span> <span class="fu">rbind</span>(forecast.metrics, <span class="fu">cbind.data.frame</span>(</span>
<span id="cb429-7"><a href="pred-mod.html#cb429-7" aria-hidden="true" tabindex="-1"></a>                          <span class="at">month =</span> test_dat<span class="sc">$</span>month,</span>
<span id="cb429-8"><a href="pred-mod.html#cb429-8" aria-hidden="true" tabindex="-1"></a>                          <span class="at">actual =</span> test_dat<span class="sc">$</span>turnover_rate,</span>
<span id="cb429-9"><a href="pred-mod.html#cb429-9" aria-hidden="true" tabindex="-1"></a>                          <span class="at">predicted =</span> <span class="fu">predict</span>(train.cube.fit, test_dat, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>),</span>
<span id="cb429-10"><a href="pred-mod.html#cb429-10" aria-hidden="true" tabindex="-1"></a>                          <span class="at">lwr_bound =</span> <span class="fu">as.data.frame</span>(<span class="fu">predict</span>(train.cube.fit, test_dat, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>, <span class="at">interval =</span> <span class="st">&quot;predict&quot;</span>))<span class="sc">$</span>lwr,</span>
<span id="cb429-11"><a href="pred-mod.html#cb429-11" aria-hidden="true" tabindex="-1"></a>                          <span class="at">upr_bound =</span> <span class="fu">as.data.frame</span>(<span class="fu">predict</span>(train.cube.fit, test_dat, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>, <span class="at">interval =</span> <span class="st">&quot;predict&quot;</span>))<span class="sc">$</span>upr))</span></code></pre></div>
<ul>
<li><strong>Step 4</strong>: Evaluate model performance.</li>
</ul>
<p>Next, we can pass a vector of actual and corresponding predicted values into the <code>forecast.perf()</code> function to return MAD, MSE, and MAPE performance metrics for the fitted model applied to year 5 data.</p>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="pred-mod.html#cb430-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate error rates for year 5 forecast</span></span>
<span id="cb430-2"><a href="pred-mod.html#cb430-2" aria-hidden="true" tabindex="-1"></a><span class="fu">forecast.perf</span>(<span class="at">actual =</span> forecast.metrics<span class="sc">$</span>actual, <span class="at">predicted =</span> forecast.metrics<span class="sc">$</span>predicted)</span></code></pre></div>
<pre><code>##    mad   mse  mape
## 1 3.84 14.75 48.39</code></pre>
<p>Given 95% of the variance in turnover rates was explained by the cubic regression model fitted to our training data (<span class="math inline">\(R^2 = .95)\)</span>, these error rates are surprisingly high for the test data.</p>
<p>Evaluating the average turnover rate by year will help in reconciling the high <span class="math inline">\(R^2\)</span> on the training data with the high error rates on the test data:</p>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="pred-mod.html#cb432-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate year-wise turnover rate mean</span></span>
<span id="cb432-2"><a href="pred-mod.html#cb432-2" aria-hidden="true" tabindex="-1"></a>yr1_avg <span class="ot">&lt;-</span> train_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(remote <span class="sc">==</span> <span class="st">&#39;Yes&#39;</span> <span class="sc">&amp;</span> year <span class="sc">==</span> <span class="dv">1</span>) <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarize</span>(<span class="at">Mean =</span> <span class="fu">mean</span>(turnover_rate))</span>
<span id="cb432-3"><a href="pred-mod.html#cb432-3" aria-hidden="true" tabindex="-1"></a>yr2_avg <span class="ot">&lt;-</span> train_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(remote <span class="sc">==</span> <span class="st">&#39;Yes&#39;</span> <span class="sc">&amp;</span> year <span class="sc">==</span> <span class="dv">2</span>) <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarize</span>(<span class="at">Mean =</span> <span class="fu">mean</span>(turnover_rate))</span>
<span id="cb432-4"><a href="pred-mod.html#cb432-4" aria-hidden="true" tabindex="-1"></a>yr3_avg <span class="ot">&lt;-</span> train_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(remote <span class="sc">==</span> <span class="st">&#39;Yes&#39;</span> <span class="sc">&amp;</span> year <span class="sc">==</span> <span class="dv">3</span>) <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarize</span>(<span class="at">Mean =</span> <span class="fu">mean</span>(turnover_rate))</span>
<span id="cb432-5"><a href="pred-mod.html#cb432-5" aria-hidden="true" tabindex="-1"></a>yr4_avg <span class="ot">&lt;-</span> train_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">filter</span>(remote <span class="sc">==</span> <span class="st">&#39;Yes&#39;</span> <span class="sc">&amp;</span> year <span class="sc">==</span> <span class="dv">4</span>) <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarize</span>(<span class="at">Mean =</span> <span class="fu">mean</span>(turnover_rate))</span>
<span id="cb432-6"><a href="pred-mod.html#cb432-6" aria-hidden="true" tabindex="-1"></a>yr5_avg <span class="ot">&lt;-</span> test_dat <span class="sc">%&gt;%</span> dplyr<span class="sc">::</span><span class="fu">summarize</span>(<span class="at">Mean =</span> <span class="fu">mean</span>(turnover_rate))</span>
<span id="cb432-7"><a href="pred-mod.html#cb432-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb432-8"><a href="pred-mod.html#cb432-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Display year-wise turnover rate mean</span></span>
<span id="cb432-9"><a href="pred-mod.html#cb432-9" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">c</span>(yr1_avg, yr2_avg, yr3_avg, yr4_avg, yr5_avg))</span></code></pre></div>
<pre><code>## $Mean
## [1] 4.506667
## 
## $Mean
## [1] 4.816667
## 
## $Mean
## [1] 4.046667
## 
## $Mean
## [1] 4.386667
## 
## $Mean
## [1] 7.996667</code></pre>
<p>There is clearly a significant difference in average turnover for year 5 (test data) relative to years 1-4 (training data). Since the fitted model had no visibility into year 5 data, it did not account for the spike in turnover beyond year 4.</p>
<p>Differences are further evidenced in Figure <a href="pred-mod.html#fig:mdl-yr4-5">13.8</a>, in which actual values for year 5 are far and away outside the 95% prediction interval.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mdl-yr4-5"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/mdl-yr4-5-1.png" alt="Left: Fitted model (red dashed line) with good fit to year 4 training data (black dots). Right: Fitted model (red dashed line) with poor fit to year 5 test data (black dots). 95 percent prediction interval is represented by the red shaded area around the fit line." width="100%" />
<p class="caption">
Figure 13.8: Left: Fitted model (red dashed line) with good fit to year 4 training data (black dots). Right: Fitted model (red dashed line) with poor fit to year 5 test data (black dots). 95 percent prediction interval is represented by the red shaded area around the fit line.
</p>
</div>
<p>This is an important lesson that highlights the centrality of cross-validation in evaluating whether predictive models will generalize beyond the available data. We can easily fit a model that performs well on training data and claim that the model has exceptional accuracy. However, what matters in predictive modeling is how well the model performs on data it has not seen as part of the training process.</p>
</div>
<div id="review-questions-11" class="section level2" number="13.6">
<h2><span class="header-section-number">13.6</span> Review Questions</h2>
<ol style="list-style-type: decimal">
<li><p>What factors influence the balance between model interpretability and flexibility?</p></li>
<li><p>How does cross-validation (CV) help improve the performance of predictive models?</p></li>
<li><p>What is bias-variance tradeoff?</p></li>
<li><p>In a classification setting, what performance metrics are available in a confusion matrix?</p></li>
<li><p>What are some measures used to evaluate the performance of a forecast?</p></li>
<li><p>What is Synthetic Minority Oversampling Technique (SMOTE), and how does it help improve the performance of machine learning (ML) models?</p></li>
<li><p>How is the stack ranking of predictors in a variable importance plot determined?</p></li>
<li><p>How does a prediction interval differ from a confidence interval?</p></li>
<li><p>How can a prediction interval be calculated in R?</p></li>
<li><p>What does high <span class="math inline">\(R^2\)</span> on training data and high <span class="math inline">\(MSE\)</span> on test data indicate about the utility of a predictive model?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="log.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsup-lrn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["The_Fundamentals_of_People_Analytics.pdf", "The_Fundamentals_of_People_Analytics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
