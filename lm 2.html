<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Linear Regression | The Fundamentals of People Analytics: With Applications in R</title>
  <meta name="description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="generator" content="bookdown 0.24.4 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Linear Regression | The Fundamentals of People Analytics: With Applications in R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  <meta name="github-repo" content="crstarbuck/peopleanalytics-book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Linear Regression | The Fundamentals of People Analytics: With Applications in R" />
  
  <meta name="twitter:description" content="An end-to-end guide for successful analytics projects in the social sciences" />
  

<meta name="author" content="Craig Starbuck" />


<meta name="date" content="2022-09-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="aod.html"/>
<link rel="next" href="lme.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/viz-1.8.2/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-1.0.6.1/grViz.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Fundamentals of People Analytics: With Applications in R</a></li>

<li class="divider"></li>
<li><a href="dedication.html#dedication" id="toc-dedication">Dedication</a></li>
<li><a href="foreword.html#foreword" id="toc-foreword">Foreword</a></li>
<li><a href="preface.html#preface" id="toc-preface">Preface</a></li>
<li><a href="getting-started.html#getting-started" id="toc-getting-started"><span class="toc-section-number">1</span> Getting Started</a>
<ul>
<li><a href="getting-started.html#guiding-principles" id="toc-guiding-principles"><span class="toc-section-number">1.1</span> Guiding Principles</a>
<ul>
<li><a href="getting-started.html#pro-employee-thinking" id="toc-pro-employee-thinking"><span class="toc-section-number">1.1.1</span> Pro-Employee Thinking</a></li>
<li><a href="getting-started.html#quality" id="toc-quality"><span class="toc-section-number">1.1.2</span> Quality</a></li>
<li><a href="getting-started.html#prioritization" id="toc-prioritization"><span class="toc-section-number">1.1.3</span> Prioritization</a></li>
</ul></li>
<li><a href="getting-started.html#tooling" id="toc-tooling"><span class="toc-section-number">1.2</span> Tooling</a></li>
<li><a href="getting-started.html#data-sets" id="toc-data-sets"><span class="toc-section-number">1.3</span> Data Sets</a>
<ul>
<li><a href="getting-started.html#employees" id="toc-employees"><span class="toc-section-number">1.3.1</span> Employees</a></li>
<li><a href="getting-started.html#turnover-trends" id="toc-turnover-trends"><span class="toc-section-number">1.3.2</span> Turnover Trends</a></li>
<li><a href="getting-started.html#survey-responses" id="toc-survey-responses"><span class="toc-section-number">1.3.3</span> Survey Responses</a></li>
</ul></li>
<li><a href="getting-started.html#d-framework" id="toc-d-framework"><span class="toc-section-number">1.4</span> 4D Framework</a></li>
</ul></li>
<li><a href="r-intro.html#r-intro" id="toc-r-intro"><span class="toc-section-number">2</span> Introduction to R</a>
<ul>
<li><a href="r-intro.html#getting-started-1" id="toc-getting-started-1"><span class="toc-section-number">2.1</span> Getting Started</a>
<ul>
<li><a href="r-intro.html#installing-r" id="toc-installing-r"><span class="toc-section-number">2.1.1</span> Installing R</a></li>
<li><a href="r-intro.html#installing-r-studio" id="toc-installing-r-studio"><span class="toc-section-number">2.1.2</span> Installing R Studio</a></li>
<li><a href="r-intro.html#installing-packages" id="toc-installing-packages"><span class="toc-section-number">2.1.3</span> Installing Packages</a></li>
<li><a href="r-intro.html#case-sensitivity" id="toc-case-sensitivity"><span class="toc-section-number">2.1.4</span> Case Sensitivity</a></li>
<li><a href="r-intro.html#help" id="toc-help"><span class="toc-section-number">2.1.5</span> Help</a></li>
<li><a href="r-intro.html#objects" id="toc-objects"><span class="toc-section-number">2.1.6</span> Objects</a></li>
<li><a href="r-intro.html#comments" id="toc-comments"><span class="toc-section-number">2.1.7</span> Comments</a></li>
<li><a href="r-intro.html#testing-early-and-often" id="toc-testing-early-and-often"><span class="toc-section-number">2.1.8</span> Testing Early and Often</a></li>
</ul></li>
<li><a href="r-intro.html#vectors" id="toc-vectors"><span class="toc-section-number">2.2</span> Vectors</a></li>
<li><a href="r-intro.html#matrices" id="toc-matrices"><span class="toc-section-number">2.3</span> Matrices</a></li>
<li><a href="r-intro.html#factors" id="toc-factors"><span class="toc-section-number">2.4</span> Factors</a></li>
<li><a href="r-intro.html#data-frames" id="toc-data-frames"><span class="toc-section-number">2.5</span> Data Frames</a></li>
<li><a href="r-intro.html#lists" id="toc-lists"><span class="toc-section-number">2.6</span> Lists</a></li>
<li><a href="r-intro.html#loops" id="toc-loops"><span class="toc-section-number">2.7</span> Loops</a></li>
<li><a href="r-intro.html#user-defined-functions-udfs" id="toc-user-defined-functions-udfs"><span class="toc-section-number">2.8</span> User-Defined Functions (UDFs)</a></li>
<li><a href="r-intro.html#graphics" id="toc-graphics"><span class="toc-section-number">2.9</span> Graphics</a></li>
<li><a href="r-intro.html#review-questions" id="toc-review-questions"><span class="toc-section-number">2.10</span> Review Questions</a></li>
</ul></li>
<li><a href="sql-intro.html#sql-intro" id="toc-sql-intro"><span class="toc-section-number">3</span> Introduction to SQL</a>
<ul>
<li><a href="sql-intro.html#basics" id="toc-basics"><span class="toc-section-number">3.1</span> Basics</a></li>
<li><a href="sql-intro.html#aggregate-functions" id="toc-aggregate-functions"><span class="toc-section-number">3.2</span> Aggregate Functions</a></li>
<li><a href="sql-intro.html#joins" id="toc-joins"><span class="toc-section-number">3.3</span> Joins</a></li>
<li><a href="sql-intro.html#subqueries" id="toc-subqueries"><span class="toc-section-number">3.4</span> Subqueries</a></li>
<li><a href="sql-intro.html#virtual-tables" id="toc-virtual-tables"><span class="toc-section-number">3.5</span> Virtual Tables</a></li>
<li><a href="sql-intro.html#window-functions" id="toc-window-functions"><span class="toc-section-number">3.6</span> Window Functions</a></li>
<li><a href="sql-intro.html#common-table-expressions-ctes" id="toc-common-table-expressions-ctes"><span class="toc-section-number">3.7</span> Common Table Expressions (CTEs)</a></li>
<li><a href="sql-intro.html#review-questions-1" id="toc-review-questions-1"><span class="toc-section-number">3.8</span> Review Questions</a></li>
</ul></li>
<li><a href="measure-sampl.html#measure-sampl" id="toc-measure-sampl"><span class="toc-section-number">4</span> Measurement &amp; Sampling</a>
<ul>
<li><a href="measure-sampl.html#variable-types" id="toc-variable-types"><span class="toc-section-number">4.1</span> Variable Types</a>
<ul>
<li><a href="measure-sampl.html#independent-variables-iv" id="toc-independent-variables-iv"><span class="toc-section-number">4.1.1</span> Independent Variables (IV)</a></li>
<li><a href="measure-sampl.html#dependent-variables-dv" id="toc-dependent-variables-dv"><span class="toc-section-number">4.1.2</span> Dependent Variables (DV)</a></li>
<li><a href="measure-sampl.html#control-variables-cv" id="toc-control-variables-cv"><span class="toc-section-number">4.1.3</span> Control Variables (CV)</a></li>
<li><a href="measure-sampl.html#moderating-variables" id="toc-moderating-variables"><span class="toc-section-number">4.1.4</span> Moderating Variables</a></li>
<li><a href="measure-sampl.html#mediating-variables" id="toc-mediating-variables"><span class="toc-section-number">4.1.5</span> Mediating Variables</a></li>
<li><a href="measure-sampl.html#endogenous-vs.-exogenous-variables" id="toc-endogenous-vs.-exogenous-variables"><span class="toc-section-number">4.1.6</span> Endogenous vs. Exogenous Variables</a></li>
</ul></li>
<li><a href="measure-sampl.html#measurement-scales" id="toc-measurement-scales"><span class="toc-section-number">4.2</span> Measurement Scales</a>
<ul>
<li><a href="measure-sampl.html#discrete-variables" id="toc-discrete-variables"><span class="toc-section-number">4.2.1</span> Discrete Variables</a></li>
<li><a href="measure-sampl.html#continuous-variables" id="toc-continuous-variables"><span class="toc-section-number">4.2.2</span> Continuous Variables</a></li>
</ul></li>
<li><a href="measure-sampl.html#sampling-methods" id="toc-sampling-methods"><span class="toc-section-number">4.3</span> Sampling Methods</a>
<ul>
<li><a href="measure-sampl.html#probability-sampling" id="toc-probability-sampling"><span class="toc-section-number">4.3.1</span> Probability Sampling</a></li>
<li><a href="measure-sampl.html#non-probability-sampling" id="toc-non-probability-sampling"><span class="toc-section-number">4.3.2</span> Non-Probability Sampling</a></li>
</ul></li>
<li><a href="measure-sampl.html#sampling-nonsampling-error" id="toc-sampling-nonsampling-error"><span class="toc-section-number">4.4</span> Sampling &amp; Nonsampling Error</a>
<ul>
<li><a href="measure-sampl.html#sampling-error" id="toc-sampling-error"><span class="toc-section-number">4.4.1</span> Sampling Error</a></li>
<li><a href="measure-sampl.html#nonsampling-error" id="toc-nonsampling-error"><span class="toc-section-number">4.4.2</span> Nonsampling Error</a></li>
</ul></li>
<li><a href="measure-sampl.html#reliability-and-validity" id="toc-reliability-and-validity"><span class="toc-section-number">4.5</span> Reliability and Validity</a>
<ul>
<li><a href="measure-sampl.html#reliability" id="toc-reliability"><span class="toc-section-number">4.5.1</span> Reliability</a></li>
<li><a href="measure-sampl.html#validity" id="toc-validity"><span class="toc-section-number">4.5.2</span> Validity</a></li>
</ul></li>
<li><a href="measure-sampl.html#review-questions-2" id="toc-review-questions-2"><span class="toc-section-number">4.6</span> Review Questions</a></li>
</ul></li>
<li><a href="research.html#research" id="toc-research"><span class="toc-section-number">5</span> Research Fundamentals</a>
<ul>
<li><a href="research.html#research-questions" id="toc-research-questions"><span class="toc-section-number">5.1</span> Research Questions</a></li>
<li><a href="research.html#research-hypotheses" id="toc-research-hypotheses"><span class="toc-section-number">5.2</span> Research Hypotheses</a></li>
<li><a href="research.html#internal-vs.-external-validity" id="toc-internal-vs.-external-validity"><span class="toc-section-number">5.3</span> Internal vs. External Validity</a></li>
<li><a href="research.html#research-methods" id="toc-research-methods"><span class="toc-section-number">5.4</span> Research Methods</a></li>
<li><a href="research.html#research-designs" id="toc-research-designs"><span class="toc-section-number">5.5</span> Research Designs</a></li>
<li><a href="research.html#review-questions-3" id="toc-review-questions-3"><span class="toc-section-number">5.6</span> Review Questions</a></li>
</ul></li>
<li><a href="data-prep.html#data-prep" id="toc-data-prep"><span class="toc-section-number">6</span> Data Preparation</a>
<ul>
<li><a href="data-prep.html#data-extraction" id="toc-data-extraction"><span class="toc-section-number">6.1</span> Data Extraction</a>
<ul>
<li><a href="data-prep.html#data-architecture" id="toc-data-architecture"><span class="toc-section-number">6.1.1</span> Data Architecture</a></li>
<li><a href="data-prep.html#database-normalization" id="toc-database-normalization"><span class="toc-section-number">6.1.2</span> Database Normalization</a></li>
<li><a href="data-prep.html#modern-data-infrastructure" id="toc-modern-data-infrastructure"><span class="toc-section-number">6.1.3</span> Modern Data Infrastructure</a></li>
</ul></li>
<li><a href="data-prep.html#data-screening-cleaning" id="toc-data-screening-cleaning"><span class="toc-section-number">6.2</span> Data Screening &amp; Cleaning</a>
<ul>
<li><a href="data-prep.html#missingness" id="toc-missingness"><span class="toc-section-number">6.2.1</span> Missingness</a></li>
<li><a href="data-prep.html#outliers" id="toc-outliers"><span class="toc-section-number">6.2.2</span> Outliers</a></li>
<li><a href="data-prep.html#low-variability" id="toc-low-variability"><span class="toc-section-number">6.2.3</span> Low Variability</a></li>
<li><a href="data-prep.html#inconsistent-categories" id="toc-inconsistent-categories"><span class="toc-section-number">6.2.4</span> Inconsistent Categories</a></li>
<li><a href="data-prep.html#data-binning" id="toc-data-binning"><span class="toc-section-number">6.2.5</span> Data Binning</a></li>
</ul></li>
<li><a href="data-prep.html#one-hot-encoding" id="toc-one-hot-encoding"><span class="toc-section-number">6.3</span> One-Hot Encoding</a></li>
<li><a href="data-prep.html#feature-engineering" id="toc-feature-engineering"><span class="toc-section-number">6.4</span> Feature Engineering</a></li>
<li><a href="data-prep.html#review-questions-4" id="toc-review-questions-4"><span class="toc-section-number">6.5</span> Review Questions</a></li>
</ul></li>
<li><a href="desc-stats.html#desc-stats" id="toc-desc-stats"><span class="toc-section-number">7</span> Descriptive Statistics</a>
<ul>
<li><a href="desc-stats.html#univariate-analysis" id="toc-univariate-analysis"><span class="toc-section-number">7.1</span> Univariate Analysis</a>
<ul>
<li><a href="desc-stats.html#measures-of-central-tendency" id="toc-measures-of-central-tendency"><span class="toc-section-number">7.1.1</span> Measures of Central Tendency</a></li>
<li><a href="desc-stats.html#measures-of-spread" id="toc-measures-of-spread"><span class="toc-section-number">7.1.2</span> Measures of Spread</a></li>
</ul></li>
<li><a href="desc-stats.html#bivariate-analysis" id="toc-bivariate-analysis"><span class="toc-section-number">7.2</span> Bivariate Analysis</a>
<ul>
<li><a href="desc-stats.html#covariance" id="toc-covariance"><span class="toc-section-number">7.2.1</span> Covariance</a></li>
<li><a href="desc-stats.html#correlation" id="toc-correlation"><span class="toc-section-number">7.2.2</span> Correlation</a></li>
</ul></li>
<li><a href="desc-stats.html#review-questions-5" id="toc-review-questions-5"><span class="toc-section-number">7.3</span> Review Questions</a></li>
</ul></li>
<li><a href="inf-stats.html#inf-stats" id="toc-inf-stats"><span class="toc-section-number">8</span> Statistical Inference</a>
<ul>
<li><a href="inf-stats.html#introduction-to-probability" id="toc-introduction-to-probability"><span class="toc-section-number">8.1</span> Introduction to Probability</a>
<ul>
<li><a href="inf-stats.html#probability-distributions" id="toc-probability-distributions"><span class="toc-section-number">8.1.1</span> Probability Distributions</a></li>
<li><a href="inf-stats.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">8.1.2</span> Conditional Probability</a></li>
</ul></li>
<li><a href="inf-stats.html#central-limit-theorem" id="toc-central-limit-theorem"><span class="toc-section-number">8.2</span> Central Limit Theorem</a></li>
<li><a href="inf-stats.html#confidence-intervals" id="toc-confidence-intervals"><span class="toc-section-number">8.3</span> Confidence Intervals</a>
<ul>
<li><a href="inf-stats.html#hypothesis-testing" id="toc-hypothesis-testing"><span class="toc-section-number">8.3.1</span> Hypothesis Testing</a></li>
<li><a href="inf-stats.html#alpha" id="toc-alpha"><span class="toc-section-number">8.3.2</span> Alpha</a></li>
<li><a href="inf-stats.html#type-i-ii-errors" id="toc-type-i-ii-errors"><span class="toc-section-number">8.3.3</span> Type I &amp; II Errors</a></li>
<li><a href="inf-stats.html#p-values" id="toc-p-values"><span class="toc-section-number">8.3.4</span> <span class="math inline">\(p\)</span>-Values</a></li>
<li><a href="inf-stats.html#bonferroni-correction" id="toc-bonferroni-correction"><span class="toc-section-number">8.3.5</span> Bonferroni Correction</a></li>
<li><a href="inf-stats.html#statistical-power" id="toc-statistical-power"><span class="toc-section-number">8.3.6</span> Statistical Power</a></li>
</ul></li>
<li><a href="inf-stats.html#review-questions-6" id="toc-review-questions-6"><span class="toc-section-number">8.4</span> Review Questions</a></li>
</ul></li>
<li><a href="aod.html#aod" id="toc-aod"><span class="toc-section-number">9</span> Analysis of Differences</a>
<ul>
<li><a href="aod.html#parametric-vs.-nonparametric-tests" id="toc-parametric-vs.-nonparametric-tests"><span class="toc-section-number">9.1</span> Parametric vs. Nonparametric Tests</a></li>
<li><a href="aod.html#differences-in-discrete-data" id="toc-differences-in-discrete-data"><span class="toc-section-number">9.2</span> Differences in Discrete Data</a></li>
<li><a href="aod.html#differences-in-continuous-data" id="toc-differences-in-continuous-data"><span class="toc-section-number">9.3</span> Differences in Continuous Data</a></li>
<li><a href="aod.html#review-questions-7" id="toc-review-questions-7"><span class="toc-section-number">9.4</span> Review Questions</a></li>
</ul></li>
<li><a href="lm.html#lm" id="toc-lm"><span class="toc-section-number">10</span> Linear Regression</a>
<ul>
<li><a href="lm.html#sample-size" id="toc-sample-size"><span class="toc-section-number">10.1</span> Sample Size</a></li>
<li><a href="lm.html#simple-linear-regression" id="toc-simple-linear-regression"><span class="toc-section-number">10.2</span> Simple Linear Regression</a></li>
<li><a href="lm.html#multiple-linear-regression" id="toc-multiple-linear-regression"><span class="toc-section-number">10.3</span> Multiple Linear Regression</a></li>
<li><a href="lm.html#moderation" id="toc-moderation"><span class="toc-section-number">10.4</span> Moderation</a></li>
<li><a href="lm.html#mediation" id="toc-mediation"><span class="toc-section-number">10.5</span> Mediation</a></li>
<li><a href="lm.html#review-questions-8" id="toc-review-questions-8"><span class="toc-section-number">10.6</span> Review Questions</a></li>
</ul></li>
<li><a href="lme.html#lme" id="toc-lme"><span class="toc-section-number">11</span> Linear Model Extensions</a>
<ul>
<li><a href="lme.html#model-comparisons" id="toc-model-comparisons"><span class="toc-section-number">11.1</span> Model Comparisons</a></li>
<li><a href="lme.html#hierarchical-regression" id="toc-hierarchical-regression"><span class="toc-section-number">11.2</span> Hierarchical Regression</a></li>
<li><a href="lme.html#multilevel-models" id="toc-multilevel-models"><span class="toc-section-number">11.3</span> Multilevel Models</a></li>
<li><a href="lme.html#polynomial-regression" id="toc-polynomial-regression"><span class="toc-section-number">11.4</span> Polynomial Regression</a></li>
<li><a href="lme.html#review-questions-9" id="toc-review-questions-9"><span class="toc-section-number">11.5</span> Review Questions</a></li>
</ul></li>
<li><a href="log.html#log" id="toc-log"><span class="toc-section-number">12</span> Logistic Regression</a>
<ul>
<li><a href="log.html#binomial-logistic-regression" id="toc-binomial-logistic-regression"><span class="toc-section-number">12.1</span> Binomial Logistic Regression</a></li>
<li><a href="log.html#multinomial-logistic-regression" id="toc-multinomial-logistic-regression"><span class="toc-section-number">12.2</span> Multinomial Logistic Regression</a></li>
<li><a href="log.html#ordinal-logistic-regression" id="toc-ordinal-logistic-regression"><span class="toc-section-number">12.3</span> Ordinal Logistic Regression</a></li>
<li><a href="log.html#review-questions-10" id="toc-review-questions-10"><span class="toc-section-number">12.4</span> Review Questions</a></li>
</ul></li>
<li><a href="pred-mod.html#pred-mod" id="toc-pred-mod"><span class="toc-section-number">13</span> Predictive Modeling</a>
<ul>
<li><a href="pred-mod.html#cross-validation" id="toc-cross-validation"><span class="toc-section-number">13.1</span> Cross-Validation</a></li>
<li><a href="pred-mod.html#model-performance" id="toc-model-performance"><span class="toc-section-number">13.2</span> Model Performance</a></li>
<li><a href="pred-mod.html#bias-variance-tradeoff" id="toc-bias-variance-tradeoff"><span class="toc-section-number">13.3</span> Bias-Variance Tradeoff</a></li>
<li><a href="pred-mod.html#tree-based-algorithms" id="toc-tree-based-algorithms"><span class="toc-section-number">13.4</span> Tree-Based Algorithms</a></li>
<li><a href="pred-mod.html#predictive-modeling" id="toc-predictive-modeling"><span class="toc-section-number">13.5</span> Predictive Modeling</a></li>
<li><a href="pred-mod.html#review-questions-11" id="toc-review-questions-11"><span class="toc-section-number">13.6</span> Review Questions</a></li>
</ul></li>
<li><a href="unsup-lrn.html#unsup-lrn" id="toc-unsup-lrn"><span class="toc-section-number">14</span> Unsupervised Learning</a>
<ul>
<li><a href="unsup-lrn.html#factor-analysis" id="toc-factor-analysis"><span class="toc-section-number">14.1</span> Factor Analysis</a>
<ul>
<li><a href="unsup-lrn.html#exploratory-factor-analysis-efa" id="toc-exploratory-factor-analysis-efa"><span class="toc-section-number">14.1.1</span> Exploratory Factor Analysis (EFA)</a></li>
<li><a href="unsup-lrn.html#confirmatory-factor-analysis-cfa" id="toc-confirmatory-factor-analysis-cfa"><span class="toc-section-number">14.1.2</span> Confirmatory Factor Analysis (CFA)</a></li>
</ul></li>
<li><a href="unsup-lrn.html#clustering" id="toc-clustering"><span class="toc-section-number">14.2</span> Clustering</a>
<ul>
<li><a href="unsup-lrn.html#k-means-clustering" id="toc-k-means-clustering"><span class="toc-section-number">14.2.1</span> <span class="math inline">\(K\)</span>-Means Clustering</a></li>
<li><a href="unsup-lrn.html#hierarchical-clustering" id="toc-hierarchical-clustering"><span class="toc-section-number">14.2.2</span> Hierarchical Clustering</a></li>
</ul></li>
<li><a href="unsup-lrn.html#review-questions-12" id="toc-review-questions-12"><span class="toc-section-number">14.3</span> Review Questions</a></li>
</ul></li>
<li><a href="data-viz.html#data-viz" id="toc-data-viz"><span class="toc-section-number">15</span> Data Visualization</a>
<ul>
<li><a href="data-viz.html#best-practices" id="toc-best-practices"><span class="toc-section-number">15.1</span> Best Practices</a>
<ul>
<li><a href="data-viz.html#color-palette" id="toc-color-palette"><span class="toc-section-number">15.1.1</span> Color Palette</a></li>
<li><a href="data-viz.html#chart-borders" id="toc-chart-borders"><span class="toc-section-number">15.1.2</span> Chart Borders</a></li>
<li><a href="data-viz.html#zero-baseline" id="toc-zero-baseline"><span class="toc-section-number">15.1.3</span> Zero Baseline</a></li>
<li><a href="data-viz.html#intuitive-layout" id="toc-intuitive-layout"><span class="toc-section-number">15.1.4</span> Intuitive Layout</a></li>
<li><a href="data-viz.html#preattentive-attributes" id="toc-preattentive-attributes"><span class="toc-section-number">15.1.5</span> Preattentive Attributes</a></li>
</ul></li>
<li><a href="data-viz.html#step-by-step-visual-upgrade" id="toc-step-by-step-visual-upgrade"><span class="toc-section-number">15.2</span> Step-by-Step Visual Upgrade</a>
<ul>
<li><a href="data-viz.html#step-1-build-bar-chart-with-defaults" id="toc-step-1-build-bar-chart-with-defaults"><span class="toc-section-number">15.2.1</span> Step 1: Build Bar Chart with Defaults</a></li>
<li><a href="data-viz.html#step-2-remove-legend" id="toc-step-2-remove-legend"><span class="toc-section-number">15.2.2</span> Step 2: Remove Legend</a></li>
<li><a href="data-viz.html#step-3-assign-colors-strategically" id="toc-step-3-assign-colors-strategically"><span class="toc-section-number">15.2.3</span> Step 3: Assign Colors Strategically</a></li>
<li><a href="data-viz.html#step-4-add-axis-titles-and-margins" id="toc-step-4-add-axis-titles-and-margins"><span class="toc-section-number">15.2.4</span> Step 4: Add Axis Titles and Margins</a></li>
<li><a href="data-viz.html#step-5-add-left-justified-title" id="toc-step-5-add-left-justified-title"><span class="toc-section-number">15.2.5</span> Step 5: Add Left-Justified Title</a></li>
<li><a href="data-viz.html#step-6-remove-background" id="toc-step-6-remove-background"><span class="toc-section-number">15.2.6</span> Step 6: Remove Background</a></li>
<li><a href="data-viz.html#step-7-remove-axis-ticks" id="toc-step-7-remove-axis-ticks"><span class="toc-section-number">15.2.7</span> Step 7: Remove Axis Ticks</a></li>
<li><a href="data-viz.html#step-8-mute-titles" id="toc-step-8-mute-titles"><span class="toc-section-number">15.2.8</span> Step 8: Mute Titles</a></li>
<li><a href="data-viz.html#step-9-flip-axes" id="toc-step-9-flip-axes"><span class="toc-section-number">15.2.9</span> Step 9: Flip Axes</a></li>
<li><a href="data-viz.html#step-10-sort-data" id="toc-step-10-sort-data"><span class="toc-section-number">15.2.10</span> Step 10: Sort Data</a></li>
</ul></li>
<li><a href="data-viz.html#visualization-types" id="toc-visualization-types"><span class="toc-section-number">15.3</span> Visualization Types</a>
<ul>
<li><a href="data-viz.html#tables" id="toc-tables"><span class="toc-section-number">15.3.1</span> Tables</a></li>
<li><a href="data-viz.html#heatmaps" id="toc-heatmaps"><span class="toc-section-number">15.3.2</span> Heatmaps</a></li>
<li><a href="data-viz.html#scatterplots" id="toc-scatterplots"><span class="toc-section-number">15.3.3</span> Scatterplots</a></li>
<li><a href="data-viz.html#line-graphs" id="toc-line-graphs"><span class="toc-section-number">15.3.4</span> Line Graphs</a></li>
<li><a href="data-viz.html#slopegraphs" id="toc-slopegraphs"><span class="toc-section-number">15.3.5</span> Slopegraphs</a></li>
<li><a href="data-viz.html#bar-charts" id="toc-bar-charts"><span class="toc-section-number">15.3.6</span> Bar Charts</a></li>
<li><a href="data-viz.html#combination-charts" id="toc-combination-charts"><span class="toc-section-number">15.3.7</span> Combination Charts</a></li>
<li><a href="data-viz.html#waterfall-charts" id="toc-waterfall-charts"><span class="toc-section-number">15.3.8</span> Waterfall Charts</a></li>
<li><a href="data-viz.html#waffle-charts" id="toc-waffle-charts"><span class="toc-section-number">15.3.9</span> Waffle Charts</a></li>
<li><a href="data-viz.html#sankey-diagrams" id="toc-sankey-diagrams"><span class="toc-section-number">15.3.10</span> Sankey Diagrams</a></li>
<li><a href="data-viz.html#pie-charts" id="toc-pie-charts"><span class="toc-section-number">15.3.11</span> Pie Charts</a></li>
<li><a href="data-viz.html#d-visuals" id="toc-d-visuals"><span class="toc-section-number">15.3.12</span> 3D Visuals</a></li>
</ul></li>
<li><a href="data-viz.html#elegant-data-visualization" id="toc-elegant-data-visualization"><span class="toc-section-number">15.4</span> Elegant Data Visualization</a></li>
<li><a href="data-viz.html#review-questions-13" id="toc-review-questions-13"><span class="toc-section-number">15.5</span> Review Questions</a></li>
</ul></li>
<li><a href="storytelling.html#storytelling" id="toc-storytelling"><span class="toc-section-number">16</span> Data Storytelling</a>
<ul>
<li><a href="storytelling.html#know-your-audience" id="toc-know-your-audience"><span class="toc-section-number">16.1</span> Know Your Audience</a></li>
<li><a href="storytelling.html#production-status" id="toc-production-status"><span class="toc-section-number">16.2</span> Production Status</a></li>
<li><a href="storytelling.html#structural-elements" id="toc-structural-elements"><span class="toc-section-number">16.3</span> Structural Elements</a>
<ul>
<li><a href="storytelling.html#tldr" id="toc-tldr"><span class="toc-section-number">16.3.1</span> TL;DR</a></li>
<li><a href="storytelling.html#purpose" id="toc-purpose"><span class="toc-section-number">16.3.2</span> Purpose</a></li>
<li><a href="storytelling.html#methodology" id="toc-methodology"><span class="toc-section-number">16.3.3</span> Methodology</a></li>
<li><a href="storytelling.html#results" id="toc-results"><span class="toc-section-number">16.3.4</span> Results</a></li>
<li><a href="storytelling.html#limitations" id="toc-limitations"><span class="toc-section-number">16.3.5</span> Limitations</a></li>
<li><a href="storytelling.html#next-steps" id="toc-next-steps"><span class="toc-section-number">16.3.6</span> Next Steps</a></li>
<li><a href="storytelling.html#appendix" id="toc-appendix"><span class="toc-section-number">16.3.7</span> Appendix</a></li>
</ul></li>
<li><a href="storytelling.html#qa" id="toc-qa"><span class="toc-section-number">16.4</span> Q&amp;A</a></li>
<li><a href="storytelling.html#review-questions-14" id="toc-review-questions-14"><span class="toc-section-number">16.5</span> Review Questions</a></li>
</ul></li>
<li><a href="bibli.html#bibli" id="toc-bibli"><span class="toc-section-number">17</span> Bibliography</a></li>
<li><a href="storytelling.html#appendix" id="toc-appendix"><span class="toc-section-number">18</span> Appendix</a>
<ul>
<li><a href="appendix.html#d-framework-1" id="toc-d-framework-1"><span class="toc-section-number">18.1</span> 4D Framework</a>
<ul>
<li><a href="appendix.html#discover" id="toc-discover"><span class="toc-section-number">18.1.1</span> Discover</a></li>
<li><a href="appendix.html#design" id="toc-design"><span class="toc-section-number">18.1.2</span> Design</a></li>
<li><a href="appendix.html#develop" id="toc-develop"><span class="toc-section-number">18.1.3</span> Develop</a></li>
<li><a href="appendix.html#deliver" id="toc-deliver"><span class="toc-section-number">18.1.4</span> Deliver</a></li>
</ul></li>
<li><a href="appendix.html#data-visualization" id="toc-data-visualization"><span class="toc-section-number">18.2</span> Data Visualization</a>
<ul>
<li><a href="appendix.html#step-by-step-visual-upgrade-1" id="toc-step-by-step-visual-upgrade-1"><span class="toc-section-number">18.2.1</span> Step-by-Step Visual Upgrade</a></li>
<li><a href="appendix.html#tables-1" id="toc-tables-1"><span class="toc-section-number">18.2.2</span> Tables</a></li>
<li><a href="appendix.html#heatmaps-1" id="toc-heatmaps-1"><span class="toc-section-number">18.2.3</span> Heatmaps</a></li>
<li><a href="appendix.html#scatterplots-1" id="toc-scatterplots-1"><span class="toc-section-number">18.2.4</span> Scatterplots</a></li>
<li><a href="appendix.html#line-charts" id="toc-line-charts"><span class="toc-section-number">18.2.5</span> Line Charts</a></li>
<li><a href="appendix.html#slopegraphs-1" id="toc-slopegraphs-1"><span class="toc-section-number">18.2.6</span> Slopegraphs</a></li>
<li><a href="appendix.html#bar-charts-1" id="toc-bar-charts-1"><span class="toc-section-number">18.2.7</span> Bar Charts</a></li>
<li><a href="appendix.html#combination-charts-1" id="toc-combination-charts-1"><span class="toc-section-number">18.2.8</span> Combination Charts</a></li>
<li><a href="appendix.html#waterfall-charts-1" id="toc-waterfall-charts-1"><span class="toc-section-number">18.2.9</span> Waterfall Charts</a></li>
<li><a href="appendix.html#waffle-charts-1" id="toc-waffle-charts-1"><span class="toc-section-number">18.2.10</span> Waffle Charts</a></li>
<li><a href="appendix.html#sankey-diagrams-1" id="toc-sankey-diagrams-1"><span class="toc-section-number">18.2.11</span> Sankey Diagrams</a></li>
<li><a href="appendix.html#pie-charts-1" id="toc-pie-charts-1"><span class="toc-section-number">18.2.12</span> Pie Charts</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Fundamentals of People Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lm" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Linear Regression</h1>
<p>It’s important to draw a distinction between inferential and predictive models. Inferential models are highly interpretable and their utility is largely in understanding the nature and magnitude of the effect variables have on outcomes. Inferential models also lend to quantifying the extent to which we can generalize the observed effects to the larger population from which the sample was drawn. The objective in predictive modeling is to also to learn from patterns in historical data but for the purpose of achieving the most accurate predictions for future events – even at the expense of interpretability. To be clear, this isn’t to say that predictive models cannot be interpreted – they certainly can – but there are relatively few applications for predictive modeling in people analytics because models generally need to be highly interpretable to support action planning.</p>
<p>This chapter is dedicated to inferential modeling to support a working understanding of how to interpret model output and communicate clear, data-driven narratives that respect the nuance and noise characteristic of people data. Chapter <a href="pred-mod.html#pred-mod">13</a> will provide an overview of predictive modeling frameworks.</p>
<p>Regression is perhaps the most important statistical learning technique for people analytics. If you have taken a statistics course at the undergraduate or graduate levels, you have surely already encountered it. Let’s first develop an intuitive understanding of the mechanics of regression.</p>
<p>Imagine we are sitting at a large public park in NYC on a nice fall afternoon. If asked to estimate the annual compensation of the next person to walk by, how would you estimate this in the absence of any additional information? Most would likely estimate the <em>average</em> annual compensation of everyone capable of walking by. Since this would include both residents and visitors, this would be a very large population of people! The obvious limitation with this approach is that among the large group of people capable of walking by, there is likely a significant range of annual compensation values. Many walking by may be children, unemployed, or retirees who earn no annual compensation, while others may be highly compensated senior executives at the pinnacle of their careers. Since the range of annual compensation could be zero to millions of dollars, estimating the average of such a large population is likely going to be highly inaccurate without more information.</p>
<p>Let’s consider that we are sitting outside on a weekday afternoon. Should this influence our annual compensation estimate? It is likely that we can eliminate a large segment of those likely to walk by, as we would expect most children to be in school on a typical fall weekday afternoon. It’s also unlikely that those who are employed and not on vacation will walk by on a fall weekday afternoon. Therefore, factoring in that it is a weekday should limit the size of the population which in turn may reduce the range of annual compensation values for our population of passerbys.</p>
<p>Let’s now consider that the park is open only to invited guests for a symposium on people analytics. Though it may be difficult to believe, a relatively small subset of the population is likely interested in attending such a symposium, so this information will likely be quite helpful in reducing the size of the population who could walk by. This should further reduce the range of annual compensation since we probably have a good idea of the profile of those most likely to attend. This probably also lessens (or altogether eliminates) the importance of the weekday factor in explaining why people vary in the amount of compensation they earn each year. That an important variable may become unimportant in the presence of another variable is a key feature of regression.</p>
<p>Furthermore, let’s consider that only those who reside in NYC and Boise were invited, and that the next person to walk by resides in Boise. Most companies apply a significant cost of living multiplier to the compensation for those in an expensive region such as NYC, resulting in a significant difference in compensation relative to those residing in a much less expensive city like Boise – all else being equal. Therefore, if we can partition attendees into two groups based on their geography, this should limit the range of annual compensation significantly <em>within each</em> – likely making the average compensation in each group a more nuanced and reasonable estimate.</p>
<p>What if we also learn the specific zip code in which the next passerby from Boise resides? The important information is likely captured at the larger city level (NYC vs. Boise), as the compensation for the specific zip codes within each city are unlikely to vary to a significant degree. Assuming this is true, it probably would not make sense to consider both the city name and zip code since they are effectively redundant pieces of information with regard to explaining differences in annual compensation.</p>
<p>What if we learn that the next person to walk by will be wearing a blue shirt? Does this influence your estimate? Unless there is research to suggest shirt color and earnings are related, this information will likely not contribute any significant information to our understanding of why people vary in the amount of compensation they earn and should, therefore, not be considered.</p>
<p>You can probably think of many relevant variables that would help further narrow the range of annual compensation. These may include job, level, years of experience, education, among other factors. The main thing to understand is that for each group of observations with the same characteristics – such as senior analysts with a graduate degree who reside in NYC – there is a distribution of annual compensation. This distribution reflects unexplained variance. That is, we do not have information to explain why the compensation for each and every person is not the same and in social science contexts, it simply is not practical to explain 100 percent of the variance in outcomes. For example, two people may be similar on dozens of factors (experience, education, skills) but one was simply a more effective negotiator when offered the same role and commanded a higher salary. It’s likely we do not have data on salary negotiation ability so this information would leave us with unexplained variance in compensation. The goal is simply to identify the variables that provide the most information in helping us tighten the distribution so that estimating the average value will generally be an accurate estimate for those in the larger population with the same characteristics.</p>
<p>While we can generally improve our estimates with more relevant information (not shirt color or residential zip code in this case), it is important to understand that samples which are too small (<span class="math inline">\(n\)</span> &lt; 30) lend to anomalies; modeling noise in sparse data can result in models that are unlikely to generalize beyond the sample data. For example, if the only people from Boise to attend the people analytics symposium happen to be two ultra wealthy tech entrepreneurs who earn millions each year, it would not be appropriate to use this as the basis for our estimates of all future attendees from Boise. This is a phenomenon known as overfitting that will be covered in Chapter <a href="pred-mod.html#pred-mod">13</a>.</p>
<p>This is the essence of linear regression modeling: find a limited number of variables which independently and/or jointly provide significant information that helps explain (by reducing) variance around the average value. As illustrated in this example, adding additional variables (information) can impact the importance of other variables or may offer no incremental information at all. In this chapter, we will cover how to identify which variables are important and how to quantify the effect they have on an outcome.</p>
<p><strong>Assumptions &amp; Diagnostics</strong></p>
<p>First, the <strong>sample size</strong> needs to be large enough to model and detect significant associations of one or more predictors with the response variable. In practice, people analytics practitioners are often constrained by the data at hand, which is to say that one generally has little control over the amount of data that can be collected. For example, despite the most earnest participation campaigns, only a subset of invited employees are likely to complete a survey, so collecting additional data to achieve a larger sample is likely not a viable option. It is important to establish a minimum – and realistic – <span class="math inline">\(n\)</span>-count threshold during the planning stage of a project based on the research objectives and variables that need to be factored into the analysis.</p>
<p>Consistent with the assumptions of parametric tests covered in Chapter <a href="aod.html#aod">9</a>, there are several assumptions that need to be validated to determine if a linear model is appropriate for understanding relationships in the data. These assumptions largely relate to the residuals (<span class="math inline">\(\hat{y}\)</span> - <span class="math inline">\(y\)</span>):</p>
<ol style="list-style-type: decimal">
<li><strong>Independence</strong>: Residuals are independent of each other; consecutive residuals in time series data are unrelated</li>
<li><strong>Homoscedasticity</strong>: Variance of residuals is constant across values of <span class="math inline">\(X\)</span></li>
<li><strong>Normality</strong>: Residuals must be normally distributed (with mean of 0) across values of <span class="math inline">\(X\)</span></li>
<li><strong>Linearity</strong>: Relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is linear</li>
</ol>
<p>Beyond these core assumptions for linear models, additional diagnostics are important to incorporate into the early data screening stage:</p>
<ol style="list-style-type: decimal">
<li><strong>High-Leverage Observations</strong>: Influential data that significantly changes the model fit</li>
<li><strong>Collinearity</strong>: Independent variables that are highly correlated (these should be <em>independent</em>)</li>
</ol>
<div id="sample-size" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Sample Size</h2>
<p>While a general rule-of-thumb for regression analysis is a minimum of a 20:1 ratio of observations to IVs, the power analysis section in Chapter <a href="inf-stats.html#inf-stats">8</a> covered a more rigorous approach to calculating the sample size needed to observe significant effects.</p>
<p>For linear regression, power analysis involves a comparison of model fit between a model with a full set of predictors relative to one with only a subset of the full model’s predictors. The function from the <code>pwr</code> library to call is <code>pwr.f2.test(u = , v = , f2 = , sig.level = , power = )</code>, where <code>u</code> and <code>v</code> are the numerator and denominator degrees of freedom, respectively, and <code>f2</code> is defined as:</p>
<p><span class="math display">\[ f^2 = \frac {R^2_{AB} - R^2_A}{1 - R^2_{AB}}, \]</span></p>
<p>where <span class="math inline">\(R^2_{AB}\)</span> represents the variance accounted for by a full model with all predictors, and <span class="math inline">\(R^2_A\)</span> represents the variance accounted for by a model containing only a subset of the full model’s predictors. For example, power analysis can be leveraged in determining the sample size needed for detecting the incremental main effects for a set of predictors beyond the variance accounted for by a set of controls.</p>
</div>
<div id="simple-linear-regression" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Simple Linear Regression</h2>
<p><strong>Simple linear regression</strong> is a simple technique for estimating the value of a quantitative DV, denoted as <span class="math inline">\(Y\)</span>, on the basis of a single IV, denoted as <span class="math inline">\(X\)</span>. It is assumed that there is an approximately linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Often, this relationship is expressed as <em>regressing</em> <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and is defined mathematically as:</p>
<p><span class="math display">\[ Y = \beta_0 + \beta_1 X + \epsilon, \]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the expected value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X = 0\)</span> (the <em>intercept</em>), and <span class="math inline">\(\beta_1\)</span> represents the average change in <span class="math inline">\(Y\)</span> for a one-unit increase in <span class="math inline">\(X\)</span> (the <em>slope</em>). <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown <em>parameters</em> or <em>coefficients</em>. The error term, <span class="math inline">\(\epsilon\)</span>, acknowledges that there is variation in <span class="math inline">\(Y\)</span> not accounted for by this simple linear model. In other words, it is highly unlikely that there is a perfectly linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, as additional variables not included in the model are likely influencing <span class="math inline">\(Y\)</span>.</p>
<p>Once we estimate the unknown model coefficients, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, we can estimate <span class="math inline">\(Y\)</span> for a particular value of <span class="math inline">\(X\)</span> by calculating:</p>
<p><span class="math display">\[ \hat{y_i} = \hat{\beta}_0 + \hat{\beta_1}x_i, \]</span></p>
<p>where <span class="math inline">\(\hat{y}\)</span> represents an estimate of <span class="math inline">\(Y\)</span> for the <span class="math inline">\(i\)</span>-ith value of <span class="math inline">\(X\)</span> equal to <span class="math inline">\(x\)</span>. The <span class="math inline">\(\hat{}\)</span> symbol is used to denote an estimated value of an unknown coefficient, parameter, or outcome.</p>
<p>The earliest form of linear regression is the <em>least squares</em> method, which was developed at the beginning of the nineteenth century and applied to astronomy problems (James, Witten, Hastie, &amp; Tibshirani, 2013). While there are several approaches to fitting a linear regression model, <strong>ordinary least squares (OLS)</strong> is the most common. OLS selects coefficients for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> that minimize the residual sum of squares (RSS) defined by:</p>
<p><span class="math display">\[RSS = (y_1 - \hat{\beta}_0 - \hat{\beta}_1x_1)^2 + (y_2 - \hat{\beta}_0 - \hat{\beta}_1x_2)^2 + {...} + (y_n - \hat{\beta}_0 - \hat{\beta}_1x_n)^2\]</span></p>
<p>For each value of <span class="math inline">\(X\)</span>, OLS fits a model for which the squared difference between the predicted (<span class="math inline">\(\hat{\beta}_0 + \hat{\beta}_1x_i\)</span>) and actual (<span class="math inline">\(y_i\)</span>) values are as small as possible. Figure <a href="lm.html#fig:lm-residuals">10.1</a> illustrates the result of minimizing <span class="math inline">\(RSS\)</span> using OLS:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lm-residuals"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/lm-residuals-1.png" alt="Minimizing RSS with Ordinary Least Squares (OLS) Fit" width="100%" />
<p class="caption">
Figure 10.1: Minimizing RSS with Ordinary Least Squares (OLS) Fit
</p>
</div>
<p>The minimizers for the least squares coefficient estimates are defined by:</p>
<p><span class="math display">\[\hat\beta_1 = \frac{\displaystyle\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})} {\displaystyle\sum_{i=1}^{n}(x_i - \bar{x})^2}\]</span></p>
<p><span class="math display">\[\hat\beta_0 = \bar{y} - \hat{\beta}_1 \bar{x},\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> are sample means.</p>
<p>It is important to understand the role sample size plays in achieving accurate estimates of <span class="math inline">\(Y\)</span>. Figure <a href="lm.html#fig:lm-fit-compare">10.2</a> illustrates the impact of fitting a model to too few observations. With <span class="math inline">\(n\)</span> = 2, it would be easy to fit a perfect model to the data; that is, one representing a line that connects the two data points. However, it is highly unlikely that these data points represent the best model for a larger sample, as there would likely be some distribution of <span class="math inline">\(Y\)</span> for each value of <span class="math inline">\(X\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lm-fit-compare"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/lm-fit-compare-1.png" alt="Left: Least squares regression model fit to n = 2 observations. Right: Least squares regression model fit to n = 20 observations." width="100%" />
<p class="caption">
Figure 10.2: Left: Least squares regression model fit to n = 2 observations. Right: Least squares regression model fit to n = 20 observations.
</p>
</div>
<p>In R, we can build (or <em>fit</em>) a simple linear regression model using the <code>lm()</code> function. The syntax is <code>lm(Y ~ X, dataset)</code>:</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="lm.html#cb336-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb336-2"><a href="lm.html#cb336-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb336-3"><a href="lm.html#cb336-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb336-4"><a href="lm.html#cb336-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load employee data</span></span>
<span id="cb336-5"><a href="lm.html#cb336-5" aria-hidden="true" tabindex="-1"></a>employees <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/crstarbuck/peopleanalytics_book/master/data/employees.csv&quot;</span>)</span>
<span id="cb336-6"><a href="lm.html#cb336-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb336-7"><a href="lm.html#cb336-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Subset employees data frame; leads are only applicable for those in sales positions</span></span>
<span id="cb336-8"><a href="lm.html#cb336-8" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">subset</span>(employees, job_title <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&#39;Sales Executive&#39;</span>, <span class="st">&#39;Sales Representative&#39;</span>))</span>
<span id="cb336-9"><a href="lm.html#cb336-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb336-10"><a href="lm.html#cb336-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress YTD leads on engagement</span></span>
<span id="cb336-11"><a href="lm.html#cb336-11" aria-hidden="true" tabindex="-1"></a>slm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(ytd_leads <span class="sc">~</span> engagement, data)</span></code></pre></div>
<p>In practice, linear assumptions are rarely – if ever – perfectly met, but there must be evidence that the assumptions are <em>generally</em> satisfied.</p>
<p>Before performing model diagnostics, it is important to note the following:</p>
<ol style="list-style-type: decimal">
<li>Collinearity diagnostics are only applicable in the context of multiple regression, as simple linear models have only one IV (this will be covered later in the chapter).</li>
<li>Outliers are not always an issue, as we discussed in Chapter <a href="#data-arch-prep"><strong>??</strong></a>. Figure <a href="lm.html#fig:high-leverage">10.3</a> illustrates differences between an outlier that does not influence the model fit (left) relative to one which has significant leverage on the model fit (right).</li>
</ol>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.4332 -0.5482 -0.0275  0.2867  2.6413 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.4395     0.4084   1.076    0.296    
## X             3.1303     0.1515  20.659  5.5e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.8829 on 18 degrees of freedom
## Multiple R-squared:  0.9595, Adjusted R-squared:  0.9573 
## F-statistic: 426.8 on 1 and 18 DF,  p-value: 5.498e-14</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y.out ~ X.out)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.34746 -0.56288 -0.09021  0.29951  2.66197 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   0.5224     0.3399   1.537    0.141    
## X.out         3.0900     0.1077  28.703   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.863 on 19 degrees of freedom
## Multiple R-squared:  0.9775, Adjusted R-squared:  0.9763 
## F-statistic: 823.9 on 1 and 19 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y.lev ~ X.lev)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -10.3085  -1.8776   0.8556   2.0309   4.8048 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)   3.9645     1.3559   2.924  0.00871 **
## X.lev         1.4180     0.4295   3.302  0.00375 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.443 on 19 degrees of freedom
## Multiple R-squared:  0.3646, Adjusted R-squared:  0.3311 
## F-statistic:  10.9 on 1 and 19 DF,  p-value: 0.00375</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:high-leverage"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/high-leverage-1.png" alt="Left: Model fit with non-influential outlier. Right: Model fit with high leverage outlier. Outlier shown in red. Black solid line represents model fit without outliers. Red dashed line represents model fit with outliers." width="100%" />
<p class="caption">
Figure 10.3: Left: Model fit with non-influential outlier. Right: Model fit with high leverage outlier. Outlier shown in red. Black solid line represents model fit without outliers. Red dashed line represents model fit with outliers.
</p>
</div>
<p>We can conveniently perform linear model diagnostics using the <code>plot()</code> function in conjunction with the object holding linear model results (<code>slm.fit</code>). This produces the following standard plots shown in Figure <a href="lm.html#fig:slm-diagnostics">10.4</a>:</p>
<ul>
<li><strong>Residuals vs Fitted</strong>: Shows how residuals (<span class="math inline">\(y\)</span>-axis) change across the range of fitted values (<span class="math inline">\(x\)</span>-axis)</li>
<li><strong>Normal Q-Q</strong>: Compares two probability distributions by plotting their quantiles (data partitioned into equal-sized groups) against each other</li>
<li><strong>Scale-Location</strong>: Shows how <em>standardized</em> residuals (<span class="math inline">\(y\)</span>-axis) change across the range of fitted values (<span class="math inline">\(x\)</span>-axis)</li>
<li><strong>Residuals vs Leverage</strong>: Shows the leverage of each data point (<span class="math inline">\(x\)</span>-axis) against their standardized residuals (<span class="math inline">\(y\)</span>-axis)</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:slm-diagnostics"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/slm-diagnostics-1.png" alt="Simple linear regression model diagnostics." width="100%" />
<p class="caption">
Figure 10.4: Simple linear regression model diagnostics.
</p>
</div>
<p>The <em>Residuals vs Fitted</em> and <em>Scale-Location</em> plots help evaluate assumptions of homoscedasticity, linearity, and normality – which are intricately linked. Data are heteroscedastic if there is a flarring or funnel patterning about the residuals across the range of fitted values. That is, there must be a constant variance about the residual errors in order for the assumption of homoscedasticity to be met. This occurs when there is a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, in which case residuals will be normally distributed around a mean of 0. While the spread of residuals is greater for larger fitted values in this model, resulting in the lower standardized residual error for smaller fitted values indicated in the Scale-Location plot, the slope of the line in the Residuals vs Fitted plot is effectively flat which indicates that the model does not perform significantly better for certain fitted values relative to others.</p>
<p>Cook’s distance in the <em>Residuals vs Leverage</em> plot provides a measure of how much our model estimates for all observations change if high leverage observations are removed from the data. Higher numbers indicate stronger influence. R conveniently labels the three observations with the highest leverage, though the degree of leverage is only problematic for observations beyond the dashed Cook’s distance line. In this case, there are no observations with enough leverage for the dashed Cook’s distance line to show on the plot, so no action is warranted.</p>
<p>In addition to the visual inspection, we can perform the <strong>Breusch-Pagan test</strong> using the <code>bptest()</code> function from the <code>lmtest</code> library to test the null hypothesis that the data are homoscedastic. If <span class="math inline">\(p\)</span> &lt; .05 for the test statistic, we reject the null hypothesis and conclude that there is evidence of heteroscedasticity in the data.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="lm.html#cb340-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the Breusch-Pagan test for evaluate homoscedasticity</span></span>
<span id="cb340-2"><a href="lm.html#cb340-2" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">bptest</span>(slm.fit)</span></code></pre></div>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  slm.fit
## BP = 0.07603, df = 1, p-value = 0.7828</code></pre>
<p>Since <span class="math inline">\(p\)</span> &gt; .05, we fail to reject the null hypothesis of homoscedasticity; therefore, this assumption is satisified. If this was not the case, a common approach to addressing heteroscedasticity is transforming the response variable by taking the natural logarithm (<code>log()</code>) or square root (<code>sqrt()</code>) of the raw data. While transformations may correct for violations of linear model assumptions, they also result in a less intuitive interpretation of model output relative to the raw untransformed data.</p>
<p>Let’s illustrate how to transform the response variable:</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="lm.html#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Square root transformation of YTD leads</span></span>
<span id="cb342-2"><a href="lm.html#cb342-2" aria-hidden="true" tabindex="-1"></a>slm.fit.trans <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(ytd_leads) <span class="sc">~</span> engagement, data)</span>
<span id="cb342-3"><a href="lm.html#cb342-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb342-4"><a href="lm.html#cb342-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Natural logarithmic transformation of YTD leads</span></span>
<span id="cb342-5"><a href="lm.html#cb342-5" aria-hidden="true" tabindex="-1"></a>slm.fit.trans <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(ytd_leads) <span class="sc">~</span> engagement, data)</span></code></pre></div>
<p>The <em>Normal Q-Q Plot</em> in Figure <a href="lm.html#fig:slm-diagnostics">10.4</a> is used to test the assumption of normally distributed model residuals. A perfectly normal distribution of residuals will result in data lying along the line situated at 45-degrees from the <span class="math inline">\(x\)</span>-axis. Based on a visual inspection, our residuals appear to be normally distributed, as there are only a small number of minor departures in the upper and lower ends of the quantile range.</p>
<p>We can also visualize the distribution of model residuals using a histogram. In the majority of cases, the residual should be 0 – that is, in most cases the model correctly estimates YTD leads resulting in no difference between estimated and observed values (<span class="math inline">\(\hat{y}\)</span> - <span class="math inline">\(y\)</span> = 0).</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="lm.html#cb343-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce histogram to visualize distribution of model residuals</span></span>
<span id="cb343-2"><a href="lm.html#cb343-2" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb343-3"><a href="lm.html#cb343-3" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">aes</span>(slm.fit<span class="sc">$</span>residuals) <span class="sc">+</span> </span>
<span id="cb343-4"><a href="lm.html#cb343-4" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;YTD Leads Residuals&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> </span>
<span id="cb343-5"><a href="lm.html#cb343-5" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="sc">+</span></span>
<span id="cb343-6"><a href="lm.html#cb343-6" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb343-7"><a href="lm.html#cb343-7" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:slm-residuals"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/slm-residuals-1.png" alt="Distribution of model residuals." width="672" />
<p class="caption">
Figure 10.5: Distribution of model residuals.
</p>
</div>
<p>Based on both the Normal Q-Q Plot and histogram, the residuals conform to the assumption of normality. We can confirm using the Shapiro-Wilk test, in which a non-significant test statistic is sufficient evidence of normality:</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="lm.html#cb344-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Shapiro-Wilk test of normality</span></span>
<span id="cb344-2"><a href="lm.html#cb344-2" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(slm.fit<span class="sc">$</span>residuals)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  slm.fit$residuals
## W = 0.99339, p-value = 0.07029</code></pre>
<p>Next, let’s display our simple linear model results:</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="lm.html#cb346-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce model summary</span></span>
<span id="cb346-2"><a href="lm.html#cb346-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slm.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ytd_leads ~ engagement, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -14.8236  -3.7591   0.1118   3.1764  13.1764 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.6301     0.9970   1.635    0.103    
## engagement   20.0645     0.3571  56.193   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.095 on 407 degrees of freedom
## Multiple R-squared:  0.8858, Adjusted R-squared:  0.8855 
## F-statistic:  3158 on 1 and 407 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>There are several important pieces of information in this output:</p>
<ul>
<li><code>Estimate</code>: <em>Unstandardized</em> Beta coefficient associated with the predictor</li>
<li><code>Std. Error</code>: Average distance between the observed and estimated values per the fitted regression line</li>
<li><code>t value</code>: <code>Estimate</code> / <code>Standard Error</code>. Larger values provide more evidence for a non-zero coefficient (relationship) in the population.</li>
<li><code>Pr(&gt;|t|)</code>: <span class="math inline">\(p\)</span>-value for evaluating whether there is sufficient evidence in the sample that the coefficient (relationship) between the respective predictor and response variable is not 0 in the population (i.e., <span class="math inline">\(x\)</span> has a relationship with <span class="math inline">\(y\)</span>)</li>
<li><code>Intercept</code>: Mean value of the response variable when all predictors are equal to 0. Note that the interpretation of the intercept is often nonsensical since many predictors cannot have 0 values (e.g., age, height, weight, IQ).</li>
<li><code>Signif. codes</code>: Symbols to quickly ascertain whether predictors are significant at key levels, such as <span class="math inline">\(p\)</span> &lt; .001, <span class="math inline">\(p\)</span> &lt; .01, or <span class="math inline">\(p\)</span> &lt; .05.</li>
<li><code>Residual standard error</code>: Measure of model fit which reflects the standard deviation of the residuals (<span class="math inline">\(\sqrt {\sum(y-\hat{y})^2 / df}\)</span>)</li>
<li><code>Degrees of freedom</code>: <span class="math inline">\(n\)</span> - <span class="math inline">\(p\)</span>, where <span class="math inline">\(n\)</span> is the number of observations and <span class="math inline">\(p\)</span> is the number of predictors</li>
<li><code>Multiple R-squared</code>: Percent of variance in <span class="math inline">\(y\)</span> (when multiplied by 100) explained by the predictors in the model. This is also known as the <strong>Coefficient of Determination</strong>. For simple linear regression, this is simply the squared value of Pearson’s <span class="math inline">\(r\)</span> for the bivariate relationship between the predictor and response (execute <code>cor(data$engagement, data$ytd_leads)^2</code> to validate).</li>
<li><code>Adjusted R-squared</code>: Modified version of <span class="math inline">\(R^2\)</span> that adjusts the estimate for non-significant predictors. A large delta between <span class="math inline">\(R^2\)</span> and Adjusted <span class="math inline">\(R^2\)</span> coefficients generally indicates a model containing a larger number of non-significant predictors relative to when <span class="math inline">\(R^2\)</span> and Adjusted <span class="math inline">\(R^2\)</span> values are similar.</li>
<li><code>F-statistic</code>: Statistic used in conjunction with the <code>p-value</code> for testing differences between the specified model and an intercept-only model (a model with no predictors). This test helps us evaluate whether our predictors are helpful in explaining variance in <span class="math inline">\(y\)</span>.</li>
</ul>
<p>The output of this simple linear regression model indicates that for each level increase in engagement, the average increase in YTD leads is about 20 (<span class="math inline">\(\beta\)</span> = 20.1, <span class="math inline">\(t\)</span>(407) = 56.2, <span class="math inline">\(p\)</span> &lt; .001). Had we transformed the response variable from its original unit of measurement, the interpretation would be expressed in the transformed units (e.g., <span class="math inline">\(\beta\)</span> is the square root or natural log of the average change in leads for a one-level increase in engagement).</p>
<p>Thanks to the properties of the CLT, which requires normally distributed <em>residuals</em>, we can be confident that the true relationship between engagement and YTD leads in the population is not 0 since the 95% CI (<span class="math inline">\(\beta\)</span> +/- 2<span class="math inline">\(SE\)</span>) does not include 0.</p>
<p>While it may be tempting to conclude that employee engagement has a significant influence on leads based on the model output, we know that bivariate relationships may be spurious; that is, engagement may be correlated with another variable that is actually influencing leads. In practice, a simple linear model is rarely sufficient for explaining a meaningful percent of variance in a response variable, and additional predictors are usually needed to capture the complex and nuanced relationships characteristic of people analytics problems.</p>
<p>The <span class="math inline">\(R^2\)</span> value indicates that 8.9% of the variance in leads can be explained by the variation in engagement levels. Put differently, this simple model does not account for 91.1% of variation in leads. Since a large portion of the variance in leads is unexplained, we need signal from additional predictors to understand the other dimensions along which leads vary.</p>
<p>Figure <a href="lm.html#fig:slm-fit">10.6</a> illustrates how the regression equation for this simple linear model (<span class="math inline">\(y\)</span> = 20.1<span class="math inline">\(x\)</span> + 1.6) fits the data points for sales employees. The spread of leads at each engagement level indicates that there are other factors that explain variance in leads that need to be accounted for in the regression equation to achieve more accurate estimates. The reduction in the spread of leads for a combination of significant predictor values increases <span class="math inline">\(R^2\)</span> (explained variance in YTD leads).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:slm-fit"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/slm-fit-1.png" alt="Simple Linear Model Fit Line for y = 20.1x + 1.6" width="100%" />
<p class="caption">
Figure 10.6: Simple Linear Model Fit Line for y = 20.1x + 1.6
</p>
</div>
<p>We can use the <code>extract_eq()</code> function from the <code>equatiomatic</code> library to easily build a properly formatted LaTex regression equation from the model object:</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="lm.html#cb348-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb348-2"><a href="lm.html#cb348-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(equatiomatic)</span>
<span id="cb348-3"><a href="lm.html#cb348-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb348-4"><a href="lm.html#cb348-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert model to LaTex regression equation</span></span>
<span id="cb348-5"><a href="lm.html#cb348-5" aria-hidden="true" tabindex="-1"></a>equatiomatic<span class="sc">::</span><span class="fu">extract_eq</span>(slm.fit)</span></code></pre></div>
<p><span class="math display">\[
\operatorname{ytd\_leads} = \alpha + \beta_{1}(\operatorname{engagement}) + \epsilon
\]</span></p>
</div>
<div id="multiple-linear-regression" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> Multiple Linear Regression</h2>
<p><strong>Multiple linear regression</strong> extends the simple linear model to one with two or more predictor variables. A multiple regression model generally explains more variance in the response relative to the simple linear model, and is defined by:</p>
<p><span class="math display">\[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + {...} + \beta_p X_p + \epsilon \]</span></p>
<p>Once we estimate the unknown model coefficients, <span class="math inline">\(\beta_0\)</span> through <span class="math inline">\(\beta_p\)</span>, we can estimate <span class="math inline">\(Y\)</span> for a particular combination of values for <span class="math inline">\(X_1\)</span> through <span class="math inline">\(X_p\)</span> by calculating:</p>
<p><span class="math display">\[ \hat{y_i} = \hat{\beta}_0 + \hat{\beta_1}x_{i1} + \hat{\beta_2}x_{i2} + {...} + \hat{\beta_p} x_{ip} \]</span></p>
<p><strong>Collinearity Diagnostics</strong></p>
<p>In addition to the assumptions we tested in the context of the simple linear model, multiple linear regression warrants collinearity diagnostics. <strong>Collinearity</strong> refers to situations in which predictors that are related to the response variable also have strong associations with one another. In practice, there is usually some level of collinearity between variables, so the goal of collinearity diagnostics is to identify and address <em>problematic</em> levels of collinearity.</p>
<p>Models should be built with predictors that have a strong association with the outcome but not with one another. If predictors are highly correlated with each other, it indicates that they are redundant and do not provide unique information. A large amount of collinearity can cause serious issues with the underlying calculus of a regression model, which can manifest in the form of effects of significant predictors being masked or suppressed or a negative sign/effect showing in the output when a positive association between the predictor and the response actually exists (or vice versa). As a result, it would be premature to fit a linear model before running collinearity diagnostics, as there may be false negatives – predictors that appear unimportant but are actually statistical drivers of the response. If problematic collinearity is not addressed, false conclusions may be drawn from the model output which may lead to bad business decisions.</p>
<p>Kuhn and Johnson (2013) recommend the simple procedure outlined below to identify and address problematic collinearity:</p>
<ol style="list-style-type: decimal">
<li>Determine the two predictors associated with the largest absolute pairwise correlation (whether they are positively or negatively related does not matter) – call them predictors A and B.</li>
<li>Determine the average absolute correlation between predictor A and the other variables. Do the same for predictor B.</li>
<li>If predictor A has a larger average absolute correlation, remove it; otherwise, remove predictor B. The exception to this rule is when predictors A and B have similar average absolute correlations with all other predictors but the predictor with the slightly higher correlation is a key variable that, if dropped, will prevent you from addressing one or more stated objectives or hypotheses.</li>
<li>Repeat steps 1-3 until <span class="math inline">\(|r|\)</span> &lt; .7.</li>
</ol>
<p>Let’s demonstrate the procedures and mechanics for multiple linear regression by estimating YTD sales using multiple predictor variables. While not appropriate in practice, we will select a subset of the available predictors from <code>data</code> to simplify the model and explanation. In Chapter <a href="pred-mod.html#pred-mod">13</a>, we will discuss the use of machine learning (ML) models for more efficient and comprehensive variable selection.</p>
<p>We can leverage the <code>ggpairs()</code> function introduced in Chapter <a href="desc-stats.html#desc-stats">7</a> to efficiently compute bivariate correlations and visualize data distributions:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ggpairs-ytd-sales"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/ggpairs-ytd-sales-1.png" alt="GGpairs bivariate correlations and data distributions." width="100%" />
<p class="caption">
Figure 10.7: GGpairs bivariate correlations and data distributions.
</p>
</div>
<p>Based on the correlations, <code>org_tenure</code> is highly correlated with both <code>job_tenure</code>, <code>mgr_tenure</code> and <code>work_exp</code>. These relationships indicate that job and manager changes for those in sales roles have been infrequent since joining the organization, and that a large portion of their work experience has been with this organization. Since <code>org_tenure</code> has the strongest relationship with our response, <code>ytd_sales</code>, we will drop <code>job_tenure</code>, <code>mgr_tenure</code>, and <code>work_exp</code>.</p>
<p>Since <span class="math inline">\(|r|\)</span> &lt; .7 for all pairwise relationships, let’s fit the more parsimonious multiple regression model using the resulting subset of predictors. We can include multiple predictors in the model using the <code>+</code> symbol in the <code>lm()</code> function:</p>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="lm.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress YTD sales on a combination of predictors</span></span>
<span id="cb349-2"><a href="lm.html#cb349-2" aria-hidden="true" tabindex="-1"></a>mlm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(ytd_sales <span class="sc">~</span> engagement <span class="sc">+</span> job_lvl <span class="sc">+</span> stock_opt_lvl <span class="sc">+</span> org_tenure, data)</span></code></pre></div>
<p>While Kuhn and Johnson’s procedure is a good first step, this may not eliminate what is known as <strong>multicollinearity</strong>, which is collinearity among three or more predictors. It is possible for collinearity to exist between three or more variables, even in the absence of a strong correlation for a pair of variables. We can evaluate the <strong>Variance Inflation Factor (VIF)</strong> for the predictors that remain following the bivariate correlation review to ensure multicollinearity is not present. <span class="math inline">\(VIF\)</span> is defined by:</p>
<p><span class="math display">\[ VIF(\hat{\beta_j}) = \frac{1}{1 - R^2_{X_j|X_{-j}}}, \]</span></p>
<p>where the denominator, <span class="math inline">\(R^2_{X_j|X_{-j}}\)</span>, is the <span class="math inline">\(R^2\)</span> from regressing <span class="math inline">\(X_j\)</span> onto all other predictors. The smallest value of <span class="math inline">\(VIF\)</span> is 1, which indicates a complete absence of collinearity. Problematic collinearity exists if <span class="math inline">\(VIF\)</span> for any variable exceeds 5.</p>
<p>We can produce <span class="math inline">\(VIF\)</span> for each variable using the <code>vif()</code> function from the <code>car</code> library:</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="lm.html#cb350-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load library</span></span>
<span id="cb350-2"><a href="lm.html#cb350-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb350-3"><a href="lm.html#cb350-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb350-4"><a href="lm.html#cb350-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce VIF for each predictor</span></span>
<span id="cb350-5"><a href="lm.html#cb350-5" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(mlm.fit)</span></code></pre></div>
<pre><code>##    engagement       job_lvl stock_opt_lvl    org_tenure 
##      1.001459      1.407192      1.009435      1.395797</code></pre>
<p>Based on the output, <span class="math inline">\(VIF\)</span> &lt; 5 for each predictor, which indicates that multicollinearity is not an issue.</p>
<p>Next, let’s evaluate the linear model assumptions to validate that fitting a linear model to these data is appropriate:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pre-mlm-diagnostics"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/pre-mlm-diagnostics-1.png" alt="Multiple linear regression model diagnostics." width="100%" />
<p class="caption">
Figure 10.8: Multiple linear regression model diagnostics.
</p>
</div>
<p>Based on these visuals, there are obvious violations of linear model assumptions that need to first be addressed.</p>
<p>First, given the long right tail for the <code>ytd_sales</code> distribution shown in Figure <a href="lm.html#fig:pre-mlm-diagnostics">10.8</a>, let’s apply a square root transformation to the response variable:</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="lm.html#cb352-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Regress YTD sales on a combination of predictors</span></span>
<span id="cb352-2"><a href="lm.html#cb352-2" aria-hidden="true" tabindex="-1"></a>mlm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">sqrt</span>(ytd_sales) <span class="sc">~</span> engagement <span class="sc">+</span> job_lvl <span class="sc">+</span> stock_opt_lvl <span class="sc">+</span> org_tenure, data)</span></code></pre></div>
<p>Additionally, the diagnostic plots indicate that there are data points with high leverage on the fit. Let’s address using Cook’s distance as the criterion:</p>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb353-1"><a href="lm.html#cb353-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Remove high leverage observations per Cook&#39;s distance</span></span>
<span id="cb353-2"><a href="lm.html#cb353-2" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">rstudent</span>(mlm.fit)) <span class="sc">&lt;</span> <span class="dv">3</span> <span class="sc">&amp;</span> <span class="fu">abs</span>(<span class="fu">cooks.distance</span>(mlm.fit)) <span class="sc">&lt;</span> <span class="dv">4</span><span class="sc">/</span><span class="fu">nrow</span>(mlm.fit<span class="sc">$</span>model)</span>
<span id="cb353-3"><a href="lm.html#cb353-3" aria-hidden="true" tabindex="-1"></a>mlm.fit <span class="ot">&lt;-</span> <span class="fu">update</span>(mlm.fit, <span class="at">weights =</span> <span class="fu">as.numeric</span>(w))</span></code></pre></div>
<p>Now we can produce a refreshed set of diagnostic plots to evaluate the impact of transforming the response variable and removing high leverage observations:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:post-mlm-diagnostics"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/post-mlm-diagnostics-1.png" alt="Multiple linear regression model diagnostics (post-transformation)." width="100%" />
<p class="caption">
Figure 10.9: Multiple linear regression model diagnostics (post-transformation).
</p>
</div>
<p>There is a clear improvement towards satisifying linear model assumptions.</p>
<p>Let’s perform the Breusch-Pagan test to validate that the assumption of homoscedasticity is met:</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="lm.html#cb354-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the Breusch-Pagan test for evaluate homoscedasticity</span></span>
<span id="cb354-2"><a href="lm.html#cb354-2" aria-hidden="true" tabindex="-1"></a>lmtest<span class="sc">::</span><span class="fu">bptest</span>(mlm.fit)</span></code></pre></div>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  mlm.fit
## BP = 2.2111, df = 4, p-value = 0.697</code></pre>
<p>Next, let’s ensure residuals are normally distributed around 0:</p>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="lm.html#cb356-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce histogram to visualize distribution of model residuals</span></span>
<span id="cb356-2"><a href="lm.html#cb356-2" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb356-3"><a href="lm.html#cb356-3" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">aes</span>(mlm.fit<span class="sc">$</span>residuals) <span class="sc">+</span> </span>
<span id="cb356-4"><a href="lm.html#cb356-4" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;YTD Leads Residuals&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span> </span>
<span id="cb356-5"><a href="lm.html#cb356-5" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">fill =</span> <span class="st">&quot;#414141&quot;</span>) <span class="sc">+</span></span>
<span id="cb356-6"><a href="lm.html#cb356-6" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">geom_density</span>(<span class="at">fill =</span> <span class="st">&quot;#ADD8E6&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb356-7"><a href="lm.html#cb356-7" aria-hidden="true" tabindex="-1"></a>ggplot2<span class="sc">::</span><span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlm-residuals"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/mlm-residuals-1.png" alt="Distribution of model residuals" width="672" />
<p class="caption">
Figure 10.10: Distribution of model residuals
</p>
</div>
<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb357-1"><a href="lm.html#cb357-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute Shapiro-Wilk test of normality</span></span>
<span id="cb357-2"><a href="lm.html#cb357-2" aria-hidden="true" tabindex="-1"></a><span class="fu">shapiro.test</span>(mlm.fit<span class="sc">$</span>residuals)</span></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  mlm.fit$residuals
## W = 0.99342, p-value = 0.07202</code></pre>
<p>Based on the diagnostic plots and statistical tests, our data satisify the requirements for building a multiple linear regression model.</p>
<p><strong>Variable Selection</strong></p>
<p>Next, we need to reduce our model to the subset of predictors with statistically significant relationships with the response variable. <strong>Backward Stepwise Selection</strong> is a common and simple variable selection procedure, and the steps are outlined below:</p>
<ol style="list-style-type: decimal">
<li>Remove the predictor with the highest <span class="math inline">\(p\)</span>-value greater than the critical value (<span class="math inline">\(\alpha\)</span> = .05).</li>
<li>Refit the model, and repeat step 1.</li>
<li>Stop when all <span class="math inline">\(p\)</span>-values are less than the critical value.</li>
</ol>
<p>Each predictor in our model has a statistically significant relationship with <code>ytd_sales</code> – indicating that the slope of the relationships with the response is unlikely 0 in the population – so further variable reduction is not required.</p>
<pre><code>## 
## Call:
## lm(formula = sqrt(ytd_sales) ~ engagement + job_lvl + stock_opt_lvl + 
##     org_tenure, data = data, weights = as.numeric(w))
## 
## Weighted Residuals:
##    Min     1Q Median     3Q    Max 
## -66.31 -16.01   0.00  16.72  78.50 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   125.9807     6.9032  18.250  &lt; 2e-16 ***
## engagement     10.1046     1.9742   5.118 4.93e-07 ***
## job_lvl        33.7679     2.4254  13.922  &lt; 2e-16 ***
## stock_opt_lvl   5.1662     1.6211   3.187  0.00156 ** 
## org_tenure      6.8118     0.3414  19.952  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 26.15 on 375 degrees of freedom
## Multiple R-squared:  0.7783, Adjusted R-squared:  0.7759 
## F-statistic: 329.1 on 4 and 375 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can view the multiple regression equation for estimating <code>sqrt(ytd_sales)</code> with these four predictors using the <code>extract_eq</code> function:</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="lm.html#cb360-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert model to LaTex regression equation</span></span>
<span id="cb360-2"><a href="lm.html#cb360-2" aria-hidden="true" tabindex="-1"></a>equatiomatic<span class="sc">::</span><span class="fu">extract_eq</span>(mlm.fit)</span></code></pre></div>
<p><span class="math display">\[
\operatorname{sqrt(ytd\_sales)} = \alpha + \beta_{1}(\operatorname{engagement}) + \beta_{2}(\operatorname{job\_lvl}) + \beta_{3}(\operatorname{stock\_opt\_lvl}) + \beta_{4}(\operatorname{org\_tenure}) + \epsilon
\]</span></p>
<p>Based on the model output, the combination of predictors explains about 78% of the variance in YTD sales (<span class="math inline">\(R^2\)</span> = .778). In people analytics settings, it is rare to explain three-quarters of the variance for an outcome given people data are especially noisy.</p>
<p>By default, the coefficients on the predictors are unstandardized; that is, they represent the average change in the square root transformed response for each one-unit increase for the respective predictor. Since the predictors have different units of measurement, such as <code>stock_opt_lvl</code> ranging from 0 to 3 and <code>org_tenure</code> ranging from 0 to 40, the unstandardized coefficients cannot be compared to determine which variable has the largest effect on YTD sales. We must standardize these coefficients and adjust for differences in the units of measurement for an apples-to-apples comparison.</p>
<p>We can scale variables by subtracting the variable’s mean from <span class="math inline">\(x\)</span> and dividing the difference into the variable’s standard deviation:</p>
<p><span class="math display">\[ x_{scaled} = \frac{x - \bar{x}} {s} \]</span></p>
<p>We can leverage the <code>scale()</code> function to standardize the predictors’ units of measurement and determine which has the largest effect on <code>ytd_sales</code>:</p>
<pre><code>## 
## Call:
## lm(formula = ytd_sales ~ engagement + job_lvl + stock_opt_lvl + 
##     org_tenure, data = data_std)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.47125 -0.28770 -0.04054  0.30025  2.26587 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   -3.313e-18  2.606e-02   0.000  1.00000    
## engagement     1.247e-01  2.611e-02   4.777 2.49e-06 ***
## job_lvl        3.845e-01  3.095e-02  12.423  &lt; 2e-16 ***
## stock_opt_lvl  6.788e-02  2.622e-02   2.589  0.00997 ** 
## org_tenure     5.637e-01  3.083e-02  18.285  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.527 on 404 degrees of freedom
## Multiple R-squared:  0.7249, Adjusted R-squared:  0.7222 
## F-statistic: 266.2 on 4 and 404 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Based on the standardized coefficients in Figure <a href="#fig:mlm-scaled"><strong>??</strong></a>, <code>org_tenure</code> has the largest effect (<span class="math inline">\(\beta\)</span> = .56, <span class="math inline">\(t\)</span>(404) = 18.29, <span class="math inline">\(p\)</span> &lt; .001) and <code>job_lvl</code> has the second largest effect (<span class="math inline">\(\beta\)</span> = .39, <span class="math inline">\(t\)</span>(404) = 12.42, <span class="math inline">\(p\)</span> &lt; .001).</p>
</div>
<div id="moderation" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Moderation</h2>
<p>As discussed in Chapter <a href="measure-sampl.html#measure-sampl">4</a>, a moderating variable is a third variable which amplifies (strengthens) or attenuates (weakens) the relationship between an IV and the response. Accounting for a moderating variable in an linear model requires an <strong>interaction term</strong>, which is the product of the two variables (<span class="math inline">\(X_1\)</span> * <span class="math inline">\(X_2\)</span>).</p>
<p>Let’s examine whether <code>org_tenure</code> influences the strength of the relationship between <code>job_lvl</code> and <code>sqrt(ytd_sales)</code>. We would generally expect sales to increase as the job level of salespeople increases, and longer tenure may amplify the strength of this association. Step one is testing whether the interaction term is statistically significant, and step two is determining the nature of any observed statistical interaction. Including the interaction term in the model (<code>job_lvl</code> * <code>org_tenure</code>) will add the predictors independently <em>and</em> jointly:</p>
<pre><code>## 
## Call:
## lm(formula = sqrt(ytd_sales) ~ job_lvl * org_tenure, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -89.090 -16.918  -0.589  17.490 107.461 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        123.8354     6.2497  19.815  &lt; 2e-16 ***
## job_lvl             50.5727     3.0119  16.791  &lt; 2e-16 ***
## org_tenure          12.4813     0.8653  14.425  &lt; 2e-16 ***
## job_lvl:org_tenure  -2.5142     0.2996  -8.393 8.01e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 29.4 on 405 degrees of freedom
## Multiple R-squared:  0.7412, Adjusted R-squared:  0.7393 
## F-statistic: 386.6 on 3 and 405 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The results show that both the main and interaction effects are statistically significant (<span class="math inline">\(p\)</span> &lt; .001).</p>
<p>Since interaction terms can be highly correlated with independent predictors, it is good to check for collinearity:</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="lm.html#cb363-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce VIF for each model term</span></span>
<span id="cb363-2"><a href="lm.html#cb363-2" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(mlm.fit.int)</span></code></pre></div>
<pre><code>##            job_lvl         org_tenure job_lvl:org_tenure 
##           2.189565           9.246489          12.308142</code></pre>
<p><span class="math inline">\(VIF\)</span> is greater than 5 for both <code>org_tenure</code> and the interaction term; therefore, there is a problematic level of collinearity between these variables.</p>
<p>A common method of addressing collinearity in the context of interaction testing is <strong>variable centering</strong>, in which each value of the predictor is subtracted from its mean (<span class="math inline">\(x - \bar{x}\)</span>). Unlike other transformations we have explored, centering does not impact the interpretation of model coefficients. Coefficients continue to represent the average change in the response for a one-unit change in a predictor, as the range of values for centered variables is consistent with the range for untransformed variables. However, the coefficients on centered variables may be considerably different relative to the model with untransformed variables due to the effects of high collinearity.</p>
<pre><code>## 
## Call:
## lm(formula = sqrt(ytd_sales) ~ job_lvl_cntrd * org_tenure_cntrd, 
##     data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -89.090 -16.918  -0.589  17.490 107.461 
## 
## Coefficients:
##                                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                    276.5606     1.5653 176.680  &lt; 2e-16 ***
## job_lvl_cntrd                   34.0427     2.4065  14.146  &lt; 2e-16 ***
## org_tenure_cntrd                 7.2623     0.3789  19.165  &lt; 2e-16 ***
## job_lvl_cntrd:org_tenure_cntrd  -2.5142     0.2996  -8.393 8.01e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 29.4 on 405 degrees of freedom
## Multiple R-squared:  0.7412, Adjusted R-squared:  0.7393 
## F-statistic: 386.6 on 3 and 405 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb366"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb366-1"><a href="lm.html#cb366-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Produce VIF for centered variables</span></span>
<span id="cb366-2"><a href="lm.html#cb366-2" aria-hidden="true" tabindex="-1"></a>car<span class="sc">::</span><span class="fu">vif</span>(mlm.fit.int)</span></code></pre></div>
<pre><code>##                  job_lvl_cntrd               org_tenure_cntrd 
##                       1.397803                       1.773279 
## job_lvl_cntrd:org_tenure_cntrd 
##                       1.337702</code></pre>
<p>After centering the variables, <span class="math inline">\(VIF\)</span> is well beneath the threshold of 5.</p>
<p>Comparing Figure <a href="#fig:mlm-int"><strong>??</strong></a> to Figure <a href="#fig:mlm-int-cntrd"><strong>??</strong></a>, we can observe that the main effects for <code>job_lvl</code> and <code>org_tenure</code> are inflated – and population parameter estimates less precise (larger <span class="math inline">\(SE\)</span>) – when high collinearity is present.</p>
<p>To better understand the nature of the interaction effect (<span class="math inline">\(\beta\)</span> = -2.51, <span class="math inline">\(t\)</span>(405) = -8.39, <span class="math inline">\(p\)</span> &lt; .001), two equations can be built to evaluate changes in the slope of the relationship with high (<span class="math inline">\(\bar{x}\)</span> + 1<span class="math inline">\(s\)</span>) and low (<span class="math inline">\(\bar{x}\)</span> - 1<span class="math inline">\(s\)</span>) organization tenure:</p>
<ul>
<li><em>High organization tenure</em>: <span class="math inline">\(Y = -2.51 (6.57 + 5.12)X + 276.56\)</span></li>
<li><em>Low organization tenure</em>: <span class="math inline">\(Y = -2.51 (6.57 - 5.12)X + 276.56,\)</span></li>
</ul>
<p>where 6.57 is <code>mean(data$org_tenure)</code>, 5.12 is <code>sd(data$org_tenure)</code>, and <span class="math inline">\(X\)</span> is a vector of values for <code>job_lvl</code>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:int-effects"></span>
<img src="The_Fundamentals_of_People_Analytics_files/figure-html/int-effects-1.png" alt="Regression of square root transformed YTD sales onto job level x organization tenure interaction term. High organization tenure (red line): Y = -2.51 (6.57 + 5.12)X + 276.56. Low organization tenure (blue line): Y = -2.51 (6.57 - 5.12)X + 276.56." width="100%" />
<p class="caption">
Figure 10.11: Regression of square root transformed YTD sales onto job level x organization tenure interaction term. High organization tenure (red line): Y = -2.51 (6.57 + 5.12)X + 276.56. Low organization tenure (blue line): Y = -2.51 (6.57 - 5.12)X + 276.56.
</p>
</div>
<p>As shown in Figure <a href="lm.html#fig:int-effects">10.11</a>, the slope of both regression lines is negative. However, the drop in sales is much more significant as job level increases for those with high (<span class="math inline">\(\bar{x} + 1s\)</span>) organization tenure relative to those with low (<span class="math inline">\(\bar{x} - 1s\)</span>) organization tenure. Perhaps those with longer tenure in the organization gain additional responsibilities beyond selling (e.g., mentoring junior salespeople) as they are promoted into higher job levels.</p>
</div>
<div id="mediation" class="section level2" number="10.5">
<h2><span class="header-section-number">10.5</span> Mediation</h2>
<p>As discussed in Chapter <a href="measure-sampl.html#measure-sampl">4</a>, mediating variables may fully or partially mediate the relationship between a predictor and response. Full mediation indicates that the mediator fully explains the effect; in other words, without the mediator in the model, there is no relationship between an IV and DV. Partial mediation indicates that the mediator partially explains the effect; that is, there is still a relationship between an IV and DV without the mediator in the model.</p>
<p>Baron and Kenny’s (1986) four-step approach involves several regression analyses to examine paths a, b, and c shown in Figure <a href="lm.html#fig:med-paths">10.12</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:med-paths"></span>
<img src="graphics/mediation_paths.png" alt="Paths for mediation analysis" width="75%" />
<p class="caption">
Figure 10.12: Paths for mediation analysis
</p>
</div>
<ul>
<li><strong>Step 1:</strong> Fit a simple linear regression model with <span class="math inline">\(X\)</span> predicting <span class="math inline">\(Y\)</span> (path c), <span class="math inline">\(Y = \beta_0 + \beta_1 X + \epsilon\)</span>.</li>
<li><strong>Step 2:</strong> Fit a simple linear regression model with <span class="math inline">\(X\)</span> predicting <span class="math inline">\(M\)</span> (path a), <span class="math inline">\(M = \beta_0 + \beta_1 X + \epsilon\)</span>.</li>
<li><strong>Step 3:</strong> Fit a simple linear regression model with <span class="math inline">\(M\)</span> predicting <span class="math inline">\(Y\)</span> (path b), <span class="math inline">\(Y = \beta_0 + \beta_1 M + \epsilon\)</span>.</li>
<li><strong>Step 4:</strong> Fit a multiple linear regression model with <span class="math inline">\(X\)</span> and <span class="math inline">\(M\)</span> predicting <span class="math inline">\(Y\)</span> (paths b and c), <span class="math inline">\(Y = \beta_0 + \beta_1 X + \beta_2 M + \epsilon\)</span>.</li>
</ul>
<p>The purpose of Steps 1-3 is to determine if zero-order relationships exist. If one or more of these relationships is nonsignificant, mediation is unlikely – though not impossible. Mediation exists if the relationship between <span class="math inline">\(M\)</span> and <span class="math inline">\(Y\)</span> (path b) remains significant after controlling for <span class="math inline">\(X\)</span> in Step 4. If <span class="math inline">\(X\)</span> is no longer significant in Step 4, support for full mediation exists; if <span class="math inline">\(X\)</span> remains significant, support for partial mediation exists.</p>
<p>Let’s illustrate the implementation of this approach in R by testing the following hypothesis: Job level mediates the relationship between education level and YTD sales. Stated differently, the relationship between job level and YTD sales exists because those with more education tend to have higher job levels, and those in higher job levels tend to have stronger sales performance.</p>
<pre><code>## 
## Call:
## lm(formula = sqrt(ytd_sales) ~ ed_lvl, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -138.34  -36.55    0.50   34.72  249.05 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  245.229      8.472  28.945  &lt; 2e-16 ***
## ed_lvl         9.072      2.740   3.311  0.00101 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 56.89 on 407 degrees of freedom
## Multiple R-squared:  0.02623,    Adjusted R-squared:  0.02384 
## F-statistic: 10.96 on 1 and 407 DF,  p-value: 0.001012</code></pre>
<pre><code>## 
## Call:
## lm(formula = job_lvl ~ ed_lvl, data = data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.19782 -0.19782 -0.08516  0.14016  2.14016 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.74717    0.10522  16.605  &lt; 2e-16 ***
## ed_lvl       0.11266    0.03403   3.311  0.00101 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7065 on 407 degrees of freedom
## Multiple R-squared:  0.02623,    Adjusted R-squared:  0.02384 
## F-statistic: 10.96 on 1 and 407 DF,  p-value: 0.001012</code></pre>
<pre><code>## 
## Call:
## lm(formula = sqrt(ytd_sales) ~ job_lvl, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -85.572 -28.593  -2.794  25.059 148.629 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  152.761      6.156   24.81   &lt;2e-16 ***
## job_lvl       57.294      2.804   20.43   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 40.51 on 407 degrees of freedom
## Multiple R-squared:  0.5063, Adjusted R-squared:  0.5051 
## F-statistic: 417.4 on 1 and 407 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre><code>## 
## Call:
## lm(formula = sqrt(ytd_sales) ~ ed_lvl + job_lvl, data = data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -87.177 -29.481  -3.048  23.932 146.922 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  146.220      7.805  18.734   &lt;2e-16 ***
## ed_lvl         2.688      1.975   1.361    0.174    
## job_lvl       56.668      2.839  19.961   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 40.47 on 406 degrees of freedom
## Multiple R-squared:  0.5085, Adjusted R-squared:  0.5061 
## F-statistic:   210 on 2 and 406 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Output from these models show significant paths for Steps 1-3 but when adding both <code>ed_lvl</code> and <code>job_lvl</code> in the multiple regression model in Step 4, <code>ed_lvl</code> is no longer significant. Therefore, support for full mediation exists.</p>
</div>
<div id="review-questions-8" class="section level2" number="10.6">
<h2><span class="header-section-number">10.6</span> Review Questions</h2>
<ol style="list-style-type: decimal">
<li><p>What is Ordinary Least Squares (OLS) regression, and how does it work?</p></li>
<li><p>What assumptions must be satisifed to fit a linear model?</p></li>
<li><p>What does a statistically significant result for the Breusch-Pagan test indicate about linear model assumptions?</p></li>
<li><p>What does a statistically significant result for the Shapiro-Wilk test indicate about linear model assumptions?</p></li>
<li><p>In what ways can high collinearity among predictors impact the quality of model results?</p></li>
<li><p>When are outliers problematic for fitting a regression model?</p></li>
<li><p>How is unstandardized <span class="math inline">\(\beta\)</span> interpreted in the output of a linear model?</p></li>
<li><p>How does the delta between <span class="math inline">\(R^2\)</span> and Adjusted <span class="math inline">\(R^2\)</span> change as additional non-significant variables are included in a model?</p></li>
<li><p>How does the backward stepwise variable selection procedure work?</p></li>
<li><p>What is the purpose of interaction effects in a regression model?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="aod.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lme.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["The_Fundamentals_of_People_Analytics.pdf", "The_Fundamentals_of_People_Analytics.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
