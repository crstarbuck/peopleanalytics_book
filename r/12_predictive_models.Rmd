# Predictive Models {#pred-mod}

  In people analytics, inferential models like those covered in Chapters \@ref(lm) and \@ref(lme) are generally warranted by the research objectives. However, there are times when we need to go beyond interpreting coefficients to understand relative influences of predictors on an outcome and leverage the models to estimate or *predict* the most likely future values. This type of modeling is often referred to as **predictive analytics** and is the subject of this chapter.
  
  A branch of Artificial Intelligence (AI) known as Machine Learning (ML) is often associated with predictive modeling. ML is a set of methods which aim to improve performance on a set of tasks by learning from data (Mitchell, 1997). ML applications can be found in medical diagnostics, autonomous vehicles, speech recognition, automated securities trading, lending decisions, marketing, and many other domains. It is important to note that while there are AI/ML applications for people analytics, feeding data to black box models without an understanding of how the underlying algorithms work is simply a bad idea. Despite the glamour predictive analytics has seen in recent years, and the allure of a magical elixir that can portend the future, more times than not inferential statistical approaches are more appropriate in a people analytics setting. Unfortunately, the hype has given rise to AI snake oil to justify a premium price point for modern HR tech solutions, which are often little more than the descriptive statistics covered in Chapter \@ref(desc-stats).
  
  It is both a blessing and a curse that a predictive model can be built with a single line of code in R, and it is dangerous to blindly make recommendations on the basis of the output. A simpler model that you can understand and explain is generally a better option than a more complex one that you cannot. There is a high probability that stakeholders will ask deeper questions about *why* a particular segment of the workforce is predicted to exit at higher rates, for example, and answering these questions requires a deeper understanding of the factors that led to them being classified as such.
  
  People data are messy, and understanding why people vary in attitudes, perceptions, and behaviors is an inherently difficult endeavor. Spoiler alert: There is no crystal ball that will soften this reality.

## Cross-Validation

  Predictive modeling involves training a model on a data set referred to as a **training set** and using the model to make predictions for observations in a separate data set known as the **test set** or **validation set**. Since the model is blind to data in the test set, since only data in the training set are used to *train* the model, the test set provides a convenient way to compare the actual known values to the predicted values and estimate how well the model will generalize to other data. This procedure is known as **cross-validation (CV)**. While there are many methods of partitioning data into training and test sets, a common feature among all is that the partitioning strategy is random. Without randomization, the model may learn patterns characteristic of the training set that result in inaccurate predictions for other data which do not feature such patterning.
  
  Let's explore a few of the most common CV methods.
  
  **Validation Set Approach**
  
  The **validation set approach** is the most basic form of CV. This approach involves defining proportions by which to partition data into training and test sets -- usually 2/3 and 1/3, respectively. The model is trained on the training set and then differences between actual $y$ and predicted $\hat y$ values are calculated on the test set to evaluate model performance.
  
  **Leave-One-Out**
  
  **Leave-one out** CV fits a model using $n-1$ observations $n$ times. The test error is then evaluated by calculating differences between actual $y$ and predicted $\hat y$ values for all omitted observations.
  
  $\textbf k$**-Fold**
  
  $\textbf k$**-fold** CV randomly partitions observations into $k$ groups, or *folds*, that are approximately equal in size. The first fold is treated as the test set, and the model is trained on the remaining $k-1$ folds. This procedure is repeated $k$ times, each with a different set of observations (*fold*) as the test set. The test error is then evaluated by calculating differences between actual $y$ and predicted $\hat y$ values across all test sets.
  
## Model Performance

  There are several methods of quantifying how well models perform on test data in order to assess the extent to which the model will generalize. Predicting modeling applications will be categorized as either *classification* or *forecasting* to reflect the families of use cases germane to people analytics.
  
  **Classification**
  
  In a classification setting, predictions are either right or wrong. Therefore, calculating the overall error rate across test data is straightforward:
  
  $$ \frac{1}{n} \displaystyle\sum_{i=1}^{n} I(y_i \ne \hat y_i), $$
  
  where $I$ is an indicator variable equal to $1$ if $y_i \ne \hat y_i$ and $0$ if $y_i = \hat y_i$.
  
  A **confusion matrix** is often used in classification to parse the overall model accuracy rate into component parts and understand whether the model performs at a level appropriate to a defined tolerance level per the research objective. In an attrition project, it may be more important to correctly predict high performers who leave than to correctly predict those who stay, as prediction errors for the former are likely far more costly. Several performance metrics are provided by the confusion matrix to aid in a more granular understanding of model performance, which are outlined in Figure \@ref(fig:confusion-mtx):
  
  * **True Positive**: Number of correct true predictions
  * **True Negative**: Number of correct false predictions
  * **False Positive**: Number of incorrect true predictions (type 1 error)
  * **False Negative**: Number of incorrect false predictions (type 2 error)
  * **Accuracy**: Rate of correct predictions overall
  * **Sensitivity**: Rate of actual true cases predicted correctly (also known as **Recall**)
  * **Specificity**: Rate of actual false cases predicted correctly
  * **Precision**: Rate of correct predictions among all cases predicted true
  * **Negative Predictive Value**: Rate of correct predictions among all cases predicted false
  
```{r confusion-mtx, out.width = "100%", echo = FALSE, fig.cap = 'Confusion Matrix', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/confusion_matrix.png")

```
  
  **Forecasting**
  
  While predictions are either right or wrong in a classification context, evaluating model performance in a forecasting context involves assessing the *magnitude* of differences between actual $y$ and predicted $\hat y$ values -- usually across time periods. There are many methods for assessing forecasting model performance, and we will focus on some of the most common.
  
  * Mean absolute deviation (MAD): Average absolute difference between actual $y$ and predicted $\hat y$ values
  
  $$ \frac{\sum|y_i - \hat y_i|}{n} $$

  * Mean square error (MSE): Average squared difference between actual $y$ and predicted $\hat y$ values
  
  $$ \frac{\sum(y_i - \hat y_i)^2}{n} $$
  
  Squaring differences accomplishes two key objectives: (1) converts negative differences to positive consistent with the MAD approach, and (2) imposes a greater penalty on larger differences, which causes error rates to increase at an exponential rather than linear rate (e.g., $1^2 = 1$, $2^2 = 4$, $3^2 = 9$, $4^2 = 16$). MSE is perhaps the most pervasive model performance measure in predictive modeling.
  
  * Mean absolute percentage error (MAPE): Average absolute difference expressed as a percentage
  
  $$ (\frac{100}{n})\sum|\frac{y_i - \hat y_i}{y_i}| $$
  
## Bias-Variance Tradeoff

  **Bias-variance tradeoff** refers to the important endeavor of minimizing two sources of error that prevent models from generalizing beyond their training data: *bias* and *variance*. 
  
  * **Bias**: Error from erroneous assumptions in the model. High bias results from models that are too simplistic to accurately capture the relationships between predictors and the outcome; this is known as **underfitting**.
  * **Variance**: Error from sensitivity to small fluctuations in training data. High variance results from models that capture random noise rather than the significant patterns in the training data; this is known as **overfitting**.

  As a general rule, the more flexible the model, the more variance and less bias. As shown in Figure \@ref(fig:bias-var-tradeoff), minimizing test error by achieving a model with optimal fit to the data requires limiting both bias and variance.

```{r bias-var-tradeoff, out.width = "75%", echo = FALSE, fig.cap = 'Bias-variance tradeoff. Dashed line represents optimal model performance.', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/bias_variance_tradeoff.png")

```

## Tree-Based Algorithms

  While there are many flexible ML algorithms, such as **Extreme Gradient Boosting (XGBoost)** and **Support Vector Machines (SVM)**, that tend to perform well across a range of prediction problems, these will not be covered as we will focus on more interpretable tree-based algorithms that are more suitable for people analytics. 
  
  **Decision Trees**
  
  In addition to the inferential models covered in previous chapters, **decision trees** are also excellent tools that lend to simple and effective narratives about factors influencing outcomes in either a regression or classification setting. As illustrated in Figure \@ref(fig:decision-tree), decision trees resemble a tree that depicts a set of decisions as well as consequences of those decisions. The top-level `Department` variable is known as a root node, and the remaining `Tenure`, `Performance Rating`, and `Remote` nodes are known as interior nodes. Decisions represented in `Active` and `Inactive` boxes are referred to as leaf, terminal, or end nodes. 
  
  As evidenced by the inactive status prediction in the leaf node, this decision tree shows that employees in the Engineering department are unlikely to stick around for two or more years. In addition, employees in other departments terminate if they are low performers or if they are high performers who do not work remotely. It's important to note that in practice, it is rare to achieve complete purity in leaf nodes, as there is usually a mix of results in a given node -- though a more frequent class or range of values is expected. If leaf nodes for a classification problem, for example, are comprised of a single class, it may be evidence of overfitting (especially if the $n$-count is small). 
  
```{r decision-tree, out.width = "75%", echo = FALSE, fig.cap = 'Decision tree for employee attrition prediction.', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/decision_tree.png")

```
  
  The accuracy of predictions based on a deep tree with an excessive number of partitions and few observations will likely be unacceptable beyond the training data. The goal of decision trees is to arrive at a set of decisions that best delineate one class or range of values from others by identifying patterns and natural cutpoints among a reasonably large subset of the data at each level. There must be signal in the features used to partition the data such that predictions are an improvement on random guessing (e.g., 50/50 chance in a binary classification setting).

  **Random Forests**
  
  A **random forest** is a natural extension of the decision tree. As the name implies, a random forest is a large number (forest) of individual decision trees that operate as an ensemble. This is a case of *wisdom of the crowd* decision-making in which a large number of uncorrelated trees (models) functioning as a committee should outperform individual trees.
  
  To understand the mechanics of a random forest, consider an investment strategy in which you diversify an investment portfolio by spreading investments across different assets. By investing in assets that are uncorrelated, there is a lower likelihood that the portfolio's value will be negatively impacted by a negative event impacting a single holding. In the same way, a random forest constructs an ensemble of trees that are each based on different randomized subsets of data and combinations of features to amalgamate the information and arrive at more accurate predictions. The potential for poor performance from a single tree is mitigated by the many trees working in concert with one another.
  
  Though random forests combine information from many decision trees, there are still intuitive ways of understanding which features are most important in segmenting employees to understand drivers of various outcomes.

## Predictive Modeling

  We will now integrate these concepts into attrition classification and forecasting examples. The high-level prediction workflow will follow four steps:
  
  * **Step 1**: Partition data into training (2/3) and test (1/3) sets for cross-validation.
  * **Step 2**: Build models using training data.
  * **Step 3**: Use models to make predictions on test data.
  * **Step 4**: Evaluate model performance.
  
  **Classification**
  
```{r, message = FALSE, warning = FALSE}

# Load library
library(dplyr)

# Load employee data
prediction_dat <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")

```


  * **Step 1**: Partition data into training (2/3) and test (1/3) sets for cross-validation.
  
  * **Step 2**: Build models using training data.
  
    Machine learning (ML) algorithms are sensitive to imbalanced classes; that is, when there is not an equal representation of each class we wish to predict in the data (e.g., employees who left and employees who did not), it can adversely impact our results. In the case of our `employees` data set, there are 1,233 observations for employees who remained with the organization but only 237 observations for employees who left. Therefore, we will introduce a popular technique to address this known as **Synthetic Minority Oversampling Technique (SMOTE)**. Simply put, this technique takes random samples with replacement (i.e., each observation may be chosen more than once) from the minority class to augment the class's representation in the dataset and balance the classes. Note that the classes do not have to be perfectly balanced, but they need to be better balanced than they are in these data at present.
    
  While there are packages in R which provide performance metrics for predictive models, we will create a function for greater visibility of each metric. 

```{r, message = FALSE, warning = FALSE}

# Develop function for assessing classification model performance
classifer.perf <- function(truth, predicted){
  
  # Check for missing values; metrics will be computed on non-missing values only
  predicted <- predicted[!is.na(truth)]
  truth <- truth[!is.na(truth)]
  truth <- truth[!is.na(predicted)]
  
  # Produce counts for model performance metrics
  TP <- sum(truth == 1 & predicted == 1) # true positives
  TN <- sum(truth == 0 & predicted == 0) # true negatives
  FP <- sum(truth == 0 & predicted == 1) # false positives
  FN <- sum(truth == 1 & predicted == 0) # false negatives
  P <- TP + FN # total positives
  N <- FP + TN # total negatives
  
  # Store rates to variables
  accuracy <- signif(sum(truth == predicted) * 100 / length(truth), 3)
  sensitivity <- signif(100 * TP / P, 3)
  specificity <- signif(100 * TN / N, 3)
  precision <- signif(100 * TP / (TP + FP), 3)
  false.discovery <- signif(100 * FP / (TP + FP), 3)
  FPR <- signif(100 * FP / N, 3)
  
  # Return results as slots in a list
  list(accuracy = accuracy, 
       sensitivity = sensitivity, 
       specificity = specificity, 
       precision = precision, 
       false.discovery = false.discovery, 
       FPR = FPR)
}

```
  
  * **Step 3**: Use models to make predictions on test data.
  
  * **Step 4**: Evaluate model performance.
  
  
  
  **Forecasting**
  
```{r, message = FALSE, warning = FALSE}

# Load library
library(dplyr)

# Load employee data
forecasting_dat <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/turnover_trends.csv")

```
  
  * **Step 1**: Partition data into training (2/3) and test (1/3) sets for cross-validation.
  
  * **Step 2**: Build models using training data.
  
  * **Step 3**: Use models to make predictions on test data.
  
  * **Step 4**: Evaluate model performance.



## Review Questions

