# Analysis of Differences {#aod}

  There are many statistical tests that can be used to test for differences within or between two or more groups. This chapter will cover the most common types of differences in people analytics and the tests applicable to each.

## Parametric vs. Nonparametric Tests

  In the context of continuous (quantitative) data, we will cover parametric tests along with their nonparametric counterparts. When the hypothesis relates to average (mean) differences and $n$ is large, **parametric tests** are preferred as they generally have more statistical power. **Nonparametric tests** are **distribution-free tests** that do not require the population's distribution to be characterized by certain parameters, such as a normal distribution defined by a mean and standard deviation. Nonparametric tests are great for qualitative data since the distribution of non-numeric data cannot be characterized by parameters.
  
  Assuming the assumptions of parametric tests hold, parametric tests generally have more power than their nonparametric counterparts. This means that with a nonparametric test, we are less likely to reject the null hypothesis when it is false if the data come from normally distributed populations. Since the mean (expected value) is the most common measure of central tendency, parametric tests usually focus on comparing the mean or variance of data. You may recall that $\mu$ and $\sigma$ are sufficient to characterize a population distribution when data are situated symmetrically around the mean. However, the mean can be sensitive to outliers. If outliers are present in the data, the median may be a better way of representing the central tendency of data; in this case, nonparametric tests may be more appropriate. Just remember that the use of nonparametric tests requires hypotheses to be modified to adjust for *median* -- rather than *mean* -- centers.
  
  In addition to normally distributed data in the population, and ensuring outliers are not materially influencing the mean, parametric tests also assume **homogeneity of variance** and **independence**. Homogeneity of variance assumes the variances across multiple groups are equal, though parametric tests are generally robust to violations of equal variances when the sample sizes are large. The assumption of independence requires observations to be randomly sampled from the population and independent of one another; that is, the value of one observation does not influence or depend on the value of another.
  
  You may be wondering whether the magical elixir that is the CLT, which we covered in Chapter \@ref(inf-stats), influences our ability to utilize parametric tests. It's important to remember that the normal distribution properties under the CLT relate to the *sampling distribution of means* -- not to the distribution of the population or to the data for an individual sample. The CLT is important for estimating population parameters, but it does not transform a population distribution from nonnormal to normal. If we know the population distribution is nonnormal (e.g., ordinal, nominal, or skewed data), nonparametric tests should be considered. This is why we used Spearman's correlation coefficient -- a nonparametric test -- in Chapter \@ref(desc-stats) to evaluate the relationship between job level and education; these ordinal data are not normally distributed in the population.

```{r, message = FALSE, warning = FALSE}

# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")

```




```{r, message = FALSE, warning = FALSE}

# Set seed for reproducible random numbers
set.seed(123)

# Draw random numbers from normal distribution per defined parameters
a <- data.frame(x = rnorm(100000, 100, .5))
b <- data.frame(x = rnorm(100000, 101, .5))

# Label groups and combine within single df
a$group <- 'a'
b$group <- 'b'
ab <- rbind(a, b)

# Visualize distributions
ggplot2::ggplot(ab, aes(x, fill = group)) + 
ggplot2::labs(title = "Mean Difference = 1 (p < .001)", x = "x", y = "Density") + 
ggplot2::geom_density(alpha = 0.6) +
ggplot2::scale_fill_manual( values = c("skyblue","lightgrey")) +
ggplot2::theme_bw()

```

## Differences in Discrete Data



  **Chi-Square Test**

  The most common strength test when a significant Chi-Square statistic is observed is Cramerâ€™s V. 
  
  **Fischer Exact Test**
  

```{r discrete-tests, out.width = "100%", echo = FALSE, fig.cap = 'Chi-Square and Fisher Exact Test Criteria for Discrete Data', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/discrete_differences_test_table.png")

```

## Differences in Continuous Data


  Figure \@ref(fig:continuous-tests) provides a side-by-side of parametric and corresponding nonparametric tests of differences for continuous data.
  
```{r continuous-tests, out.width = "100%", echo = FALSE, fig.cap = 'Parametric and Nonparametric Tests of Differences for Continuous Data', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/continuous_differences_test_table.png")

```


### 2 Groups

  **Independent Samples T-Test**

  **Mann-Whitney U Test**
  
  **Wilcoxon Rank-Sum Test**
  
  **Wilcoxon Signed-Rank Test**

  **Paired Samples T-Test**
  
  
  
  In addition to testing for statistical significance, it is important to understand the magnitude of any observed differences. Comparing unstandardized mean differences between groups isn't too helpful, as the magnitude of difference is scale-dependent. For example, we can observe a small mean difference between two groups that is statistically significant, while a relatively large mean difference between other groups may not be statistically significant due to the spread of data. 
  
  The examples in Figure \@ref(fig:mean-group-diff) illustrate differences between unstandardized group means for random numbers sampled from normal distributions with very different parameters.

```{r mean-group-diff, out.width = "100%", echo = FALSE, fig.cap = 'Unstandardized Mean Differences of Independent Samples', fig.align = 'center', message = FALSE, warning = FALSE}

# Set seed for reproducible random numbers
set.seed(123)

### Example 1 ###

# Define sample size
n1 = 50

# Draw random numbers from normal distribution per defined parameters
a1 <- data.frame(x = rnorm(n1, 100, .5))
b1 <- data.frame(x = rnorm(n1, 101, .5))

# Label groups and combine within single df
a1$group <- 'a'
b1$group <- 'b'
ab1 <- rbind(a1, b1)

# Calculate absolute mean difference
xbar_delta_1 <- abs(round(mean(ab1[ab1$group == 'b', 'x']) - mean(ab1[ab1$group == 'a', 'x']), 0))
t_test_1 <- t.test(ab1[ab1$group == 'a', 'x'], ab1[ab1$group == 'b', 'x'])

# Store viz to object
p1 <- ggplot2::ggplot(ab1, aes(x, fill = group)) + 
      ggplot2::labs(title = paste0("MD = ", xbar_delta_1, "\n(n = ", n1, ", ", ifelse(t_test_1$p.value < .05, "p < .05", "p > .05"), ")"),
                    x = "x", y = "Density") + 
      ggplot2::scale_x_continuous(limits = c(95, 105)) +
      ggplot2::geom_density(alpha = 0.6) +
      ggplot2::scale_fill_manual( values = c("skyblue","lightgrey")) +
      ggplot2::theme_bw() +
      ggplot2::theme(plot.title = element_text(hjust = 0.5))

### Example 2 ###

# Define sample size
n2 = 50

# Draw random numbers from normal distribution per defined parameters
a2 <- data.frame(x = rnorm(n2, 200, 90))
b2 <- data.frame(x = rnorm(n2, 150, 90))

# Label groups and combine within single df
a2$group <- 'a'
b2$group <- 'b'
ab2 <- rbind(a2, b2)

# Calculate absolute mean difference
xbar_delta_2 <- abs(round(mean(ab2[ab2$group == 'b', 'x']) - mean(ab2[ab2$group == 'a', 'x']), 0))
t_test_2 <- t.test(ab2[ab2$group == 'a', 'x'], ab2[ab2$group == 'b', 'x'])

# Store viz to object
p2 <- ggplot2::ggplot(ab2, aes(x, fill = group)) + 
      ggplot2::labs(title = paste0("MD = ", xbar_delta_2, "\n(n = ", n2, ", ", ifelse(t_test_2$p.value < .05, "p < .05", "p > .05"), ")"),
                    x = "x", y = "Density") + 
      ggplot2::scale_x_continuous(limits = c(0, 500)) +
      ggplot2::geom_density(alpha = 0.6) +
      ggplot2::scale_fill_manual( values = c("skyblue","lightgrey")) +
      ggplot2::theme_bw() +
      ggplot2::theme(plot.title = element_text(hjust = 0.5))

### Plot ###

# Visualize density plots side-by-side
ggpubr::ggarrange(p1, p2, ncol = 2, nrow = 1)

```

  There are several standardized measures available for quantifying the size of observed differences beyond significance testing.

  **Cohen's d**

  **Cohen's d** is a standardized measure of the difference between two means. Cohen's $d$ is defined by:
  
  $$ d = \frac{\bar{x}_1 - \bar{x}_2} {s_p},  $$
  
  where $s_p$ represents the pooled standard deviation defined by:
  
  $$ s_p = \sqrt\frac{s^2_1 + s^2_2}{2} $$
  
  Cohen's $d$ can be produced using the `cohen.d()` function from the `effsize` package in R. The following thresholds can be referenced as a *general* rule of thumb for interpreting effect size:
  
  * **Small** = 0.2
  * **Medium** = 0.5
  * **Large** = 0.8

  **Cliff's Delta**

  **Cliff's delta** provides the effect size for ordinal variables. Simply put, Cliff's delta measures how often a value in one distribution is higher than values in another, and this is appropriate in situations in which a nonparametric test of differences is used. This statistic can be produced using the `cliff.delta()` function from the `effsize` package in R.
  
  Some (e.g., Vargha & Delaney, 2000) have endeavored to categorize the Cliff's delta statistic, which ranges from -1 to 1, into effect size buckets. However, such categorizations are far more controversial than thresholds attributed to Cohen's d.

### More Than 2 Groups

  **Analysis of Variance (ANOVA)** is used to determine whether the means of scale-level DVs are equal across nominal-level variables with three or more independent categories.
  
  It is important to understand that $H_0$ in ANOVA not only requires all group means to be equal but their complex contrasts as well. For example, if we have four groups named A, B, C, and D, $H_0$ requires that $\mu_A = \mu_B = \mu_C = \mu_D$ is true as well as the various complex contrasts such as $\mu_{A,B} = \mu_{C,D}$ and $\mu_A = \mu_{B,C,D}$ and $\mu_D = \mu_{B,C}$. Therefore, in order to reject $H_0$ in ANOVA, at least one of the possible contrasts must be different. As a result, we may find a significant $F$-statistic but no significant differences between pairwise means.
  
  In addition, it is possible to find a significant pairwise mean difference but a non-significant result from ANOVA. As you may recall from Chapter \@ref(inf-stats), multiple comparisons reduce the power of statistical tests. Since multiple tests of mean differences are performed with ANOVA, the family-wise error rate is used to adjust for the increased probability of a Type I error across the set of analyses. Since the power of a single pairwise test is greater relative to the power of familywise comparisons, we may find a significant result for the former but not the latter.
  
  ANOVA requires IVs to be categorical (nominal or ordinal) and the DV to be continuous (interval or ratio). A **one-way ANOVA** is used to determine how one categorical IV influences a continuous DV. A **two-way ANOVA** is used to determine how two categorical IVs influence a continuous DV, while a **three-way ANOVA** is used to evaluate how three categorical IVs influence a continuous DV. An ANOVA that uses two or more categorical IVs is often referred to as a **factorial ANOVA**. As discussed in Chapter \@ref(getting-started), it is important to remain grounded in specific hypotheses, as a significant ANOVA may not actually test what is being hypothesized.
  
  ANOVA is not a test, per se, but a $F$-test underpins it. The mathematical procedure behind the $F$-test is relatively straightforward:
  
  1. Compute the **within-group variance**, which is also known as residual variance. Simply put, this tells us how different each member of the group is from the average.
  2. Compute the **between-group variance**. This represents how different the group means are from one another.
  3. Produce the $F$-statistic, which is the *within-group variance* / *between-group variance*.
  
  Beyond ensuring the data were generated from a random and representative process, as well as following the data screening procedures outlined in Chapter \@ref(data-wrang-prep) (e.g., addressing any outliers), ANOVA has three key assumptions:
  
  1. **Independence**: Observations within each group are independent of each other
  2. **Homogeneity of Variance**: Variances of populations from which samples were drawn are equal
  3. **Normality**: Residuals must be normally distributed (with mean of 0) within each group
  
  **One-Way ANOVA**
  
  To illustrate how to perform a one-way ANOVA, we will test the hypothesis that mean annual compensation is equal across job satisfaction levels.
  
  Each observation in `employees` represents a unique employee, and a given employee can only have one job satisfaction score and one annual compensation value. The assumption of independence is met since each record exists independent of one another and each job satisfaction group is comprised of different employees.
  
  Levene's test (Levene, 1960) can be used to test the homogeneity of variance assumption. This can be performed in R using the `leveneTest()` function from the `car` package:
  
```{r, message = FALSE, warning = FALSE}

# Load library for Levene's test
library(car)

# Perform Levene's test for homogeneity of variance
car::leveneTest(annual_comp ~ as.factor(job_sat), data = employees)

```  
  
  The test statistic associated with Levene's test relates to the null hypothesis that there are no significant differences in variances across the job satisfaction levels. Since $p > .05$, we fail to reject this null hypothesis and can assume equal variances.
  
  Next, let's test the assumption of normality. It is important to note that the assumption of normality *does not* apply to the distribution of the DV but to the distribution of residuals for each group of the IV. Residuals in the context of ANOVA represent the difference between the actual values of the continuous DV relative to its mean value for each level of the categorical IV (e.g., $y - \bar{y}_A$, $y - \bar{y}_B$, $y - \bar{y}_C$). In ANOVA, we expect the residuals to be normally distributed around a mean of 0 (the balance point) when the data are normally distributed within each IV category; the more skewed the data, the larger the average distance of each DV value from the mean.
  
```{r comp-dist, fig.cap = "Annual Compensation Distribution by Job Satisfaction Level", fig.align = 'center', message = FALSE, warning = FALSE}

# Load data viz library
library(ggplot2)
library(ggpubr)

# Create function to visualize distribution
dist.viz <- function(data, x) {
  
viz <- ggplot2::ggplot() + 
       ggplot2::aes(data) + 
       ggplot2::labs(title = paste("Job Sat = ", x), x = "Annual Compensation", y = "Frequency") + 
       ggplot2::geom_histogram(fill = "#414141") +
       ggplot2::theme_bw() +
       ggplot2::theme(plot.title = element_text(hjust = 0.5))

  return(viz)
}

# Produce annual compensation vectors for each job satisfaction level
# Unlist() is needed to convert the default object from subset() into a numeric vector
group_1 <- unlist(subset(employees, job_sat == 1, select = annual_comp))
group_2 <- unlist(subset(employees, job_sat == 2, select = annual_comp))
group_3 <- unlist(subset(employees, job_sat == 3, select = annual_comp))
group_4 <- unlist(subset(employees, job_sat == 4, select = annual_comp))

# Call UDF to build annual comp histogram for each job satisfaction level
viz_1 <- dist.viz(data = group_1, x = 1)
viz_2 <- dist.viz(data = group_2, x = 2)
viz_3 <- dist.viz(data = group_3, x = 3)
viz_4 <- dist.viz(data = group_4, x = 4)

# Display distribution visualizations
ggpubr::ggarrange(viz_1, viz_2, viz_3, viz_4,
          ncol = 2, nrow = 2)

``` 
  
  As we can see, annual compensation is not normally distributed within job satisfaction groups. Therefore, we would not expect the distribution of residuals to be normally distributed within these groups either.
  
  To test whether the assumption of normality is met, we will first produce and review a **quantile-quantile (Q-Q) plot**. A Q-Q plot compares two probability distributions by plotting their quantiles (data partitioned into equal-sized groups) against each other. After partitioning annual compensation into groups differentiated by job satisfaction level, we can use the `ggqqplot()` function from the `ggpubr` library to build a Q-Q plot and evaluate the distribution of residuals. 

```{r qq-plot, fig.cap = "Q-Q Plot of Annual Compensation Residuals", fig.align = 'center', message = FALSE, warning = FALSE}

# Generate residuals for each group
residuals <- c(group_1 - mean(group_1), group_2 - mean(group_2), group_3 - mean(group_3), group_4 - mean(group_4))

# Create a Q-Q plot of residuals
ggpubr::ggqqplot(residuals)

``` 

  To satisfy the assumption of normality, residuals must lie along the linear line. Based on the Q-Q plot in Figure \@ref(fig:qq-plot), there is a clear departure from normality at both ends of the theoretical range.

  There are several alternatives to a visual inspection of normality, such as the ${\chi}^2$ **Goodness-of-Fit** test (Snedecor & Cochran, 1980), **Kolmogorov-Smirnov (K-S)** test (Chakravarti, Laha, & Roy, 1967), or **Shapiro-Wilk** test (Shapiro & Wilk, 1965). The general idea is consistent for each of these tests: compare observed data to what would be expected if data are sampled from a normally distributed population. The ${\chi}^2$ Goodness-of-Fit test compares the *count* of data points across the range of values relative to what would be expected in each for a sample with the same dimensions taken from a normal distribution. For example, if data are sampled from a normal population distribution, it follows that roughly half the values should exist below the mean and half above the mean. The K-S test evaluates how the observed *cumulative* distribution compares to the properties of a normal *cumulative* distribution. The Shapiro-Wilk test is based on *correlations* between observed and expected data.
  
  Let's test for normality using the Shapiro-Wilk test. The null hypothesis for the Shapiro-Wilk test is that the data are normally distributed. The `shapiro.test()` function can be used to run this test in R using the model residuals:

```{r, message = FALSE, warning = FALSE}

# Compute Shapiro-Wilk test of normality
shapiro.test(residuals)

``` 

  Since $p < .05$, we reject the null hypothesis of normally distributed data, which indicates that the assumption of normality is violated. This should not be surprising based on the deviation from normality we observed in Figure \@ref(fig:qq-plot).
  
  Because the assumption of normality is violated, we have two options. First, we can attempt to transform the data so that the residuals using the transformed values are normally distributed. If the data are resistant to transformation, we can leverage a nonparametric alternative to ANOVA.
  
  Let's first try several common data transformations and then examine the resulting Q-Q plots:

```{r qq-plots-trans, fig.cap = "Q-Q Plots of Transformed Annual Compensation Residuals", fig.align = 'center', message = FALSE, warning = FALSE}

# Build a linear model using the natural logarithm of annual comp
ln.model <- lm(log(annual_comp) ~ job_sat, data = employees)

# Build a linear model using the log base 10 of annual comp
log10.model <- lm(log10(annual_comp) ~ job_sat, data = employees)

# Build a linear model using the square root of annual comp
sqrt.model <- lm(sqrt(annual_comp) ~ job_sat, data = employees)

# Store Q-Q plots to viz objects
ln.viz <- ggpubr::ggqqplot(residuals(ln.model)) + ggtitle("Natural Log")
log10.viz <- ggpubr::ggqqplot(residuals(log10.model)) + ggtitle("Log Base 10")
sqrt.viz <- ggpubr::ggqqplot(residuals(sqrt.model)) + ggtitle("Square Root")

# Display Q-Q plots of residuals
ggpubr::ggarrange(ln.viz, log10.viz, sqrt.viz,
          ncol = 3, nrow = 1)

``` 

  Even with these transformations, there is still a clear S-shaped curve about the residuals. 

  The **Kruskal Wallis H Test** is the nonparametric alternative to a one-way ANOVA (Daniel, 1990). This test can be performed using the `kruskal.test()` function in R:

```{r, message = FALSE, warning = FALSE}

# Nonparametric Kruskal one-way ANOVA investigating median differences in annual comp by job satisfaction
kruskal.test(annual_comp ~ job_sat, data = employees)

```

  Since $p < .05$, we can conclude that there are significant differences in median compensation across the groups. However, this test does not indicate which groups are different.

```{r, message = FALSE, warning = FALSE}

pairwise.wilcox.test(employees$annual_comp, employees$job_sat, p.adjust.method = "BH")

```

  ANOVA can be performed using the `aov()` function, followed by the `summary()` function to display model output:
    
```{r, message = FALSE, warning = FALSE}

# One-way ANOVA investigating mean differences in annual comp by job satisfaction
one.way <- aov(annual_comp ~ job_sat, data = employees)
summary(one.way)

```
  
  **Two-Way ANOVA**
  
```{r, message = FALSE, warning = FALSE}

# Two-way ANOVA investigating mean differences in annual comp by job satisfaction and dept
two.way <- aov(annual_comp ~ job_sat + dept, data = employees)
summary(two.way)

```  

```{r, message = FALSE, warning = FALSE}

# ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept
interaction <- aov(annual_comp ~ job_sat * dept, data = employees)
summary(interaction)

```

### Post-Hoc Tests
  
  **Tukey's Honest Significant Difference (HSD)** test
  
  **Scheffe**
  
  **Bonferroni**


## Exercises

1. 