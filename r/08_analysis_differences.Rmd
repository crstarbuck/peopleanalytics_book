# Analysis of Differences {#aod}

  There are many statistical tests that can be used to test for differences within or between two or more groups. This chapter will cover the most common types of differences in people analytics and the tests applicable to each.

## Parametric vs. Nonparametric Tests

  In the context of continuous (quantitative) data, we will cover parametric tests along with their nonparametric counterparts. When the hypothesis relates to average (mean) differences and $n$ is large, **parametric tests** are preferred as they generally have more statistical power. **Nonparametric tests** are **distribution-free tests** that do not require the population's distribution to be characterized by certain parameters, such as a normal distribution defined by a mean and standard deviation. Nonparametric tests are great for qualitative data since the distribution of non-numeric data cannot be characterized by parameters.
  
  Beyond ensuring the data were generated from a random and representative process, as well as following the data screening procedures outlined in Chapter \@ref(data-wrang-prep) (e.g., addressing concerning outliers), parametric tests of differences generally feature three key assumptions:
  
  1. **Independence**: Observations within each group are independent of each other
  2. **Homogeneity of Variance**: Variances of populations from which samples were drawn are equal
  3. **Normality**: Residuals must be normally distributed (with mean of 0) within each group
  
  Assuming the assumptions of parametric tests hold, parametric tests generally have more power than their nonparametric counterparts. This means that with a nonparametric test, we are less likely to reject the null hypothesis when it is false if the data come from normally distributed populations. Since the mean (expected value) is the most common measure of central tendency, parametric tests usually focus on comparing the mean or variance of data. You may recall that $\mu$ and $\sigma$ are sufficient to characterize a population distribution when data are situated symmetrically around the mean. However, the mean can be sensitive to outliers so if outliers are present in the data, the median may be a better way of representing the central tendency of data. In this case, nonparametric tests may be more appropriate. Just remember that the use of nonparametric tests requires hypotheses to be modified to adjust for *median* -- rather than *mean* -- centers.
  
  In addition to normally distributed data in the population, and ensuring outliers are not materially influencing the mean, parametric tests also assume **homogeneity of variance** and **independence**. Homogeneity of variance assumes the variances across multiple groups are equal, though parametric tests are generally robust to violations of equal variances when the sample sizes are large. The assumption of independence requires observations to be randomly sampled from the population and independent of one another; that is, the value of one observation does not influence or depend on the value of another.
  
  You may be wondering whether the magical elixir that is the CLT, which we covered in Chapter \@ref(inf-stats), influences our ability to utilize parametric tests. It's important to remember that the normal distribution properties under the CLT relate to the *sampling distribution of means* -- not to the distribution of the population or to the data for an individual sample. The CLT is important for estimating population parameters, but it does not transform a population distribution from nonnormal to normal. If we know the population distribution is nonnormal (e.g., ordinal, nominal, or skewed data), nonparametric tests should be considered. This is why we used Spearman's correlation coefficient -- a nonparametric test -- in Chapter \@ref(desc-stats) to evaluate the relationship between job level and education; these ordinal data are not normally distributed in the population.

```{r, message = FALSE, warning = FALSE}

# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")

```

## Differences in Discrete Data

  Discrete data require nonparametric tests since these data do not come from normally distributed populations. The two most commonly used tests to analyze discrete variables are the *Chi-square test* and *Fischer's exact test*. Both tests organize data within 2x2 **contingency tables** which enables us to understand interrelations between variables. Figure \@ref(fig:contingency-tbl) illustrates a 2x2 contingency table for promotion frequency between remote and non-remote workers:

```{r contingency-tbl, out.width = "75%", echo = FALSE, fig.cap = '2x2 Contingency Table of Promotions for Remote and Non-Remote Workers', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/contingency_table.png")

```

  **Chi-Square Test**

  The **Chi-square test** evaluates patterns of observations to determine if categories occur more frequently than we would expect by chance. 

  The most common strength test when a significant Chi-Square statistic is observed is Cramerâ€™s V. 
  
  **Fischer's Exact Test**
  

```{r discrete-tests, out.width = "100%", echo = FALSE, fig.cap = 'Chi-Square and Fisher Exact Test Criteria for Discrete Data', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/discrete_differences_test_table.png")

```

## Differences in Continuous Data

  A variety of parametric and nonparametric tests are available for evaluating differences between variables measured on a continuous scale. Figure \@ref(fig:continuous-tests) provides a side-by-side of these parametric and corresponding nonparametric tests of differences.
  
```{r continuous-tests, out.width = "100%", echo = FALSE, fig.cap = 'Parametric and Nonparametric Tests of Differences for Continuous Data', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/continuous_differences_test_table.png")

```

  ### Independent Samples $t$-Test
  
  When evaluating differences between two independent samples, social psychology researchers generally select from two tests: *Student's* $t$*-test* and *Welch's* $t$*-test*. There are other less common alternatives, such as *Yuen's* $t$*-test* and a *bootstrapped* $t$*-test*, but these are less commonly reported in scholarly social science journals and will not be covered in this book.
  
  The **Student's** $t$**-test** is a parametric test whose assumptions of equal variances seldom hold in people analytics. **Welch's** $t$**-test** is generally preferred to the Student's $t$-test because it has been shown to provide better control of Type 1 error rates when homogeneity of variance is not met, whilst losing little robustness (e.g., Delacre, Lakens, & Leys, 2017). When $n$ is equal between groups, the Student's $t$-test is known to be robust to violations of the equal variance assumption, as long as $n$ is sufficiently high to accurately estimate parameters and the underlying distribution is not characterized by high skewness and kurtosis.
  
  Let's explore the mechanics of independent samples $t$-tests. Figure \@ref(fig:mean-group-diff) illustrates mean differences ($MD$) for nine Welch's $t$-tests based on random sample data generated from independent normal populations. Remember that statistical power increases with a large $n$, as $t$ distributions approximate a standard normal distribution with larger $df$. In the context of analysis of differences, this translates to an increase in the likelihood of detecting statistical differences in the means of two distributions. Note that for the two cases where both $MD$ and $n$ are relatively small, mean differences are not statistically significant ($p > .05$). You may also notice that as the absolute value of the $t$-statistic approaches 0, statistical differences become less likely since a smaller $t$-statistic indicates a smaller difference between mean values.

```{r mean-group-diff, out.width = "100%", echo = FALSE, fig.cap = 'Density plots comparing distributions with a mean of 100 (blue) and 120 (grey) with sample sizes of 100, 1000, and 10000 (rows) and standard deviations of 25, 50, and 75 (columns). Dashed vertical lines represent distribution means.', fig.align = 'center', message = FALSE, warning = FALSE}

# Load library
library(dplyr)

# Set seed for reproducible random numbers
set.seed(123)

# Fill sample size vector
n_counts <- c(100, 1000, 10000)

# Fill standard deviation vector
sds <- c(25, 50, 75)

# Initialize lists
p = list()
t_test = list()

# Initialize index
i = 1

for (n in n_counts){
  
  for (sd in sds){
    
    # Draw random numbers from normal distribution per defined parameters
    a <- data.frame(x = rnorm(n, 100, sd))
    b <- data.frame(x = rnorm(n, 120, sd))

    # Label groups and combine within single df
    a$group <- 'a'
    b$group <- 'b'
    ab <- rbind(a, b)

    # Store mean values in df
    mean_df <- ab %>%
               group_by(group) %>%
               summarize(mean = mean(x))

    # Calculate absolute mean difference
    xbar_delta <- abs(round(mean(ab[ab$group == 'b', 'x']) - mean(ab[ab$group == 'a', 'x']), 0))
    t_test <- t.test(ab[ab$group == 'a', 'x'], ab[ab$group == 'b', 'x'])

    # Store viz to object
    p[[i]] <- ggplot2::ggplot(ab, aes(x, fill = group)) + 
              ggplot2::labs(title = paste0("MD = ", xbar_delta, "\n t = ", round(t_test$statistic, 1), ifelse(t_test$p.value < .05, ", p < .05", ", p > .05")), x = "x", y = "Density") +  
              ggplot2::geom_density(alpha = 0.6) +
              ggplot2::scale_fill_manual(values = c("skyblue", "lightgrey")) +
              ggplot2::geom_vline(data = mean_df, aes(xintercept = mean), colour = c("blue", "#3D3D3D"), size = .5, linetype = "dashed") +
              ggplot2::theme_bw() +
              ggplot2::theme(plot.title = element_text(hjust = 0.5)) +
              ggplot2::theme(legend.position = "none")
    
    # Increment counter variable by 1
    i = i + 1
  }
}

# Visualize density plots side-by-side
ggpubr::ggarrange(p[[1]], p[[2]], p[[3]], p[[4]], p[[5]], p[[6]], p[[7]], p[[8]], p[[9]],
                  ncol = 3, nrow = 3)

```
  
  Next, we will walk through the steps involved in performing Welch's $t$-test. Let's first visualize the distribution of data for each group using boxplots:
  
```{r comp-job-boxplots, out.width = "100%", echo = FALSE, fig.cap = 'Annual Compensation Distributions for Managers and Research Scientists', fig.align = 'center', message = FALSE, warning = FALSE}

# Load library
library(ggplot2)

# Subset data
data <- subset(employees, job_title %in% c('Manager', 'Research Scientist'), select = c(annual_comp, job_title))

# Produce boxplots to visualize compensation distribution by job title
ggplot2::ggplot(data, aes(x = as.factor(job_title), y = annual_comp, color = job_title)) +
ggplot2::labs(x = "Job Title", y = "Annual Compensation") + 
ggplot2::guides(col = guide_legend("Job Title")) +
ggplot2::theme_bw() +
ggplot2::geom_boxplot()

``` 

  While the median -- rather than the mean which is being evaluated with Welch's $t$-test -- is shown in these boxplots, this is a great way to visually inspect whether there are meaningful differences in the distribution of data between groups. We could of course use density plots as an alternative. As we can see, annual compensation for employees with a Manager job title tends to be slightly higher than for those with a Research Scientist job title. However, these distributions appear to be roughly similar.
  
  Next, let's test whether the slight differences we observed visually are statistically significant. To perform Welch's $t$-test in R, we can simply pass into the `t.test()` function a numeric vector for each of the two groups.
  
```{r, message = FALSE, warning = FALSE}

# Create compensation vectors for two jobs
comp_mgr <- unlist(subset(employees, job_title == 'Manager', select = annual_comp))
comp_rsci <- unlist(subset(employees, job_title == 'Research Scientist', select = annual_comp))

# Run Welch's t-test
t.test(comp_mgr, comp_rsci)

``` 

  Baesd on this output, we can see that $\bar{x} = 139,901$ for Managers and $\bar{x} = 138,755$ for Research Scientists, and these are not statistically different ($t(159.55) = .23, p = .82$).
  
  Note that we can access specific metrics from this output by storing results to an object and then referencing specific elements by name or index:
  
```{r, message = FALSE, warning = FALSE}

# This assigns each element of results from Welch's t-test to an indexed position in the object
t_rslts <- t.test(comp_mgr, comp_rsci)

t_rslts$statistic # t-statistic
t_rslts$parameter # df
t_rslts$p.value # p-value
t_rslts$method # type of t-test

```   

  When object elements are referenced by index, the element name is displayed in the output to clarify what the metric represents:

```{r, message = FALSE, warning = FALSE}

t_rslts[1] # t-statistic
t_rslts[2] # df
t_rslts[3] # p-value
t_rslts[9] # type of t-test

```

  Given $df = 159.55$, you may be wondering how $df$ is calculated for Welch's $t$-test given that thus far, we have only discussed the basic $df$ calculation outlined in Chapter \@ref(inf-stats); namely, $df = n - 1$. Welch's $t$-test uses the **Welch-Satterthwaite equation** for $df$ (Satterthwaite, 1946; Welch, 1947). This equation approximates $df$ for a linear combination of independent sample variances; which means that if samples are not independent, this approximation may not be valid. The Welch-Satterthwaite equation is defined by: 
  
  $$ df = \frac {(\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2})^2} {\frac{1}{n_1 - 1} (\frac{s^2_1}{n_1})^2 + \frac{1}{n_2 - 1} (\frac{s^2_2}{n_2})^2} $$

  ### Mann-Whitney U Test
  
  ### Wilcoxon Rank-Sum Test
  
  ### Wilcoxon Signed-Rank Test

  ### Paired Samples T-Test
  
  
  
  In addition to testing for statistical significance, it is important to understand the magnitude of any observed differences. We must compare standardized mean differences between groups, as the magnitude of difference is scale-dependent. There are several standardized measures available for quantifying the size of observed differences beyond significance testing.

  ### Cohen's $d$

  **Cohen's** $\textbf d$ is a standardized measure of the difference between two means. Cohen's $d$ is defined by:
  
  $$ d = \frac{\bar{x}_1 - \bar{x}_2} {s_p},  $$
  
  where $s_p$ represents the pooled standard deviation defined by:
  
  $$ s_p = \sqrt\frac{s^2_1 + s^2_2}{2} $$
  
  Cohen's $d$ can be produced using the `cohen.d()` function from the `effsize` package in R. The following thresholds can be referenced as a *general* rule of thumb for interpreting effect size:
  
  * **Small** = 0.2
  * **Medium** = 0.5
  * **Large** = 0.8

  ### Cliff's Delta

  **Cliff's delta** provides the effect size for ordinal variables. Simply put, Cliff's delta measures how often a value in one distribution is higher than values in another, and this is appropriate in situations in which a nonparametric test of differences is used. This statistic can be produced using the `cliff.delta()` function from the `effsize` package in R.
  
  Some (e.g., Vargha & Delaney, 2000) have endeavored to categorize the Cliff's delta statistic, which ranges from -1 to 1, into effect size buckets. However, such categorizations are far more controversial than thresholds attributed to Cohen's d.

  ### Analysis of Variance (ANOVA)

  **Analysis of Variance (ANOVA)** is used to determine whether the means of scale-level DVs are equal across nominal-level variables with three or more independent categories.
  
  It is important to understand that $H_0$ in ANOVA not only requires all group means to be equal but their complex contrasts as well. For example, if we have four groups named A, B, C, and D, $H_0$ requires that $\mu_A = \mu_B = \mu_C = \mu_D$ is true as well as the various complex contrasts such as $\mu_{A,B} = \mu_{C,D}$ and $\mu_A = \mu_{B,C,D}$ and $\mu_D = \mu_{B,C}$. Therefore, in order to reject $H_0$ in ANOVA, at least one of the possible contrasts must be different. As a result, we may find a significant $F$-statistic but no significant differences between pairwise means.
  
  In addition, it is possible to find a significant pairwise mean difference but a non-significant result from ANOVA. As you may recall from Chapter \@ref(inf-stats), multiple comparisons reduce the power of statistical tests. Since multiple tests of mean differences are performed with ANOVA, the family-wise error rate is used to adjust for the increased probability of a Type I error across the set of analyses. Since the power of a single pairwise test is greater relative to the power of familywise comparisons, we may find a significant result for the former but not the latter.
  
  ANOVA requires IVs to be categorical (nominal or ordinal) and the DV to be continuous (interval or ratio). A **one-way ANOVA** is used to determine how one categorical IV influences a continuous DV. A **two-way ANOVA** is used to determine how two categorical IVs influence a continuous DV, while a **three-way ANOVA** is used to evaluate how three categorical IVs influence a continuous DV. An ANOVA that uses two or more categorical IVs is often referred to as a **factorial ANOVA**. As discussed in Chapter \@ref(getting-started), it is important to remain grounded in specific hypotheses, as a significant ANOVA may not actually test what is being hypothesized.
  
  ANOVA is not a test, per se, but a $F$-test underpins it. The mathematical procedure behind the $F$-test is relatively straightforward:
  
  1. Compute the **within-group variance**, which is also known as residual variance. Simply put, this tells us how different each member of the group is from the average.
  2. Compute the **between-group variance**. This represents how different the group means are from one another.
  3. Produce the $F$-statistic, which is the *within-group variance* / *between-group variance*.
  
  **One-Way ANOVA**
  
  To illustrate how to perform a one-way ANOVA, we will test the hypothesis that mean annual compensation is equal across job satisfaction levels.
  
  Each observation in `employees` represents a unique employee, and a given employee can only have one job satisfaction score and one annual compensation value. The assumption of independence is met since each record exists independent of one another and each job satisfaction group is comprised of different employees.
  
  Levene's test (Levene, 1960) can be used to test the homogeneity of variance assumption. This can be performed in R using the `leveneTest()` function from the `car` package:
  
```{r, message = FALSE, warning = FALSE}

# Load library for Levene's test
library(car)

# Perform Levene's test for homogeneity of variance
car::leveneTest(annual_comp ~ as.factor(job_sat), data = employees)

```  
  
  The test statistic associated with Levene's test relates to the null hypothesis that there are no significant differences in variances across the job satisfaction levels. Since $p > .05$, we fail to reject this null hypothesis and can assume equal variances.
  
  Next, let's test the assumption of normality. It is important to note that the assumption of normality *does not* apply to the distribution of the DV but to the distribution of residuals for each group of the IV. Residuals in the context of ANOVA represent the difference between the actual values of the continuous DV relative to its mean value for each level of the categorical IV (e.g., $y - \bar{y}_A$, $y - \bar{y}_B$, $y - \bar{y}_C$). In ANOVA, we expect the residuals to be normally distributed around a mean of 0 (the balance point) when the data are normally distributed within each IV category; the more skewed the data, the larger the average distance of each DV value from the mean.
  
```{r comp-dist, fig.cap = "Annual Compensation Distribution by Job Satisfaction Level", fig.align = 'center', message = FALSE, warning = FALSE}

# Load data viz library
library(ggplot2)
library(ggpubr)

# Create function to visualize distribution
dist.viz <- function(data, x) {
  
viz <- ggplot2::ggplot() + 
       ggplot2::aes(data) + 
       ggplot2::labs(title = paste("Job Sat = ", x), x = "Annual Compensation", y = "Frequency") + 
       ggplot2::geom_histogram(fill = "#414141") +
       ggplot2::theme_bw() +
       ggplot2::theme(plot.title = element_text(hjust = 0.5))

  return(viz)
}

# Produce annual compensation vectors for each job satisfaction level
# Unlist() is needed to convert the default object from subset() into a numeric vector
group_1 <- unlist(subset(employees, job_sat == 1, select = annual_comp))
group_2 <- unlist(subset(employees, job_sat == 2, select = annual_comp))
group_3 <- unlist(subset(employees, job_sat == 3, select = annual_comp))
group_4 <- unlist(subset(employees, job_sat == 4, select = annual_comp))

# Call UDF to build annual comp histogram for each job satisfaction level
viz_1 <- dist.viz(data = group_1, x = 1)
viz_2 <- dist.viz(data = group_2, x = 2)
viz_3 <- dist.viz(data = group_3, x = 3)
viz_4 <- dist.viz(data = group_4, x = 4)

# Display distribution visualizations
ggpubr::ggarrange(viz_1, viz_2, viz_3, viz_4,
          ncol = 2, nrow = 2)

``` 
  
  As we can see, annual compensation is not normally distributed within job satisfaction groups. Therefore, we would not expect the distribution of residuals to be normally distributed within these groups either.
  
  To test whether the assumption of normality is met, we will first produce and review a **quantile-quantile (Q-Q) plot**. A Q-Q plot compares two probability distributions by plotting their quantiles (data partitioned into equal-sized groups) against each other. After partitioning annual compensation into groups differentiated by job satisfaction level, we can use the `ggqqplot()` function from the `ggpubr` library to build a Q-Q plot and evaluate the distribution of residuals. 

```{r qq-plot, fig.cap = "Q-Q Plot of Annual Compensation Residuals", fig.align = 'center', message = FALSE, warning = FALSE}

# Generate residuals for each group
residuals <- c(group_1 - mean(group_1), group_2 - mean(group_2), group_3 - mean(group_3), group_4 - mean(group_4))

# Create a Q-Q plot of residuals
ggpubr::ggqqplot(residuals)

``` 

  To satisfy the assumption of normality, residuals must lie along the linear line. Based on the Q-Q plot in Figure \@ref(fig:qq-plot), there is a clear departure from normality at both ends of the theoretical range.

  There are several alternatives to a visual inspection of normality, such as the ${\chi}^2$ **Goodness-of-Fit** test (Snedecor & Cochran, 1980), **Kolmogorov-Smirnov (K-S)** test (Chakravarti, Laha, & Roy, 1967), or **Shapiro-Wilk** test (Shapiro & Wilk, 1965). The general idea is consistent for each of these tests: compare observed data to what would be expected if data are sampled from a normally distributed population. The ${\chi}^2$ Goodness-of-Fit test compares the *count* of data points across the range of values relative to what would be expected in each for a sample with the same dimensions taken from a normal distribution. For example, if data are sampled from a normal population distribution, it follows that roughly half the values should exist below the mean and half above the mean. The K-S test evaluates how the observed *cumulative* distribution compares to the properties of a normal *cumulative* distribution. The Shapiro-Wilk test is based on *correlations* between observed and expected data.
  
  Let's test for normality using the Shapiro-Wilk test. The null hypothesis for the Shapiro-Wilk test is that the data are normally distributed. The `shapiro.test()` function can be used to run this test in R using the model residuals:

```{r, message = FALSE, warning = FALSE}

# Compute Shapiro-Wilk test of normality
shapiro.test(residuals)

``` 

  Since $p < .05$, we reject the null hypothesis of normally distributed data, which indicates that the assumption of normality is violated. This should not be surprising based on the deviation from normality we observed in Figure \@ref(fig:qq-plot).
  
  Because the assumption of normality is violated, we have two options. First, we can attempt to transform the data so that the residuals using the transformed values are normally distributed. If the data are resistant to transformation, we can leverage a nonparametric alternative to ANOVA.
  
  Let's first try several common data transformations and then examine the resulting Q-Q plots:

```{r qq-plots-trans, fig.cap = "Q-Q Plots of Transformed Annual Compensation Residuals", fig.align = 'center', message = FALSE, warning = FALSE}

# Build a linear model using the natural logarithm of annual comp
ln.model <- lm(log(annual_comp) ~ job_sat, data = employees)

# Build a linear model using the log base 10 of annual comp
log10.model <- lm(log10(annual_comp) ~ job_sat, data = employees)

# Build a linear model using the square root of annual comp
sqrt.model <- lm(sqrt(annual_comp) ~ job_sat, data = employees)

# Store Q-Q plots to viz objects
ln.viz <- ggpubr::ggqqplot(residuals(ln.model)) + ggtitle("Natural Log")
log10.viz <- ggpubr::ggqqplot(residuals(log10.model)) + ggtitle("Log Base 10")
sqrt.viz <- ggpubr::ggqqplot(residuals(sqrt.model)) + ggtitle("Square Root")

# Display Q-Q plots of residuals
ggpubr::ggarrange(ln.viz, log10.viz, sqrt.viz,
          ncol = 3, nrow = 1)

``` 

  Even with these transformations, there is still a clear S-shaped curve about the residuals. 

  The **Kruskal Wallis H Test** is the nonparametric alternative to a one-way ANOVA (Daniel, 1990). This test can be performed using the `kruskal.test()` function in R:

```{r, message = FALSE, warning = FALSE}

# Nonparametric Kruskal one-way ANOVA investigating median differences in annual comp by job satisfaction
kruskal.test(annual_comp ~ job_sat, data = employees)

```

  Since $p < .05$, we can conclude that there are significant differences in median compensation across the groups. However, this test does not indicate which groups are different.

```{r, message = FALSE, warning = FALSE}

pairwise.wilcox.test(employees$annual_comp, employees$job_sat, p.adjust.method = "BH")

```

  ANOVA can be performed using the `aov()` function, followed by the `summary()` function to display model output:
    
```{r, message = FALSE, warning = FALSE}

# One-way ANOVA investigating mean differences in annual comp by job satisfaction
one.way <- aov(annual_comp ~ job_sat, data = employees)
summary(one.way)

```
  
  **Factorial ANOVA**
  
```{r, message = FALSE, warning = FALSE}

# Factorial ANOVA investigating mean differences in annual comp by job satisfaction and dept
factorial <- aov(annual_comp ~ job_sat + dept, data = employees)
summary(factorial)

```  

```{r, message = FALSE, warning = FALSE}

# ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept
interaction <- aov(annual_comp ~ job_sat * dept, data = employees)
summary(interaction)

```

### Post-Hoc Tests
  
  **Tukey's Honest Significant Difference (HSD)** test
  
  **Scheffe**
  
  **Bonferroni**


## Exercises

1. 