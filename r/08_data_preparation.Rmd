# Data Preparation {#data-prep}

  The initial data review process is sometimes referred to as **exploratory data analysis (EDA)**. EDA involves investigating patterns, completeness, anomalies, and assumptions using summary statistics and graphical representations.
   
## Data Screening & Cleaning

  A handy function in base R for initial data screening is `summary()`:
  
```{r, message = FALSE, warning = FALSE}

# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")

# Summarize df
summary(employees)

```

  Note that fields with `NA's` contain missing values. Also, by default `employee_id` is treated as an integer in R, which is why descriptive statistics appropriate for numeric data are provided. `employee_id` should actually be treated as a character string since we will not perform any arithmetic operations using these ids.
  
  **Missingness**

  Before considering whether and how to handle missing data, it is important to distinguish between *structural missingness* and *informative missingness* (Kuhn & Johnson, 2013). 
  
  **Structural missingness** relates to data that are missing for a logical reason. For example, we would not expect a new joiner with a few days of tenure to have a performance score. Likewise, we would not expect an active employee who is not a rehire to have a termination date. Therefore, it would not make sense to define a value to address missing data in these cases.
  
  **Informative missingness** relates to missing data that is informative regarding an outcome of interest. For example, in a survey context we may find a relationship between missing values on manager effectiveness questions and unfavorability on a psychological safety scale. This may indicate that employees who are fearful of retaliation are uncomfortable providing honest feedback about their managers, while employees who feel it is safe to speak up about issues are more comfortable responding in prosocial ways.
  
  In some cases, we have the luxury of simply removing observations with missing values and using the remaining complete cases for analysis. However, since we are often working with wide datasets containing relatively few observations in people analytics, this may not be feasible. As we will cover in later chapters, sample size considerations are fundamental to achieving adequate power in statistical testing, so case removal is only possible with larger datasets. Also, elimination of many impacted records -- even with a sufficiently large sample -- may bias analyses if there are missingness patterns. 
  
  **Data imputation** refers to the methods by which missing data are replaced with substituted values when case removal is not appropriate. The most common data imputation method is replacing missing values with a descriptive statistic such as the mean, median, or mode based on available data. For example, if most employees have an age in the system, the average, median, or most frequent age could be used in place of the cases with a missing age. To be more precise, the average, median, or most frequent age of those with *similar characteristics* may be used (e.g., similar years of experience, job, level). We would expect there to be less variability in age within a well-defined segment relative to the entire employee population, so this would likely be a more accurate substitute for an individual's actual age.
  
  Let's evaluate the `employees` data frame for missing `annual_comp` values using the logical `is.na()` function, and return values of variables relevant in determining one's annual compensation:

```{r, message = FALSE, warning = FALSE}

# Store original annual comp for sample employee
orig_comp <- subset(employees, employee_id == '2176', select = annual_comp)

# Force a NA in lieu of annual comp for illustrative purposes
employees[employees$employee_id == '2176', 'annual_comp'] <- NA

# Return relevant employee characteristics where annual comp is missing
subset(employees, is.na(annual_comp), select = c(employee_id, job_title, job_lvl))

```

  Next, we can impute the average value of `annual_comp` based on employees with the same values for the relevant variables:

```{r, message = FALSE, warning = FALSE}

# Return average annual comp for employees with similar characteristics, excluding employees with missing comp values
imputed_comp <- sapply(subset(employees, job_title == 'Manufacturing Director' & job_lvl == 2, select = annual_comp), mean, na.rm = TRUE)

# Impute missing comp for relevant segment
employees[employees$employee_id == '2176', 'annual_comp'] <- imputed_comp

# Display absolute difference between original and imputed comp
round(abs(orig_comp - subset(employees, employee_id == '2176', select = annual_comp)), 0)

```

  While this approach should help in demonstrating the mechanics of imputing a missing value on a case-by-case basis, a more scalable solution is needed for data with a large number of missing values across employees with different values of these variables. There are more sophisticated methods of data imputation that involve models to estimate missing values. *Linear regression* and *K-Nearest Neighbor (KNN)* models are commonly used for this and will be covered in later chapters. These modeling techniques leverage a similar approach to the method outlined above in that the target values of cases with similar characteristics to those with missing values are used to aid estimation. **Multiple imputation** combines the information from multiple data sets imputed using different methods with a goal of minimizing the potential bias introduced by a singular method of imputation.

  **Outliers**

  The treatment of outliers is a controversial topic. Appropriate methods for defining and addressing outliers are domain-specific, and there are many important considerations that should inform whether and how outliers should be treated. As discussed in Chapter \@ref(desc-stats), a common method of outlier detection is identifying values which fall outside the following interval:
  
  $$I = Q1 - 1.5 * IQR; Q3 + 1.5 * IQR$$

  **Low Variability**
  
  Variables with **low variability** often do not provide sufficient information for identifying patterns in data. For example, if we are interested in using information on stock options to understand why employees vary in their levels of retention risk, but find that the employee stock purchase plan (ESPP) terms are identical for nearly all employees, including a stock option variable in the analysis is unlikely to provide any meaningful signal.
  
  When working with survey data, checking for **straightlining** should be an early data screening step. Straightlining refers to a constant response across all survey items, which may be evidence that the respondent lost motivation or was not attentive and thoughtful when taking the survey. Since straight-line responses may influence results, it is often best to discard these cases -- especially when the sample size is adequately large for the planned analyses without them. If the same response is given for both positively and negatively worded versions of a question (e.g., comparing "I plan to be here in a year" to "I do not plan to be here in a year"), which we expect to be inversely related, this gives added support for discarding these responses.
  
  Fields with low variability can be easily identified using descriptive statistics from the `summary()` function. If the `Min` and `Max` values are equal, there is no variability in the field's values. Based on the following descriptives, we should remove `standard_hrs` from the data:
  
```{r, message = FALSE, warning = FALSE}

# Return descriptives to understand distribution of standard hours
summary(employees$standard_hrs)

```

  Given that the data dictionary in Chapter \@ref(getting-started) indicates performance ratings range from 1 to 4, the following descriptives should raise red flags:

```{r, message = FALSE, warning = FALSE}

# Return descriptives to understand distribution of standard hours
summary(employees$perf_rating)

```

  Assuming not everyone in the company is a stellar performer (i.e., only Noteworthy and Exceptional ratings), we may be working with a partial data set that could bias analyses. This may be due to poor integrity of performance data in the source system or repository from which the data were pulled, or the data may have been limited via a condition in the query.

  **Inconsistent Categories**
  
  **Inconsistent categories** impact aggregation and trending by categorical dimensions. It is often necessary to create mappings based on logical rules in order to standardize dimension values across time. In the case of reorgs, a department may be disbanded, renamed, or integrated into one or multiple other departments. Therefore, when working with historical data, records may contain legacy department names that do not align with the current organizational taxonomy. Mapping from former to current departments may require logic based on manager ids, divisions, job profiles, locations, or other variables depending on the nature of reorgs over time.
  
  Job architecture projects often introduce the need for mappings as well. Jobs and levels may completely change for all employees with a job architecture revamp, in which case trending along job and level dimensions (e.g., attrition by job or level over multiple years) is only possible with logic that clarifies how legacy jobs and levels map to those in the new career framework.
  
  Changes to allowable values in source systems often result in inconsistent categorical data over time. For example, the education field may switch from a free-form text field in which employees can enter any value (e.g., B.S., B.A., BS, BA, Bachelor of Science, Bachelor of Arts, Bachelor's, Bachelors, Bachelor's Degree, Bachelor Degree, undergraduate degree, 4-year degree, four-year degree) to a standardized solution in which there is a clean and well-defined set of allowable values from which employees can choose (e.g., Bachelor's Degree, Master's Degree, Doctoral Degree). This warrants either a one-time historical cleanup upon implementing the allowable values or downstream logic to tidy up data for analytics. A best practice is to address data quality issues upstream (e.g., in the source system) to avoid duplicative data cleaning procedures across downstream applications.

  **Data Binning**
  
  **Data binning** refers to the process by which larger high-level groups of values are defined and constructed. As a general rule, extremely granular categories should be avoided – especially when there is no theoretical basis for such categories facilitating a project’s objectives or deepening insights. Where the $n$-count is expected to be consistently low for a defined categorical bin, it is usually best to define a larger bin. For example, a variable measuring highest level of educational attainment that contains 9th, 10th, 11th, and 12th grade categories may be converted into higher-level “High School Not Completed” and “High School Completed” bins.
  
  For modeling applications, it is important to let the algorithm determine the cutpoints for numeric data in relation to the outcome. For example, if organization tenure is measured in years, arbitrarily defining bin sizes of 'Less Than 1 Year', '1-5 Years', and 'More Than 5 Years' will likely result in information loss. Any variability *within* these bins that may be useful in explaining variance in the outcome would be lost with such wide bins. In fact, machine learning (ML) models such as decision trees, which will be covered in later chapters, are great for algorithmically determining cut points for binning numeric data across descriptive, diagnostic, and predictive projects alike.

## One-Hot Encoding

  **One-hot encoding**, also known as **dummy coding**, involves transforming a categorical variable into numeric values on which statistical procedures can be performed. For EDA, this is not required, as counts and percent of total metrics can be calculated on these dimensions for descriptive purposes. However, for statistical and ML modeling applications, categorical variables must be converted into $k-1$ variables, where $k$ is the number of categories, using binary (1/0) coding.
  
  Understanding how categorical data are coded is critical to a correct interpretation of output. For example, if a remote work variable exists with "Remote" or "Non-Remote" values, we may code "Remote" values as 1 and "Non-Remote" values as 0. We could then evaluate the statistical relationship of this transformed categorical variable with other numeric variables. 
  
  If a categorical variable has more than 2 values, we must create a separate 1/0 field for each value and omit one category for use as a reference group. As we will cover in Chapter \@ref(lm), one of several assumptions in linear regression is that independent variables are not collinear; that is no pair of independent variables is highly correlated. Without an omitted category, each of the one-hot encoded fields will be perfectly correlated with the others. That is, when the field representing category A is 1, the fields for other categories will always be 0. As illustrated in Figure \@ref(fig:onehot-encoding), by omitting a category there will be cases when all fields have a 0 value (i.e., rows where the value is the omitted category), which will reduce the strength of the bivariate correlations.
  
```{r onehot-encoding, out.width = "75%", echo = FALSE, fig.cap = 'One-Hot Encoding', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/onehot_encoding.png")

```

  For a categorical variable with only two values, the `ifelse()` function can be leveraged to assign values:

```{r, message = FALSE, warning = FALSE}

# Return unique values of gender field
unique(employees$gender)

```

```{r, message = FALSE, warning = FALSE}

# Gender one-hot encoding
employees$gender_ohe <- ifelse(employees$gender == 'Female', 1, 0)

# Preview records
head(subset(employees, select = c(employee_id, gender_ohe)))

```

  For variables with more than 2 categories, we can leverage the `model.matrix()` function for one-hot encoding. Let's illustrate be encoding locations.

```{r, message = FALSE, warning = FALSE}

# Return counts by department
employees %>% count(dept, sort = TRUE)

```

  By default, the `model.matrix()` function will produce a matrix of 1/0 values for $k-1$ categories. The first column in the matrix is an intercept column containing a value of 1 for each row to ensure linear independence, and the default behavior results in the first value of the factor being the omitted group. For more flexibility over which value is omitted, we can drop the intercept using `-1` in the first argument passed to `model.matrix()` and then choose the reference group for the analysis in a subsequent step.
  
```{r, message = FALSE, warning = FALSE}

# Department one-hot encoding
dept_ohe <- model.matrix(~dept-1, data = employees)

# Preview data
head(dept_ohe)

```

  We will drop the department with the lowest $n$ rather than the more arbitrary method based on the first value of the factor. Since departments are coded as either $1$ or $0$, we can use the `colSums()` function to sum each column and the `which.min()` function to identify which has the lowest sum (i.e., smallest department by employee count).

```{r, message = FALSE, warning = FALSE}

# Drop department with lowest sum (lowest n-count)
dept_ohe <- dept_ohe[, -which.min(colSums(dept_ohe))]

# Preview refined one-hot encoded data
head(dept_ohe)

```

```{r, message = FALSE, warning = FALSE}

# Combine employees and matrix containing one-hot encoded departments
employees <- cbind(employees, dept_ohe)

# Drop original department field
employees <- subset(employees, select = -c(dept))

```

## Feature Engineering

  Level one people analytics tends to utilize only the delivered fields from the HRIS (e.g., location, job profile, org tenure), but a good next step is to derive smarter variables from these fields. These can then be used to cut data differently or as inputs in models. Below are some examples of how basic data available in the HRIS can be transformed into new variables that provide different information. This can be easily accomplished using the arithmetic functions we've covered.

  + Number of jobs per unit of tenure (larger proportions tend to see greater career pathing)
  + Office/remote worker (binary variable dummy coded as 1/0)
  + Local/remote manager (binary variable dummy coded as 1/0)
  + Hire/Rehire (binary variable dummy coded as 1/0)
  + Hired/acquired (proxy for culture shock effects)
  + Gender isolation (ratio of employee’s gender to number of the same within immediate work
group)
  + Generation isolation (comparison of age bracket to most frequent generational bracket within
immediate work group)
  + Ethnic isolation (ratio of employee’s ethnicity to number of the same within immediate work
group)
  + Difference between employee and manager age
  + Percentage change between last two performance appraisal scores (per competency and/or
overall)
  + Team and department quit outbreak indicators (ratio of terms over x months relative to average
headcount over x months)
  + Industry experience (binary or length in years)

## Review Questions

1. Why is it dangerous to address missing values without domain knowledge of how the data are generated?

2. What is the difference between structural and informative missingness?

3. How can missing values be addressed when impacted records cannot be eliminated from a data set?

4. When is one-hot encoding required for categorical variables?

5. When one-hot encoding a categorical variable with more than two categories, why is an omitted category important?

6. When binning numeric data, what are some considerations in determining the size of each bin?

7. Why should variables with low to no variability be dropped?

8. Where are validation rules ideally situated to limit downstream data cleaning tasks and ensure consistent categorical dimension values?

9. What does a value of `NA` indicate in a data set within R?

10. What additional variables could be derived from standard data elements?
