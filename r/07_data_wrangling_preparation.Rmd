# Data Wrangling and Preparation {#data-wrang-prep}

  To begin a data analysis, we must first access, integrate, and clean the relevant data. These tasks account for as much as 80% of a data scientist's work.
  
```{r ds-tasks, out.width = "75%", echo = FALSE, fig.cap = 'What Data Scientists Really Do', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/data_scientist_tasks.jpeg")

```
  
## Data Management
  
  Data are extracted directly from the source systems in which they are generated or from downstream repositories such as a *data lake*, *data warehouse*, or *data mart*.
  
  A **data lake** stores myriad types of data -- both structured and unstructured -- in its native format until needed. **Structured data** refers to data that fits a predefined data model, such as hire dates formatted as `MM/DD/YYYY` or zip codes stored as a five-digit string. **Unstructured data** has no predefined data model, such as audio and video files, free-form performance review comments, emails, or digital exhaust from messenging tools like Slack; it is difficult to structure this type of data within a set of related tables. 
  
  The main difference between a data lake and data warehouse is the type of data they are designed to store. A **data warehouse (DW)** is designed to support analytics across large collections of data, such as transactional data (e.g., point-of-sale systems), point-in-time snapshots (e.g., month-end close reports), survey responses, spreadsheets, and more. As shown in Figure \@ref(fig:dw-schema), data in a DW are structured and organized into schemas of related tables to enhance the performance of queries spanning sets of large and diverse data.
  
  Figure \@ref(fig:dw-schema) illustrates how worker, position, and recruiting schemas may be related. For example, a candidate submits a job application to a posted requisition, which is connected to an open position Finance approved as part of the company's workforce plan; when the selected candidate is hired, they become a worker with one or many events (hire, promotion, transfer, termination) and activities (learning, performance appraisals, surveys) during their tenure with the company. 
  
```{r dw-schema, out.width = "100%", echo = FALSE, fig.cap = 'Related Tables Organized within Schemas', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/dw_schema.png")

```

  Tables in a DW are related using a set of keys. Each table needs a **Primary Key (PK)**, which is a unique identifier for each row in the table. A PK may be a single column or multiple columns; a multi-column PK is often referred to as a *composite key*. It is generally best to leverage non-recyclable system-generated ids for PKs, as descriptors like names tend to be unreliable. A **Foreign Key (FK)** is a column whose values correspond to the values of a PK in another table. **Referential integrity** is the logical dependency of a FK on a PK, and these constraints protect against orphaned FK values in child tables by deleting PK values from an associated parent table.
  
  Figure \@ref(fig:dw-erd) shows an **Entity Relationship Diagram (ERD)** that depicts PK/FK relationships among the Position, Worker, and Requisition tables. Notice that the PK of each related table shown in Figure \@ref(fig:dw-schema) is listed as a FK.
  
```{r dw-erd, out.width = "75%", echo = FALSE, fig.cap = 'Entity Relationship Diagram (ERD)', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/dw_erd.png")

```

  A DW may contain many different types of tables, and the two most common are *Type 1* and *Type 2* tables. These tables are sometimes referred to as **slowly changing dimensions (SCD)**. A **Type 1 table** is overwritten on a regular cadence (usually daily) and contains no history -- only current values. For example, a Type 1 table may contain the latest known attributes for each active and terminated worker such as job, location, and manager. A Type 1 table is convenient when only current information needs to be queried. A **Type 2 table** is a table in which a new record is inserted when a change occurs for one or more specified dimensions. Jobs, managers, and locations are examples of slowly changing dimensions but unlike the Type 1 table which is truncated and reloaded with the latest information, the Type 2 table houses a *start date* and *end date* for each worker and dimension to facilitate reporting and analysis on changes to attribute values over time.
  
  Figure \@ref(fig:type-2-tbl) illustrates the design of a Type 2 SCD for an active worker's job, manager, and location changes. As the data show, worker 123 was promoted from Data Analyst to Sr. Data Analyst 1.5 years after joining, began reporting to their original manager (456) after a short stint reporting to someone else (789), and has worked remotely throughout their tenure with the company.
  
  Note that `end_date = '12/31/9999'` and `current_record = 'Y'` for rows that represent *current attributes* for active workers. For inactive workers, `end_date` would be set to the worker's termination date for rows that represent the *last known attributes*, along with `current_record = 'Y'`:
  
```{r type-2-tbl, out.width = "100%", echo = FALSE, fig.cap = 'Type 2 SCD', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/type_2_table.png")

```

  While we could select rows where `current_record = 'Y'` to construct a view of the last known attributes for each worker in this table, each worker would have a separate row for each dimension which isn't ideal. Using a Type 1 table simplifies the task and output, as each dimension value is stored in a separate column. Figure \@ref(fig:type-1-tbl) shows how the current record for worker 123 would look in a Type 1 SCD:

```{r type-1-tbl, out.width = "100%", echo = FALSE, fig.cap = 'Type 1 SCD', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/type_1_table.png")

```

  A **data mart** is a subset of a DW designed to easily deliver specific information to a certain set of users on a particular subject or for a well-defined use case. For example, in a people analytics context a diversity data mart could be developed to better isolate and secure restricted data such as gender and ethnicity. This data may be used to support diversity descriptives and trends for a limited audience approved by Data Privacy and Employment Law counsel based on justified business needs.
  
## SQL

  **Structured Query Language (SQL)** is the most common language used to extract and wrangle data contained in a relational database. SQL is an essential skill for anyone working in analytics.
  
  When working with large datasets, it is best to filter records on the database side to avoid reading superfluous records into an analytics tool such as R only to then filter data to the relevant subset. For example, if we are performing an analysis on employees in the Engineering department, we should filter to this subset on the database side rather than load data on the entire workforce into R before filtering to the relevant records. Fewer records can help enhance the performance of R scripts -- especially when R is running on a local machine, such as a laptop, rather than on a more powerful server. 
  
  **Basics**
  
  There are three main *clauses* in a SQL query: (a) `SELECT`, (b) `FROM`, and (c) `WHERE`. The `SELECT` and `FROM` clauses are required, though the optional `WHERE` clause is frequently utilized.
  
  * **SELECT**: Specifies the columns to include in the output
  * **FROM**: Specifies the table(s) in which the relevant data are contained
  * **WHERE**: Specifies the rows to search
  
  Despite the clauses being ordered as shown above (`SELECT` then `FROM` then `WHERE`), the `FROM` clause is actually the first to execute since we first need to identify the relevant table(s) before filtering rows and selecting columns. The `SELECT` clause is the last to execute.
  
  Additional clauses are available for grouping and sorting data.
  
  * **GROUP BY**: Specifies the columns by which data should be grouped when using aggregate functions
  * **HAVING**: Specifies conditions for filtering rows based on aggregate functions
  * **ORDER BY**: Specifies how data should be sorted in the output
  
  When implementing aggregate functions in a `SELECT` clause, such as counting, summing, or averaging a numeric field, all other non-aggregated fields must be included in the `GROUP BY` clause.
  
  Though it is important to execute SQL queries directly on the database to minimize the amount of data read into R, we will use the `sqldf` library within R to demonstrate the mechanics of a SQL query. The `sqldf` library allows us to write SQL to query a data frame in R in lieu of a database. While the syntax of SQL may vary by database, the core structure of queries is univeral.
  
  First, let's load the `sqldf` library and store data to a data frame named `demographics`:

```{r, message = FALSE, warning = FALSE}

# Load SQL library
library(sqldf)

# Read employee demographics data
demographics <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/files/employee_demographics.csv")

# Return row and column counts
dim(demographics)

```

  Next, we will apply the `sqldf` function to our data frame to extract specific rows and columns. In addition to the `SELECT`, `FROM`, and `WHERE` clauses, we will use the `LIMIT` clause to limit the number of rows that are displayed given the data frame's size ($n = 4,875$). In a practical setting, the `LIMIT` clause is only used for efficient data profiling and troubleshooting, as we would not want to arbitrarily truncate a data set used for analysis.
  
  A best practice in writing SQL is to capitalize the names of clauses and functions and to use separate lines and indentation to make the SQL statements more readable:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string using the paste() function
sql_string <- paste("SELECT
                      employee_id
                    FROM
                      demographics
                    WHERE
                      team = 'Social Marketing'
                    LIMIT 10")

# Execute SQL query
sqldf(sql_string)

```

  This query returned a list of employee ids for the 496 employees on the Social Marketing team. 
  
  **Aggregate Functions**
  
  Next, let's take a look at the average organization tenure by job for those on the Social Marketing team:
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      AVG(org_tenure)
                    FROM
                      demographics
                    WHERE
                      team = 'Social Marketing'
                    GROUP BY
                      job_title")

# Execute SQL query
sqldf(sql_string)

```

  There are 10 distinct job titles among employees on the Social Marketing team, and the average organization tenure for these ranges from `7.5` to `10`.
  
  Since there could be few employees in certain jobs, in which case average organization tenure may not be as meaningful, we can use the `COUNT(*)` function to count the number of rows for each group. In this case, `COUNT(*)` will return the number of employees in each job on the Social Marketing team. We will also utilize aliases via `AS` in the `SELECT` clause to assign different names to the output fields:
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      AVG(org_tenure) AS avg_org_tenure
                    FROM
                      demographics
                    WHERE
                      team = 'Social Marketing'
                    GROUP BY
                      job_title
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf(sql_string)

```

  The output shows that there are only 2 Vice Presidents and 10 Directors on the Social Marketing team, while other job titles are much more common. 
  
  Since relatively few employees are Vice Presidents or Directors, let's use the `HAVING` clause to only show average organization tenure for Social Marketing team jobs with more than 10 employees:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      AVG(org_tenure) AS avg_org_tenure
                    FROM
                      demographics
                    WHERE
                      team = 'Social Marketing'
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf(sql_string)

```

  **Joins**
  
  In a practical setting, the required data are rarely contained within a single table. Therefore, we must query multiple tables and join them together. Figure \@ref(fig:sql-joins) illustrates SQL join types using Venn diagrams:

```{r sql-joins, out.width = "100%", echo = FALSE, fig.cap = 'Types of SQL Joins', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/sql_joins.png")

```

  The structure of SQL queries containing each type of join is represented below:

  **LEFT INCLUSIVE**
  
```{r, message = FALSE, warning = FALSE}

#  SELECT [Output Field List]
#  FROM A 
#  LEFT OUTER JOIN B
#  ON A.Key = B.Key
  
```

  **LEFT EXCLUSIVE**

```{r, message = FALSE, warning = FALSE}

#  SELECT [Output Field List]
#  FROM A 
#  LEFT OUTER JOIN B
#  ON A.Key = B.Key
#  WHERE B.Key IS NULL

```

  **FULL OUTER INCLUSIVE**
  
```{r, message = FALSE, warning = FALSE}

#  SELECT [Output Field List]
#  FROM A 
#  FULL OUTER JOIN B
#  ON A.Key = B.Key

```

  **FULL OUTER EXCLUSIVE**

```{r, message = FALSE, warning = FALSE}
  
#  SELECT [Output Field List]
#  FROM A 
#  FULL OUTER JOIN B
#  ON A.Key = B.Key
#  WHERE A.Key IS NULL OR B.Key IS NULL  

```

  **RIGHT INCLUSIVE**

```{r, message = FALSE, warning = FALSE}
  
#  SELECT [Output Field List]
#  FROM A 
#  RIGHT OUTER JOIN B
#  ON A.Key = B.Key

```

  **RIGHT EXCLUSIVE**

```{r, message = FALSE, warning = FALSE}
  
#  SELECT [Output Field List]
#  FROM A 
#  LEFT OUTER JOIN B
#  ON A.Key = B.Key
#  WHERE A.Key IS NULL

```

  **INNER JOIN**

```{r, message = FALSE, warning = FALSE}
  
#  SELECT [Output Field List]
#  FROM A 
#  INNER JOIN B
#  ON A.Key = B.Key

```

  To illustrate how SQL joins work, let's partition the fields used in the previous SQL queries into three separate data frames. For many people analytics projects, employee id is the PK since this identifer for one employee should not be assigned to another employee -- past, present, or future. Email or network id may also be a suitable PK. We will ensure the employee_id column is present in each of the three data frames to facilitate subsequent joins:
  
```{r, message = FALSE, warning = FALSE}

# Partition select columns into three data frames
job_df <- demographics[, c("employee_id", "job_title")]
tenure_df <- demographics[, c("employee_id", "org_tenure")]
team_df <- demographics[, c("employee_id", "team")]

```  

  Next, we will query our three new dataframes to return the average organization tenure for Social Marketing team jobs with more than 10 employees. For this, we will leverage an `INNER JOIN`, which will return records only for employee ids which are present in all three data frames. For example, if a record exists in `job_df` and `tenure_df` for a particular employee id, but there is no team designation for it in `team_df`, that employee id would not be included in the output.

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      AVG(org_tenure) AS avg_org_tenure
                    FROM
                      job_df
                        INNER JOIN
                      tenure_df
                        ON
                      job_df.employee_id = tenure_df.employee_id
                        INNER JOIN
                      team_df
                        ON
                      job_df.employee_id = team_df.employee_id
                    WHERE
                      team = 'Social Marketing'
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf(sql_string)

```

  Note that the `INNER JOIN` in this SQL query was structured such that both `tenure_df` and `team_df` were joined to `job_df` via the `employee_id` column. We could have instead joined `job_df` to `team_df` and achieved the same result since all employee ids exist in all three data frames in this case.
  
  If it were possible for all employee ids to exist in `job_df` but not in either `tenure_df` or `team_df`, we could leverage a `LEFT JOIN` to ensure all records from `job_df` are included in the output irrespective of whether they have matches in `tenure_df` or `team_df`:
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      AVG(org_tenure) AS avg_org_tenure
                    FROM
                      job_df
                        LEFT JOIN
                      tenure_df
                        ON
                      job_df.employee_id = tenure_df.employee_id
                        LEFT JOIN
                      team_df
                        ON
                      job_df.employee_id = team_df.employee_id
                    WHERE
                      team = 'Social Marketing'
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf(sql_string)

```

  **Subqueries**
  
  **Subqueries** are queries nested within other queries. Subqueries are often referred to as **inner queries**, while the main queries are referred to as **outer queries**.
  
  For example, instead of applying the `team = 'Social Marketing` condition in the `WHERE` clause, we can use a subquery to pass a list of Social Marketing employee ids into the outer query for filtering. In this case, we would not need to include `team_df` in the join conditions of our main query:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      AVG(org_tenure) AS avg_org_tenure
                    FROM
                      job_df
                        LEFT JOIN
                      tenure_df
                        ON
                      job_df.employee_id = tenure_df.employee_id
                    WHERE
                      job_df.employee_id IN (SELECT employee_id FROM team_df WHERE team = 'Social Marketing')
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf(sql_string)

```

  **Derived Tables**
  
  An alternative to a subquery is creating a **derived table** in the `FROM` clause using a SQL statement. When using an `INNER JOIN` to connect `job_df` and `tenure_df` to the derived table containing Social Marketing employee ids, any records in `job_df` or `tenure_df` that do not relate to Social Marketing employees will be dropped. With this approach, we would not need a `WHERE` clause:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      AVG(org_tenure) AS avg_org_tenure
                    FROM
                      job_df
                        INNER JOIN
                      tenure_df
                        ON
                      job_df.employee_id = tenure_df.employee_id
                        INNER JOIN
                      (SELECT employee_id FROM team_df WHERE team = 'Social Marketing') social_mktg_ids
                        ON
                      job_df.employee_id = social_mktg_ids.employee_id
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf(sql_string)

```

  **Window Functions**
  
  **Window functions** are used for performing calculations over a set of rows without collapsing the records. Unlike the aggregate functions we've covered, window functions do not collapse rows into a single value; the calculated value is returned for each of the rows over which the calculation was performed.
  
  For example, we can assign an organization tenure rank by Social Marketing job using the `RANK()` and `OVER()` functions in the `SELECT` clause. The `PARTITION BY` argument functions like a `GROUP BY` clause but without collapsing rows, while the `ORDER BY` argument sorts the records in ascending (`ASC`) or descending (`DESC`) order for proper ranking:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
# Limit output to 10 records since query does not collapse records
# Since employee_id is a column in multiple tables used in this query, we need to indicate a specific table in order to include employee_id in the output
sql_string <- paste("SELECT
                      job_df.employee_id,
                      job_title,
                      org_tenure,
                      RANK () OVER (PARTITION BY job_title ORDER BY org_tenure DESC) AS org_tenure_rank
                    FROM
                      job_df
                        INNER JOIN
                      tenure_df
                        ON
                      job_df.employee_id = tenure_df.employee_id
                        INNER JOIN
                      (SELECT employee_id FROM team_df WHERE team = 'Social Marketing') social_mktg_ids
                        ON
                      job_df.employee_id = social_mktg_ids.employee_id
                    ORDER BY
                      job_title,
                      org_tenure_rank
                    LIMIT 10")

# Execute SQL query
sqldf(sql_string)

```

  Notice that in the case of organization tenure ties, the `RANK()` function assigns the same rank and then adds the number of ties to that rank to determine the rank for the next highest tenure value.
  
  We can also treat this query as a derived table, and then filter on the derived `org_tenure_rank` field to return a list of Social Marketing employees with the highest tenure for each job. The `*` symbol in the `SELECT` clause will return all columns from the specified table:
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT tbl.*
                    FROM
                    (SELECT
                      job_df.employee_id,
                      job_title,
                      org_tenure,
                      RANK () OVER (PARTITION BY job_title ORDER BY org_tenure DESC) AS org_tenure_rank
                    FROM
                      job_df
                        INNER JOIN
                      tenure_df
                        ON
                      job_df.employee_id = tenure_df.employee_id
                        INNER JOIN
                      (SELECT employee_id FROM team_df WHERE team = 'Social Marketing') social_mktg_ids
                        ON
                      job_df.employee_id = social_mktg_ids.employee_id
                    ORDER BY
                      job_title,
                      org_tenure_rank) tbl
                    WHERE
                      tbl.org_tenure_rank = 1")

# Execute SQL query
sqldf(sql_string)

```
  
## Data Screening & Cleaning

  The initial data review process is sometimes referred to as **exporatory data analysis (EDA)**. EDA involves investigating patterns, completeness, anomalies, and assumptions using summary statistics and graphical representations.

  A handy function in base R for initial data screening is `summary()`:
  
```{r, message = FALSE, warning = FALSE}

# Summarize df
summary(demographics)

```

  Note that fields with `NA's` contain missing values. Also, by default `employee_id` and `manager_id` are both treated as integers in R, which is why descriptive statistics appropriate for numeric data are provided. `employee_id` and `manager_id` should actually be treated as character strings since we will not perform any arithmetic operations using these ids.
  
  **Missingness**

  Before considering whether and how to handle missing data, it is important to distinguish between *structural missingness* and *informative missingness* (Kuhn & Johnson, 2013). 
  
  **Structural missingness** relates to data that is missing for a logical reason. For example, we would not expect a new joiner with a few days of tenure to have a performance score. Likewise, we would not expect an active employee who is not a rehire to have a termination date. Therefore, we would not want to address missing values in these cases.
  
  **Informative missingness** relates to missing data that is informative regarding an outcome of interest. For example, in a survey context we may find a relationship between missing values on manager effectiveness questions and unfavorability on a psychological safety scale. This may indicate that employees who are fearful of retaliation are uncomfortable providing honest feedback about their managers, while employees who feel it is safe to speak up about issues are more comfortable responding in prosocial ways.
  
  In some cases, we have the luxury of simply removing observations with missing values and using the remaining complete cases for analysis. However, since we are often working with wide datasets containing relatively few observations in people analytics, this may not be feasible. As we will cover in later chapters, sample size considerations are fundamental to achieving adequate power in statistical testing, so case removal is only possible with larger datasets.
  
  **Data imputation** refers to the methods by which missing data are replaced with substituted values when case removal is not appropriate. The most common data imputation method is replacing missing values with a descriptive statistic such as the mean, median, or mode based on available data. For example, if most employees have an age in the system, the average, median, or most frequent age could be used in place of the cases with a missing age. To be more precise, the average, median, or most frequent age of those with *similar characteristics* may be used (e.g., similar years of experience, job, level). We would expect there to be less variability in age within a well-defined segment relative to the entire employee population, so this would likely be a more accurate substitute for an individual's actual age.
  
  Let's evaluate the `demographics` data frame for missing `annual_comp` values using the logical `is.na()` function, and return values of variables relevant in determining one's annual compensation:

```{r, message = FALSE, warning = FALSE}

# Force a NA in lieu of annual comp for illustrative purposes
demographics[demographics$employee_id == '4305', 'annual_comp'] <- NA

# Return relevant employee characteristics where annual comp is missing
demographics[is.na(demographics$annual_comp), c("employee_id", "job_title", "location", "job_level")]

```

  Next, we can impute the average value of `annual_comp` based on employees with the same values for the relevant variables:

```{r, message = FALSE, warning = FALSE}

# Return average annual comp for employees with similar characteristics, excluding employees with missing comp values
imputed_comp <- mean(demographics[demographics$job_title == 'Manager' & demographics$location == 'HQ 1 - Austin' & demographics$job_level == 7, 'annual_comp'], na.rm = TRUE)

# Impute missing comp for relevant segment
demographics[demographics$employee_id == '4305', 'annual_comp'] <- imputed_comp

# Display imputed comp
demographics[demographics$employee_id == '4305', 'annual_comp']

```

  While this approach should help in demonstrating the mechanics of imputing a missing value on a case-by-case basis, a more scalable solution is needed for data with a large number of missing values across employees with different values of these variables. There are more sophisticated methods of data imputation that involve models to estimate missing values. *Linear regression* and *K-Nearest Neighbor (KNN)* models are commonly used for this. These modeling techniques leverage a similar approach to the method outlined above in that the target values of cases with similar characteristics to those with missing values are used to aid estimation. These models will be covered in detail in later chapters.

  **Outliers**

  The treatment of outliers is a controversial topic. Appropriate methods for defining and addressing outliers are domain-specific, and there are many important considerations that should inform whether and how outliers should be treated. As discussed in Chapter \@ref(uni-bi-stats), a common method of outlier detection is identifying values which fall outside the following interval:
  
  $$I = Q1 - 1.5 * IQR; Q3 + 1.5 * IQR$$

  **Low Variability**
  
  Variables with **low variability** often do not provide sufficient information for identifying patterns in data. For example, if we are interested in using information on stock options to understand why employees vary in their levels of retention risk, but find that the employee stock purchase plan (ESPP) terms are identical for nearly all employees, including a stock option variable in the analysis is unlikely to provide any meaningful signal.
  
  When working with survey data, checking for **straightlining** should be an early data screening step. Straightlining refers to a constant response across all survey items, which may be evidence that the respondent lost motivation or was not attentive and thoughtful when taking the survey. Since straight-line responses may influence results, it is often best to discard these cases -- especially when the sample size is adequately large for the planned analyses without them. If the same response is given for both positively and negatively worded versions of a question (e.g., comparing "I plan to be here in a year" to "I do not plan to be here in a year"), which we expect to be inversely related, this gives added support for discarding these responses.
  
  Fields with low variability can be easily identified using descriptive statistics from the `summary()` function. If the `Min` and `Max` values are equal, there is no variability in the field's values.
  
  **Inconsistent Categories**
  
  **Inconsistent categories** impact aggregation and trending by categorical dimensions. It is often necessary to create mappings based on logical rules in order to standardize dimension values across time. In the case of reorgs, a department may be disbanded, renamed, or integrated into one or multiple other departments. Therefore, when working with historical data, records may contain legacy department names that do not align with the current organizational taxonomy. Mapping from former to current departments may require logic based on manager ids, divisions, job profiles, locations, or other variables depending on the nature of reorgs over time.
  
  Job architecture projects often introduce the need for mappings as well. Jobs and levels may completely change for all employees with a job architecture revamp, in which case trending along job and level dimensions (e.g., attrition by job or level over multiple years) is only possible with logic that clarifies how legacy jobs and levels map to those in the new career framework.
  
  Changes to allowable values in source systems often result in inconsistent categorical data over time. For example, the education field may switch from a free-form text field in which employees can enter any value (e.g., B.S., B.A., BS, BA, Bachelor of Science, Bachelor of Arts, Bachelor's, Bachelors, Bachelor's Degree, Bachelor Degree, undergraduate degree, 4-year degree, four-year degree) to a standardized solution in which there is a set of allowable values from which employees can choose (e.g., Bachelor's Degree, Master's Degree, Doctoral Degree). This will either warrant a one-time historical cleanup upon implementing the allowable values or downstream logic to tidy up data for analytics. A best practice is to address data quality issues upstream to avoid duplicative data cleaning procedures across downstream applications.

  **Data Binning**
  
  **Data binning** refers to the process by which larger high-level groups of values are defined and constructed. As a general rule, very granular categories should be avoided – especially when there is no theoretical basis for such categories facilitating a project’s objectives or deepening insights. Where the $n$-count is expected to be consistently low for a defined categorical bin, it is usually best to define a larger bin. A variable measuring highest level of educational attainment that contains 9th, 10th, 11th, and 12th grade categories may be converted into higher-level “High School Not Completed” and “High School Completed” bins.
  
  For modeling applications, it is important to let the algorithm determine the cutpoints for numeric data in relation to the outcome. For example, if organization tenure is measured in years, arbitrarily defining bin sizes of 'Less Than 1 Year', '1-5 Years', and 'More Than 5 Years' will likely result in information loss. Any variability *within* these bins that may be useful in explaining variance in the outcome would be lost with such wide bins. In fact, machine learning (ML) models such as decision trees, which will be covered in later chapters, are great for algorithmically determining cut points for binning numeric data across descriptive, diagnostic, and predictive projects alike.

## One-Hot Encoding

  **One-hot encoding**, also known as **dummy coding**, involves transforming a categorical variable into numeric values on which statistical procedures can be performed. For EDA, dummy coding is not required, as metrics such as counts and percentages can be calculated on these dimensions for descriptive purposes. However, for statistical and ML modeling applications categorical variables must be converted into $k-1$ variables, where $k$ is the number of categories, using binary (1/0) coding.
  
  Understanding how categorical data are coded is critical to a correct interpretation of output. For example, if a remote work variable exists with "Remote" or "Non-Remote" values, we could code "Remote" values as 1 and "Non-Remote" values as 0. We could then evaluate the statistical relationship of this transformed categorical variable with other numeric variables. 
  
  If a categorical variable has more than 2 values, we must create a separate 1/0 field for each value and omit one category for use as a reference group. As we will cover in Chapter \@ref(lm), one of several assumptions in linear regression is that independent variables are not collinear; that is no pair of independent variables is highly correlated. Without an omitted category, each of the one-hot encoded fields will be perfectly correlated with the others; when the field representing category A is 1, the fields for other categories will always be 0. As illustrated in Figure \@ref(fig:onehot-encoding), by omitting a category there will be cases when all fields have a 0 value (i.e., rows where the value is the omitted category), which will reduce the strength of the bivariate correlations.
  
```{r onehot-encoding, out.width = "75%", echo = FALSE, fig.cap = 'One-Hot Encoding', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/onehot_encoding.png")

```

  For a categorical variable with only two values, the `ifelse()` function can be leveraged to assign values:

```{r, message = FALSE, warning = FALSE}

# Return unique values of gender field
unique(demographics$gender)

```

```{r, message = FALSE, warning = FALSE}

# Gender one-hot encoding
demographics$gender_ohe <- ifelse(demographics$gender == 'Female', 1, 0)

# Preview records
head(demographics[, c("employee_id", "gender_ohe")])

```

  For variables with more than 2 categories, we can leverage the `model.matrix()` function for one-hot encoding. Let's illustrate be encoding locations.

```{r, message = FALSE, warning = FALSE}

# Load library
library(dplyr)

# Return counts by location
demographics %>% count(location, sort = TRUE)

```

  By default, the `model.matrix()` function will produce a matrix of 1/0 values for $k-1$ categories. The first column in the matrix is an intercept column containing a value of 1 for each row to ensure linear independence, and the default behavior results in the first value of the factor being positioned as the omitted group. For more flexibility over which value is omitted, we can drop the intercept using `-1` in the first argument passed to `model.matrix()` and then choose the omitted group in a subsequent step.
  
```{r, message = FALSE, warning = FALSE}

# Location one-hot encoding
location_ohe <- model.matrix(~location-1, data = demographics)

# Preview data
head(location_ohe)

```

  We will drop the location with the lowest $n$ rather than the more arbitrary method based on the first value of the factor. Since locations are coded as either $1$ or $0$, we can use the `colSums()` function to sum each column and the `which.min()` function to identify which has the lowest sum (i.e., least common location).

```{r, message = FALSE, warning = FALSE}

# Drop location with lowest sum (lowest n-count)
location_ohe <- location_ohe[, -which.min(colSums(location_ohe))]

# Combine demographics and matrix containing one-hot encoded locations
demographics <- cbind(demographics, location_ohe)

# Drop original location field
demographics <- subset(demographics, select = -c(location))

```

## Feature Engineering

  Level one people analytics tends to utilize only the delivered fields from the HRIS (e.g., location, job profile, org tenure), but a good next step is to derive smarter variables from these fields. These can then be used to cut data differently or as inputs in models. Below are some examples of how basic data available in the HRIS can be transformed into new variables that provide different information:

  + Number of jobs per unit of tenure (larger proportions tend to see greater career pathing)
  + Office/remote worker (binary variable dummy coded as 1/0)
  + Local/remote manager (binary variable dummy coded as 1/0)
  + Hire/Rehire (binary variable dummy coded as 1/0)
  + Hired/acquired (proxy for culture shock effects)
  + Gender isolation (ratio of employee’s gender to number of the same within immediate work
group)
  + Generation isolation (comparison of age bracket to most frequent generational bracket within
immediate work group)
  + Ethnic isolation (ratio of employee’s ethnicity to number of the same within immediate work
group)
  + Difference between employee and manager age
  + Percentage change between last two performance appraisal scores (per competency and/or
overall)
  + Team and department quit outbreak indicators (ratio of terms over x months relative to average
headcount over x months)
  + Industry experience (binary or length in years)

## Exercises

1. What are the differences between data lakes, data warehouses, and data marts?

2. What is the difference between a Type 1 and Type 2 table in a DW?

3. What two clauses must always be present in a SQL query?

4. How do aggregate functions differ from window functions in SQL?

5. What is a subquery?

6. What is the difference between an INNER JOIN and LEFT JOIN?

7. Why is it dangeous to address missing values without domain knowledge of how the data are generated?

8. How can missing values be addressed when impacted records cannot be eliminated from a data set?

9. When is one-hot encoding required for categorical variables?

10. Why should variables with low to no variability be dropped?
