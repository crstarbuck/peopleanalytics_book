# Appendix

## 4D Framework

<i>
```{r, echo = FALSE, fig.cap = 'Figure 1: 4D Framework'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/4d_framework.png")

```
</i>

<br />

<b>1. Discover</b>

  You are likely familiar with the old adage: "An ounce of prevention is worth a pound of cure." Such is the case with respect to planning in an analytics context. During the Discover phase, it is important to remain in the problem zone; seek to understand your clients' needs through active listening and questions. This is not the time for solutioning or committing to any specific deliverables. If the client's needs are ambiguous, proceeding will likely be an exercise in futility. Outlined below is a set of general questions that should be considered during this initial phase to prevent allocating scarce time and resourcing to a project that ultimately misses the mark.

* Client

  Who is the client? A client can be a person or organization that has contracting you for consulting services, or an internal stakeholder within your organization who has need. What is important to them?

* Primary Objective

  + What is the client ultimately hoping to accomplish? 
  + Is the request merely to satisfy one’s curiosity, or are there actions that can realistically be taken to materially influence said objective? 

* Problem Statement

  + One of my most important early steps is clearly defining the problem statement. If your understanding of the problem -- after translating from the business terms in which it was initially expressed -- is misaligned with the client's needs, none of the subsequent steps matter.

* Guiding Theories

  + What theoretical explanations can the client offer as potential rationalizations for the phenomena of interest?
  + Are there existing theories in the organizational literature that should guide how the problem is tackled (e.g., findings from similar research implemented in other contexts)?

* Research Questions

  To respect the nuances of the problem statement, it is important to unpack it and frame as a set of overarching questions to guide the research.

  + Q1: ...
  + Q2: ...
  + Q3: ...

* Research Hypotheses

  Once research questions are developed, what do you expect to find based on anecdotal stories or empirical findings? As a next step, these expectations should be expressed in the form of research hypotheses. Please note that these research hypotheses are different from statistical hypotheses.

  + H1: ...
  + H2: ...
  + H3: ...

  To ensure the hypotheses lend themselves to actionable analyses, it is important to consider the following: "What does success look like?" In other words, once the project is complete, against which success measures will the project's success be determined? Curiosity is not a business reason and hope is not a reasonable strategy. The following questions may prove helpful in the promotion of actionable -- over merely interesting outcomes:

  + What will be done if the hypotheses are empirically supported?
  + What will be done if the hypotheses are <b> not </b> empirically supported?

* Assumptions

  At this point, it's helpful to consider what assumptions may be embedded in this discovery work. Are the questions and hypotheses rooted in what the client has theorized, or are these the product of an ambiguous understanding of the client's needs?

* Cadence
 
  + Is this analysis a one-off, or could there be a need to refresh this analysis on a regular cadence?
  + Are there dates associated with programs, actions, etc. this analysis is intended to support?

* Aggregation

  Is there a need for individual-level detail supporting the analysis? Aaggregate data should generally be the default unless a compelling justification exists and approval from legal and privacy partners is granted. One important role of analysts is to help keep the audience focused on the bigger picture and findings. Access to individual-level detail can not only introduce unnecessary legal and compliance risk but can also lead to questions and probing that can delay taking needed actions based on the results.

* Deliverable

  What is the preferred method of communicating the results of the analysis (e.g., interactive dashboard, static slide deck, document)? It is important to determine this early so that subsequent efforts can be structured to support the preferred deliverable. For example, if an interactive dashboard is preferred, does your Engineering department need to prioritize dependent tasks such as data feeds, row-level security, BI development, and production server migrations?

* Filters & Dimensions

  How does your client prefer to segment the workforce? Some common grouping dimensions are business unit, division, team, job family, location, tenure, and management level.
  
<br />
  
<b>2. Design</b>

  Perhaps the most important initial question to answer in the design phase is: "Does anything already exist that addresses part, or all, of the client’s objectives?" If the existing solution will suffice, it's possible that there is simply a communication/education gap, and you can allocate time and resources elsewhere.
  
  The end-user experience is of paramount importance during the Design phase, as solutions should have a consistent look and feel regardless of who developed the product. To achieve this, it is important to resist siloed thinking and consider the broader set of analytics solutions the team has delivered -- or is in the process of delivering.

* Data Privacy

  Are there potential concerns with the study’s objective, planned actions, and/or requested data elements from an employee privacy or legal perspective? A cross-functional data governance committee can help with efficient and consistent decisioning on requests for people data and analytics.

* Data Sources & Elements

  + What data sources are required?
  + What data elements are required?
  
  In cases where sensitive attributes such as gender, ethnicity, age, sexual orientation, and disability status are requested, it's always best to exercise a 'safety first' mentality and consult with legal and privacy partners to ensure there is comfort with the intended use of the data. The decision on whether or not to include these sensitive data elements is often less about what the audience can view (e.g., People Partners may already have access to the information at the person level in the source system) and more anchored in what they plan to do with the information.

  Is the required data already accessible in a data warehouse or other analytics environment? If not, does it need to be? What is required to achieve this?

* Data Quality

  It is important to understand the data generative process and never make assumptions about how anomalies or missing data should be interpreted. After identifying what data sources will be required for a particular analysis, it is important to meet with source system owners and data stewards to deeply understand the business processes by which data are generated in the system(s). Are there data quality concerns that need to be explored and addressed?

* Variables

  How will the constructs be measured (e.g., survey instrument, derived attribute, calculated field)?

* Analysis Method

  What are the appropriate analysis methods based on the research hypotheses? If modeling is required, is it more important to index on accuracy or interpretability?

* Dependencies

  Are other teams required to develop this solution? What is the nature of the work each dependent team will perform? Are there required system configuration changes? Do these teams have capacity to support?

* Change Management

  Will this solution impact current processes or solutions? If so, what is the change management plan to facilitate a seamless transition and user experience?

* Sign-Off

  Generally, it is best for the client to signoff on the problem statement, analysis approach, and wire frame for the deliverable (if applicable) before providing an ETA and proceeding to the development phase. This ensures alignment on the client's needs and the perceived utility of the solution in addressing those needs.

<br />

<b>3. Develop</b>

* Development Patterns

  + Are there development patterns that should guide the development approach to support consistency? 
  + Are there existing calculated fields that can/should be leveraged for derived data?
  + Are there best practices that should be employed to optimize performance (e.g., load time for dashboards, executing complex queries during non-peak times)?
  + Are there standard color palettes that should be applied?

* Productionalizable Code

  + How do models and data science pipelines need to be developed to facilitate a seamless migration from lower to upper environments? For example, initial exploratory data analysis (EDA) may be performed using curated data in flat files for the purpose of identifying meaningful trends, relationships, and differences, but where will this data need to be sourced in production to automate the refresh of models at a regular interval? If the data were provided from multiple source systems, what joins are required to integrate the data? What transformation logic or business rules need to be applied to reproduce the curated data?

* Unit Testing

  + What test cases will ensure the veracity of data?
  + Who will perform the testing?

* UAT Testing

  + In the spirit of agility and constant contact with the client to prevent surprises, it is generally a good idea to have the client take the solution for a test run within the UAT environment and then provide sign-off before migrating to production. If the deliverable is a deck or doc with results from a model, UAT may surface clarifying questions that can be addressed before releasing to the broader audience.

<br />

<b>4. Deliver</b>

  The Deliver phase can take many forms depending on the solution being released. If the solution is designed for a large user base, a series of recorded trainings may be in order so that there is a helpful reference for those unable to attend the live sessions or new joiners in the future. It is important to monitor success measures, which could be insights aligned to research hypotheses, dashboard utilization metrics, or any number of others defined within the Discover phase.
