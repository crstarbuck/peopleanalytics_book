# Data Wrangling {#data-wrangl}

  To begin a data analysis, we must first access and organize the relevant data. These tasks -- together with data preparation steps outlined in Chapter \@ref(data-prep) -- account for a large part of the work analytics professionals do.
  
```{r ds-tasks, out.width = "75%", echo = FALSE, fig.cap = 'What Data Scientists Really Do', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/data_scientist_tasks.jpeg")

```
  
## Data Management
  
  Data are generally extracted directly from the source systems in which they are generated or from downstream repositories such as a *data lake*, *data warehouse*, or *data mart*.
  
  A **data lake** stores myriad types of data -- both structured and unstructured -- in its native format until needed. **Structured data** refers to data that fits a predefined data model, such as hire dates formatted as `MM/DD/YYYY` or zip codes stored as a five-digit string. **Unstructured data** has no predefined data model, such as audio and video files, free-form performance review comments, emails, or digital exhaust from messaging tools; it is difficult to structure this type of data within a set of related tables. 
  
  The main difference between a data lake and data warehouse is the type of data they are designed to store. A **data warehouse (DW)** is designed to support analytics across large collections of data, such as transactional data (e.g., point-of-sale systems), point-in-time snapshots (e.g., month-end close reports), survey responses, spreadsheets, and more. As shown in Figure \@ref(fig:dw-schema), data in a DW are structured and organized into schemas of related tables to enhance the performance of queries spanning sets of large and diverse data.
  
  Figure \@ref(fig:dw-schema) illustrates how worker, position, and recruiting schemas may be related. For example, a candidate submits a job application to a posted requisition, which is connected to an open position Finance approved as part of the company's workforce plan; when the selected candidate is hired, they become a worker with one or many events (hire, promotion, transfer, termination) and activities (learning, performance appraisals, surveys) during their tenure with the company. 
  
```{r dw-schema, out.width = "100%", echo = FALSE, fig.cap = 'Related Tables Organized within Schemas', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/dw_schema.png")

```

  Tables in a DW are related using a set of keys. Each table needs a **Primary Key (PK)**, which is a unique identifier for each row in the table. A PK may be a single column or multiple columns; a multi-column PK is often referred to as a *composite key*. It is generally best to leverage non-recyclable system-generated ids for PKs, as descriptors like names tend to be unreliable. A **Foreign Key (FK)** is a column whose values correspond to the values of a PK in another table. **Referential integrity** is the logical dependency of a FK on a PK, and these constraints protect against orphaned FK values in child tables by deleting PK values from an associated parent table.
  
  Figure \@ref(fig:dw-erd) shows an **Entity Relationship Diagram (ERD)** that depicts PK/FK relationships among the Position, Worker, and Requisition tables. Notice that the PK of each related table shown in Figure \@ref(fig:dw-schema) is listed as a FK.
  
```{r dw-erd, out.width = "75%", echo = FALSE, fig.cap = 'Entity Relationship Diagram (ERD)', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/dw_erd.png")

```

  A DW may contain many different types of tables, but this chapter will focus on the two most common which are known as *Type 1* and *Type 2* tables. These tables are sometimes referred to as **slowly changing dimensions (SCD)**. A **Type 1 table** is overwritten on a regular cadence (usually daily) and contains no history -- only current values. For example, a Type 1 table may contain the latest known attributes for each active and terminated worker such as job, location, and manager. A Type 1 table is convenient when only current information needs to be queried. A **Type 2 table** is a table in which a new record is inserted when a change occurs for one or more specified dimensions. Jobs, managers, and locations are examples of slowly changing dimensions but unlike the Type 1 table which contains only the latest information, the Type 2 table houses a *start date* and *end date* for each worker and dimension to facilitate reporting and analysis on changes to attribute values over time.
  
  Figure \@ref(fig:type-2-tbl) illustrates the design of a Type 2 SCD for an active worker's job, manager, and location changes. As the data show, worker 123 was promoted from Data Analyst to Sr. Data Analyst 1.5 years after joining, began reporting to their original manager (456) after a short stint reporting to someone else (789), and has worked remotely throughout their tenure with the company.
  
  Note that the combination of `end_date = '12/31/9999'` and `current_record = 'Y'` indicate *current attributes* for active workers. For inactive workers, `end_date` is set to the worker's termination date for rows that represent the *last known attributes* along with `current_record = 'Y'`:
  
```{r type-2-tbl, out.width = "100%", echo = FALSE, fig.cap = 'Type 2 SCD', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/type_2_table.png")

```

  While we could select rows where `current_record = 'Y'` to construct a view of the last known attributes for each worker in this table, the task would be unnecessarily complex. Using a Type 1 table simplifies the task and output since each dimension value is stored in a separate column and there is only one row per employee. Figure \@ref(fig:type-1-tbl) shows how the current record for worker 123 would look in a Type 1 SCD:

```{r type-1-tbl, out.width = "100%", echo = FALSE, fig.cap = 'Type 1 SCD', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/type_1_table.png")

```

  A **data mart** is a subset of a DW designed to easily deliver specific information to a certain set of users on a particular subject or for a well-defined use case. For example, in a people analytics context a diversity data mart could be developed to better isolate and secure restricted data such as gender and ethnicity. This data may be used to support diversity descriptives and trends for a limited audience approved by Data Privacy and Employment Law counsel based on justified business needs.
  
  **Modern Data Architecture**
  
  Though a deep treatment of data architecture is beyond the scope of this book, it is important to acknowledge the significant advancements in infrastructure and computation since these complex SCD architectures were introduced by Kimball (1996). These developments have greatly improved the efficiency with which analysts can translate data into information and insight. The primitive systems in place decades ago required expensive **ETL** jobs to **extract (E)** data from source systems, **transform (T)** data using complex transformation rules, and **load (L)** the transformed data to relational tables -- which then required complex queries to wrangle and analyze the data. 
  
  With modern cloud environments, the significant investment associated with humans designing, developing, and maintaining these complex architectures is often difficult to justify given how inexpensive storage and compute have become. Increasingly, the heavy computational tasks have migrated out of data pipelines and into the analytics layer wherein analytics teams have more flexibility and control. Today, daily snapshots containing *all* current and historical records can be copied and stored within partitioned DW tables for a negligible increase in storage costs, and this greatly simplifies data pipeline complexity and engineering support requirements. This changing dynamic has given rise to a new breed of data engineers focused on optimizing the heavy computation requirements of analytics teams.
  
## SQL

  **Structured Query Language (SQL)** is the most common language used to extract and wrangle data contained in a relational database. SQL is an essential skill for anyone working in analytics.
  
  When working with large datasets, it is best to filter records on the database side to avoid reading superfluous records into an analytics tool such as R only to then filter data to the relevant subset. For example, if we are performing an analysis on employees in the Engineering department, we should ideally filter to this subset on the database side rather than loading into R data on the entire workforce before filtering down to the relevant records. Fewer records can help enhance the performance of R scripts -- especially when R is running on a local machine, such as a laptop, rather than on a more powerful server. 
  
  **Basics**
  
  There are three main *clauses* in a SQL query: (a) `SELECT`, (b) `FROM`, and (c) `WHERE`. The `SELECT` and `FROM` clauses are required, though the optional `WHERE` clause is frequently needed.
  
  * **SELECT**: Specifies the columns to include in the output
  * **FROM**: Specifies the table(s) in which the relevant data are contained
  * **WHERE**: Specifies the rows to search
  
  Despite the clauses being ordered as shown above (`SELECT` then `FROM` then `WHERE`), the `FROM` clause is the first to execute since we first need to identify the relevant table(s) before filtering rows and selecting columns. The `SELECT` clause is the last to execute.
  
  Additional clauses are available for grouping and sorting data.
  
  * **GROUP BY**: Specifies the columns by which data should be grouped when using aggregate functions
  * **HAVING**: Specifies conditions for filtering rows based on aggregate functions
  * **ORDER BY**: Specifies how data should be sorted in the output
  
  When implementing aggregate functions in a `SELECT` clause, such as counting, summing, or averaging a numeric field, all other non-aggregated fields must be included in the `GROUP BY` clause.
  
  Though it is important to execute SQL queries directly on the database to minimize the amount of data read into R, we will use the `sqldf` library within R to demonstrate the mechanics of a SQL query for easily replicable examples. The `sqldf` library allows us to write SQL to query a data frame in R in lieu of a database. While the syntax of SQL may vary by database, the core structure of queries is universal.
  
  First, let's load the data sets:

```{r, message = FALSE, warning = FALSE}

# Load data sets
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
status <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/status.csv")
benefits <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/benefits.csv")
demographics <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/demographics.csv")
job <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/job.csv")
payroll <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/payroll.csv")
performance <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/performance.csv")
prior_employment <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/prior_employment.csv")
survey_response <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/survey_response.csv")
tenure <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/tenure.csv")

# Return row and column counts
dim(employees)

```

  Next, we will apply the `sqldf()` function to our data frame to extract specific rows and columns. In addition to the `SELECT`, `FROM`, and `WHERE` clauses, we will use the `LIMIT` clause to limit the number of rows that are displayed given the data frame's size ($n = 1,470$). In a practical setting, the `LIMIT` clause is only used for efficient data profiling and troubleshooting, as we would not want to arbitrarily truncate a data set used for analysis.
  
  A best practice in writing SQL is to capitalize the names of clauses and functions and to use separate lines and indentation to make the SQL statements more readable:

```{r, message = FALSE, warning = FALSE}

# Load SQL library
library(sqldf)

# Store SQL query as a character string using the paste() function
sql_string <- paste("SELECT
                      employee_id
                    FROM
                      employees
                    WHERE
                      dept = 'Research & Development'
                    LIMIT 10")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  This query returned a list of employee ids for employees in the Research & Development department. 
  
  To optimize query performance, it is important to order conditions in the `WHERE` clause beginning with the condition that will exclude the largest number of records. Conditions are executed sequentially, and each subsequent condition must evaluate all records that remain following any preceding filtering. Limiting the number of records that must be searched when evaluating each condition will reduce the time it takes the query to return results. For example, if two conditions are needed and one excludes 5,000 records while the other excludes 10, the condition that excludes 5,000 records should be listed first in the `WHERE` clause.
  
  **Aggregate Functions**
  
  Next, let's take a look at average organization tenure by job for those in the Research & Development department:
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      AVG(org_tenure)
                    FROM
                      employees
                    WHERE
                      dept = 'Research & Development'
                    GROUP BY
                      job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  There are 7 distinct job titles among employees in the Research & Development department, and the average organization tenure for these ranges from `5` to `13.7` years.
  
  Since there could be a small number of employees in certain jobs, in which case average organization tenure may not be as meaningful, we can use the `COUNT(*)` function to count the number of rows for each group. In this case, `COUNT(*)` will return the number of employees in each job in the Research & Development department. We will also apply column aliases via `AS` in the `SELECT` clause to assign different names to the output fields:
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      AVG(org_tenure) AS avg_org_tenure
                    FROM
                      employees
                    WHERE
                      dept = 'Research & Development'
                    GROUP BY
                      job_title
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  The output shows that there are only 2 Vice Presidents in the Research & Development department, while other job titles are much more prevalent. 
  
  Since relatively few employees are Vice Presidents, let's use the `HAVING` clause to only show average organization tenure for Research & Development department jobs with more than 10 employees. We can also use the `ROUND()` function to truncate average organization tenure to one significant digit:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      ROUND(AVG(org_tenure), 1) AS avg_org_tenure
                    FROM
                      employees
                    WHERE
                      dept = 'Research & Development'
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  **Joins**
  
  In a practical setting, the required data are rarely contained within a single table. Therefore, we must query multiple tables and join them together. Figure \@ref(fig:sql-joins) illustrates SQL join types using Venn diagrams:

```{r sql-joins, out.width = "100%", echo = FALSE, fig.cap = 'Types of SQL Joins', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/sql_joins.png")

```

  The structure of SQL queries containing each type of join is represented below:

  **LEFT INCLUSIVE**
  
```{r, message = FALSE, warning = FALSE}

#  SELECT [Output Field List]
#  FROM A 
#  LEFT OUTER JOIN B
#  ON A.Key = B.Key
  
```

  **LEFT EXCLUSIVE**

```{r, message = FALSE, warning = FALSE}

#  SELECT [Output Field List]
#  FROM A 
#  LEFT OUTER JOIN B
#  ON A.Key = B.Key
#  WHERE B.Key IS NULL

```

  **FULL OUTER INCLUSIVE**
  
```{r, message = FALSE, warning = FALSE}

#  SELECT [Output Field List]
#  FROM A 
#  FULL OUTER JOIN B
#  ON A.Key = B.Key

```

  **FULL OUTER EXCLUSIVE**

```{r, message = FALSE, warning = FALSE}
  
#  SELECT [Output Field List]
#  FROM A 
#  FULL OUTER JOIN B
#  ON A.Key = B.Key
#  WHERE A.Key IS NULL OR B.Key IS NULL  

```

  **RIGHT INCLUSIVE**

```{r, message = FALSE, warning = FALSE}
  
#  SELECT [Output Field List]
#  FROM A 
#  RIGHT OUTER JOIN B
#  ON A.Key = B.Key

```

  **RIGHT EXCLUSIVE**

```{r, message = FALSE, warning = FALSE}
  
#  SELECT [Output Field List]
#  FROM A 
#  LEFT OUTER JOIN B
#  ON A.Key = B.Key
#  WHERE A.Key IS NULL

```

  **INNER JOIN**

```{r, message = FALSE, warning = FALSE}
  
#  SELECT [Output Field List]
#  FROM A 
#  INNER JOIN B
#  ON A.Key = B.Key

```

  To illustrate how SQL joins work, we will leverage three of the data sets used to produce the consolidated `employees` data set we have been using: `job`, `tenure`, and `demographics`. For many people analytics projects, employee id is the PK since this identifier for one employee should not be assigned to another employee -- past, present, or future. Email or network id may also be a suitable PK. We will use the `employee_id` column in each of the three data frames to facilitate joins:
  
  Next, we will query these data frames to return the average organization tenure and average commute distance for employees in the Research & Development department, grouped by jobs with more than 10 employees. For this, we will leverage an `INNER JOIN`, which will return records only for employee ids which are present in all three data frames. For example, if a record exists in `demographics` and `tenure` for a particular employee id, but there is no corresponding record in `job`, that employee id would not be included in the output.

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      ROUND(AVG(org_tenure), 1) AS avg_org_tenure,
                      ROUND(AVG(commute_dist), 1) AS avg_commute_dist
                    FROM
                        demographics
                      INNER JOIN
                        tenure
                      ON
                        demographics.employee_id = tenure.employee_id
                      INNER JOIN
                        job
                      ON
                        demographics.employee_id = job.employee_id
                    WHERE
                      dept = 'Research & Development'
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  Note that the `INNER JOIN` in this SQL query was structured such that both `tenure` and `job` were joined to `demographics` via the `employee_id` column. We could have instead joined `job` to `tenure` since we joined `tenure` to `demographics`; this would have achieved the same result since all employee ids exist in each of the three data frames.
  
  If it were possible for all employee ids to exist in `demographics` but not in either `tenure` or `job`, we could leverage a `LEFT JOIN` to ensure all records from `demographics` are included in the output irrespective of whether they have matches in `tenure` or `job`:
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      ROUND(AVG(org_tenure), 1) AS avg_org_tenure,
                      ROUND(AVG(commute_dist), 1) AS avg_commute_dist
                    FROM
                        demographics
                      LEFT JOIN
                        tenure
                      ON
                        demographics.employee_id = tenure.employee_id
                      LEFT JOIN
                        job
                      ON
                        demographics.employee_id = job.employee_id
                    WHERE
                      dept = 'Research & Development'
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  In this case, if demographics is the base data set which contains all employee ids (i.e., the 'LEFT' dataset), it is important for `tenure` and `job` to be joined to it. Joining `job` to `tenure` may result in information loss if an employee id exists in `demographics` and `job` but not in the intermediate `tenure` data set.

  When integrating data within R, the `tidyverse` provides a more efficient and parsimonious method of joining many data sets using various join types. Within this framework, components are chained together via the package-specific `%>%` operator. The example below joins nine data sets into a single `employees` data frame using a left join on the `employee_id` column:

```{r, message = FALSE, warning = FALSE}

# Load tidyverse
library(tidyverse)

employees <- list(demographics,
                  status,
                  benefits,
                  job,
                  payroll,
                  performance,
                  prior_employment,
                  survey_response,
                  tenure) %>%
                  reduce(left_join, by = "employee_id")
                  
```

  **Subqueries**
  
  **Subqueries** are queries nested within other queries. Subqueries are often referred to as **inner queries**, while the main queries are referred to as **outer queries**.
  
  For example, if we are interested in performing an analysis on employees with more than a year of organization tenure, we can use a subquery to pass a list of employee ids that meet this criterion into the outer query for filtering. In this case, we would not need to include `tenure` in the join conditions of our main query:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
# Note: Since employee_id exists in multiple data sets, we must name a specific data set when referencing it
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      ROUND(AVG(commute_dist), 1) AS avg_commute_dist
                    FROM
                        demographics
                      LEFT JOIN
                        job
                      ON
                        demographics.employee_id = job.employee_id
                    WHERE
                        demographics.employee_id IN (SELECT employee_id FROM tenure WHERE org_tenure > 1)
                      AND 
                        dept = 'Research & Development'
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  **Virtual Tables**
  
  An alternative to a subquery is creating a **virtual table** in the `FROM` clause. When using an `INNER JOIN` to connect `demographics` to the virtual table `ids`, which provides a list of employee ids for those with more than a year of organization tenure, any records in `demographics` or `job` that do not relate to employees with at least a year of organization tenure will be dropped. This is true even though a `LEFT JOIN` is used to join `job` to `demographics` since records in `demographics` will be filtered based on `employee_id` matches in the virtual table. With this approach, our `WHERE` clause is limited to the `department = 'Research & Development'` condition:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                      job_title,
                      COUNT(*) AS employee_cnt,
                      ROUND(AVG(commute_dist), 1) AS avg_commute_dist
                    FROM
                        demographics
                      LEFT JOIN
                        job
                      ON
                        demographics.employee_id = job.employee_id
                      INNER JOIN
                        (SELECT employee_id FROM tenure WHERE org_tenure > 1) ids
                      ON
                        demographics.employee_id = ids.employee_id
                    WHERE
                      dept = 'Research & Development'
                    GROUP BY
                      job_title
                    HAVING
                      COUNT(*) > 10
                    ORDER BY
                      job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  As you can see, the output of the query using a virtual table matches the results from the preceding approach that utilized a subquery.

  **Window Functions**
  
  **Window functions** are used for performing calculations over a set of rows without collapsing the records. Unlike the aggregate functions we've covered, window functions do not collapse rows into a single value; the calculated value is returned for each of the rows over which the calculation is performed.
  
  For example, we can assign an organization tenure rank by Research & Development job using the `RANK()` and `OVER()` functions in the `SELECT` clause. The `PARTITION BY` argument functions like a `GROUP BY` clause but without collapsing rows, while the `ORDER BY` argument sorts the records in ascending (`ASC`) or descending (`DESC`) order for proper ranking:

```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
# Limit output to 10 records since query does not collapse records
sql_string <- paste("SELECT
                      demographics.employee_id,
                      job_title,
                      commute_dist,
                      RANK () OVER (PARTITION BY job_title ORDER BY commute_dist DESC) AS commute_dist_rank
                    FROM
                        demographics
                      LEFT JOIN
                        job
                      ON
                        demographics.employee_id = job.employee_id
                      INNER JOIN
                        (SELECT employee_id FROM tenure WHERE org_tenure > 1) ids
                      ON
                        demographics.employee_id = ids.employee_id
                    WHERE
                      dept = 'Research & Development'
                    ORDER BY
                      job_title,
                      commute_dist_rank
                    LIMIT 10")

# Execute SQL query
sqldf::sqldf(sql_string)

```

  Notice that in the case of commute distance ties, the `RANK()` function assigns the same rank and then adds the number of ties to that rank to determine the rank for the next highest value of commute distance.
  
  We can also treat this query as a virtual table, and then filter on the derived `commute_dist_rank` field to return the highest commute distance for each job. We can add a `DISTINCT()` function in the `SELECT` clause to collapse jobs for which there are more than one employee with the max commute distance, and display the number of ties for each using the `COUNT(*)` function:
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("SELECT
                        DISTINCT(job_title) AS job_title,
                        COUNT(*) AS employee_count,
                        commute_dist
                     FROM
                        (SELECT
                          demographics.employee_id,
                          job_title,
                          commute_dist,
                          RANK () OVER (PARTITION BY job_title ORDER BY commute_dist DESC) AS commute_dist_rank
                        FROM
                            demographics
                          LEFT JOIN
                            job
                          ON
                            demographics.employee_id = job.employee_id
                          INNER JOIN
                            (SELECT employee_id FROM tenure WHERE org_tenure > 1) ids
                          ON
                            demographics.employee_id = ids.employee_id
                        WHERE
                          dept = 'Research & Development'
                        ORDER BY
                          job_title,
                          commute_dist_rank) tbl
                      WHERE
                        tbl.commute_dist_rank = 1
                      GROUP BY
                        job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```
  
  An alternative approach is to use a **common table expression (CTE)**, which is the result set of a query that exists temporarily and only for use in a larger query. Like the virtual table example, CTEs do not persist data in objects or tables; the data exist only for the duration of the query.
  
```{r, message = FALSE, warning = FALSE}

# Store SQL query as a character string
sql_string <- paste("WITH max_commute_job
                     AS
                        (SELECT
                          demographics.employee_id,
                          job_title,
                          commute_dist,
                          RANK () OVER (PARTITION BY job_title ORDER BY commute_dist DESC) AS commute_dist_rank
                        FROM
                            demographics
                          LEFT JOIN
                            job
                          ON
                            demographics.employee_id = job.employee_id
                          INNER JOIN
                            (SELECT employee_id FROM tenure WHERE org_tenure > 1) ids
                          ON
                            demographics.employee_id = ids.employee_id
                        WHERE
                          dept = 'Research & Development'
                        ORDER BY
                          job_title,
                          commute_dist_rank)
                      
                      SELECT
                        DISTINCT(job_title) AS job_title,
                        COUNT(*) AS employee_count,
                        commute_dist
                     FROM
                        max_commute_job
                      WHERE
                        commute_dist_rank = 1
                      GROUP BY
                        job_title")

# Execute SQL query
sqldf::sqldf(sql_string)

```

## Review Questions

1. What are the differences between data lakes, data warehouses, and data marts?

2. What is the difference between a Type 1 and Type 2 table in a DW?

3. In what ways has modern cloud computing influenced data architecture?

4. What two clauses must always be present in a SQL query?

5. What SQL clause is executed first at run time?

6. To optimize the performance of a SQL query, how should conditions in the WHERE clause be ordered?

7. How do aggregate functions differ from window functions in SQL?

8. What is a subquery?

9. What is the difference between an INNER JOIN and LEFT JOIN?

10. What is the purpose of a common table expression (CTE)?
