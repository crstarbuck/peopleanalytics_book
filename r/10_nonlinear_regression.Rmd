# Non-Linear Regression {#lme}

  Linear regression is a powerful approach to understanding the relative strength of predictors' associations with a response variable. While linear models have advantages in interpretation, inference, and implementation simplicity, the linearity assumption often limits predictive power since this assumption is often a poor approximation of actual relationships in the data. In this chapter, we will discuss how to extend the linear regression framework and relax linear model assumptions to handle non-linear relationships.

## Polynomial Regression
  
  In a people analytics context, many data sets are cross-sectional and time-invariant, meaning they represent data collected at a single point in time. However, data collected across multiple points in time (time series data) are needed for forecasting future values of a dependent variable (e.g., workforce planning model that estimates hires by month).
  
  There is often a seasonality element inherent in the relationship between time and the outcome that is being estimated, which requires accounting for time-variant features (e.g., monthly attrition rate assumptions). **Seasonality** is the variation that occurs at regular intervals within a year. For example, companies with an annual bonus often experience a seasonal spike in voluntary attrition following bonus payouts (beginning in March for many organizations). Accounting for seasonality in models helps reduce error, but it requires estimating a more complex set of model coefficients relative to a more naive linear projection.
  
  The simple linear regression equation, $Y = \beta_0 + \beta_1 X + \epsilon$, can be easily extended to include higher order polynomial terms and achieve a more flexible fit. This is known as **polynomial regression**.
  
  * Quadratic (2nd Order Polynomial) Regression Equation: $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$
  * Cubic (3nd Order Polynomial) Regression Equation: $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon$
  
  Figure \@ref(fig:poly-fun) illustrates how higher-order polynomial functions can fit more curvilinear trends relative to a simple linear projection.
  
```{r poly-fun, out.width = "100%", echo = FALSE, fig.cap = 'Left: Linear turnover trend for $y = .75x + 3.5$. Middle: Quadratic turnover trend for $y = 7.3x - .53x^2 - 6.97$. Right: Cubic turnover trend for $y = -12.48x + 2.47x^2 - .13x^3 + 31.01$.', fig.align = 'center', message = FALSE, warning = FALSE}

# Load library
library(ggplot2)

# Initialize empty data frame
poly_data = NULL
  
# Generate turnover relationships having quadratic and cubic relationships with months
for (i in 1:12){

  poly_data <- rbind(poly_data, cbind.data.frame(
                     month = i, 
                     attrition_lin = .75*i + 3.5,
                     attrition_quad = 7.3*i - .53*i^2 - 6.97,
                     attrition_cube = -12.48*i + 2.47*i^2 - .13*i^3 + 31.01))
}

# Visualize linear trend
p_lin <- ggplot2::ggplot(poly_data, aes(x = month, y = attrition_lin * .01)) + 
         ggplot2::labs(title = 'Linear', x = 'Month', y = 'Turnover Rate') + 
         ggplot2::geom_line() +
         ggplot2::scale_x_continuous(breaks = 1:12) +
         ggplot2::scale_y_continuous(labels = scales::percent) +
         ggplot2::theme_bw() +
         ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Visualize quadratic trend
p_quad <- ggplot2::ggplot(poly_data, aes(x = month, y = attrition_quad * .01)) + 
          ggplot2::labs(title = 'Quadratic', x = 'Month', y = 'Turnover Rate') + 
          ggplot2::geom_line() +
          ggplot2::scale_x_continuous(breaks = 1:12) +
          ggplot2::scale_y_continuous(labels = scales::percent) +
          ggplot2::theme_bw() +
          ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Visualize cubic trend
p_cube <- ggplot2::ggplot(poly_data, aes(x = month, y = attrition_cube * .01)) + 
          ggplot2::labs(title = 'Cubic', x = 'Month', y = 'Turnover Rate') + 
          ggplot2::geom_line() +
          ggplot2::scale_x_continuous(breaks = 1:12) +
          ggplot2::scale_y_continuous(labels = scales::percent) +
          ggplot2::theme_bw() +
          ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Display distribution visualizations
ggpubr::ggarrange(p_lin, p_quad, p_cube, ncol = 3, nrow = 1)

```

  It is important to note that adding higher order terms to the regression equation usually increases $R^2$ due to a more flexible fit to the data, but the additional coefficients are not necessarily significant. $R^2$ will approach 1 as the power of $x$ approaches $n-1$ since the fit line will connect every data point. However, a model that results in a perfect -- or near perfect -- fit is likely too flexible to generalize well to other data. This problem is known as overfitting and will be covered in Chapter \@ref(pred-mod). As a general rule, it is best not to add polynomial terms beyond the second or third orders to protect against overfitting the model.
  
  Comparing the Adjusted $R^2$ for models with higher-order terms to one with only linear terms will help in determining whether higher-order polynomials add value to the model in explaining incremental variance in the response. Evaluating whether the coefficients on higher-order polynomials are statistically significant is important in determining *which variables* are contributing to any observed increases in Adjusted $R^2$.
  
  Let's demonstrate how to fit a regression model with polynomial terms in R using the `turnover_trends` dataset. First, we will subset this data frame to level 4 People Scientists who work remotely, based on the notion that turnover varies by `level` and `remote`, and then visualize the turnover trend to understand month-over-month variation across years.
  
```{r ps-turnover-trends, out.width = "100%", echo = FALSE, fig.cap = 'Year 1-5 turnover trends for level 4 People Scientists, stratified by remote (dark grey line) vs. non-remote (light grey line).', fig.align = 'center', message = FALSE, warning = FALSE}

# Load library
library(dplyr)

# Load employee data
turnover <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/turnover_trends.csv")

# Subset data
ps_turnover <- subset(turnover, job == 'People Scientist' & level == 4)

p_ps_yr1 <- ggplot2::ggplot(data = subset(ps_turnover, year == 1), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 1", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_yr2 <- ggplot2::ggplot(data = subset(ps_turnover, year == 2), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 2", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_yr3 <- ggplot2::ggplot(data = subset(ps_turnover, year == 3), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 3", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_yr4 <- ggplot2::ggplot(data = subset(ps_turnover, year == 4), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 4", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_yr5 <- ggplot2::ggplot(data = subset(ps_turnover, year == 5), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values = c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 5", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Display distribution visualizations
ggpubr::ggarrange(p_ps_yr1, p_ps_yr2, p_ps_yr3, p_ps_yr4, p_ps_yr5, ncol = 3, nrow = 2)

```
  
  As we can see in Figure \@ref(fig:ps-turnover-trends), the relationship between month and turnover rate is non-linear, and level 4 People Scientists who work remotely leave at lower rates relative to those who do not work remotely. There is a clear seasonal pattern that is consistent across all five years as well as remote vs. non-remote groups; namely, there is a spike in turnover between March and June as well as later in the year (November/December). Fitting a model to these data will require non-linear terms.
  
  Adding polynomial terms requires an indicator variable `I()` in which the value of $x$ is raised to the desired order (e.g., $x^2$ = `I(x^2)`). Let's start by fitting linear, quadratic, and cubic regression models (to compare performance) using only `month` as a predictor. Notice that the shape of the trends resemble the cubic function shown in Figure \@ref(fig:poly-fun) in that there are two discernible inflection points at which the trend reverses directions.

```{r, message = FALSE, warning = FALSE}

# Fit linear, quadratic, and cubic models to ps_turnover data
ps.lin.fit <- lm(turnover_rate ~ month, data = ps_turnover)
ps.quad.fit <- lm(turnover_rate ~ month + I(month^2), data = ps_turnover)
ps.cube.fit <- lm(turnover_rate ~ month + I(month^2) + I(month^3), data = ps_turnover)

```

```{r ps-lm-mnth-output, out.width = "100%", echo = FALSE, fig.cap = 'Linear model output for regression of turnover rate onto month.', fig.align = 'center', message = FALSE, warning = FALSE}

# Load library
library(flextable)

# Produce tabular summary of regression model output
flextable::as_flextable(ps.lin.fit)

```  

```{r ps-quad-mnth-output, out.width = "100%", echo = FALSE, fig.cap = 'Quadratic model output for regression of turnover rate onto month.', fig.align = 'center', message = FALSE, warning = FALSE}

# Produce tabular summary of regression model output
flextable::as_flextable(ps.quad.fit)

```

```{r ps-cube-mnth-output, out.width = "100%", echo = FALSE, fig.cap = 'Cubic model output for regression of turnover rate onto month.', fig.align = 'center', message = FALSE, warning = FALSE}

# Produce tabular summary of regression model output
flextable::as_flextable(ps.cube.fit)

```

  The linear ($F(1,118)$ = .71, $p$ = .40) and quadratic ($F(2,117)$ = 1.18, $p$ = .31) models are not significant. However, as expected based on the shape of the turnover trend, the cubic model is significant ($F(3,116)$ = 6.30, $p$ < .001) and the linear (`month`), quadratic (`I(month^2)`), and cubic (`I(month^3)`) terms all provide significant information in estimating turnover rates ($p$ < .001).
  
  While the cubic model achieved statistical significance at the $p$ < .001 level, 86% of the variance in monthly turnover rates remains unexplained (1 - $R^2$ = .86). To improve the performance of the model, our model needs to reflect the fact that turnover varies as a function of `year` and `remote` in addition to `month`.

```{r turnover-pred, out.width = "100%", echo = FALSE, fig.cap = 'Linear, quadratic, and cubic model fit (red dashed line) to remote (dark grey points) and non-remote (light grey points) groups.', fig.align = 'center', message = FALSE, warning = FALSE}

# Apply models to predict people scientist turnover rates for each month in year 1
ps_lin_pred <- data.frame(month = 1:12,
                          turnover_rate = predict(ps.lin.fit, subset(ps_turnover, year == 1, select = c(month, turnover_rate))))
ps_quad_pred <- data.frame(month = 1:12,
                           turnover_rate = predict(ps.quad.fit, subset(ps_turnover, year == 1, select = c(month, turnover_rate))))
ps_cube_pred <- data.frame(month = 1:12,
                           turnover_rate = predict(ps.cube.fit, subset(ps_turnover, year == 1, select = c(month, turnover_rate))))

# Plot data against regression line
p_ps_lin <- ggplot2::ggplot(data = ps_turnover, aes(x = month, y = turnover_rate, color = remote)) + 
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::geom_function(fun = function(x) {ps.lin.fit$coefficients[[2]]*x + ps.lin.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
            ggplot2::scale_color_manual(values = c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Linear", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_quad <- ggplot2::ggplot(data = ps_turnover, aes(x = month, y = turnover_rate, color = remote)) + 
             ggplot2::geom_point() +
             ggplot2::scale_x_continuous(breaks = 1:12) +
             ggplot2::geom_function(fun = function(x) {ps.quad.fit$coefficients[[2]]*x + ps.quad.fit$coefficients[[3]]*x^2 + ps.quad.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
             ggplot2::scale_color_manual(values = c("#B8BDBF", "#595959")) +
             ggplot2::labs(title = "Quadratic", x = "Month", y = "Turnover Rate") +
             ggplot2::theme_bw() +
             ggplot2::theme(legend.position = "none") +
             ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_cube <- ggplot2::ggplot(data = ps_turnover, aes(x = month, y = turnover_rate, color = remote)) + 
             ggplot2::geom_point() +
             ggplot2::scale_x_continuous(breaks = 1:12) +
             ggplot2::geom_function(fun = function(x) {ps.cube.fit$coefficients[[2]]*x + ps.cube.fit$coefficients[[3]]*x^2 + ps.cube.fit$coefficients[[4]]*x^3 + ps.cube.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
             ggplot2::scale_color_manual(values = c("#B8BDBF", "#595959")) +
             ggplot2::labs(title = "Cubic", x = "Month", y = "Turnover Rate") +
             ggplot2::theme_bw() +
             ggplot2::theme(legend.position = "none") +
             ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Display distribution visualizations
ggpubr::ggarrange(p_ps_lin, p_ps_quad, p_ps_cube, ncol = 3, nrow = 1)

```

  As shown in Figure \@ref(fig:turnover-pred), the multidimensional data vary widely around estimates produced by the two-dimensional models (i.e., `turnover_rate` predicted on the basis of `month`). While the cubic regression model reflects the seasonality in month-over-month turnover, there are notable differences between remote and non-remote turnover rates as well as differences across years.

  Let's add `remote` to the cubic regression model to see how performance changes.

```{r ps-cube-mnthrem-output, out.width = "100%", fig.cap = 'Cubic model output for regression of turnover rate onto month and remote.', fig.align = 'center', message = FALSE, warning = FALSE}

# Fit linear, quadratic, and cubic models to ps_turnover df
ps.cube.fit <- lm(turnover_rate ~ month + I(month^2) + I(month^3) + remote, data = ps_turnover)

# Produce tabular summary of regression model output
flextable::as_flextable(ps.cube.fit)

```

  As shown in Figure \@ref(fig:ps-cube-mnthrem-output), accounting for remote status increases explained variance by 21% (.35 - .14). In addition to the change in explained variance $\Delta R^2$, the coefficient on `remote` is statistically significant ($\beta$ = -1.64, $t$(115) = -6.09, $p$ < .001). On average, the turnover rate for remote People Scientists is 1.64% lower than the turnover rate for non-remote People Scientists.

  Next, let's include `year` in the model since turnover rates also vary along this dimension.

```{r ps-cube-yrmnthrem-output1, out.width = "100%", fig.cap = 'Cubic model output for regression of turnover rate onto year, month, and remote.', fig.align = 'center', message = FALSE, warning = FALSE}

# Fit linear, quadratic, and cubic models to ps_turnover df
ps.cube.fit <- lm(turnover_rate ~ year + month + I(month^2) + I(month^3) + remote, data = ps_turnover)

# Produce tabular summary of regression model output
flextable::as_flextable(ps.cube.fit)

```

  Explained variance increases to 62% by adding `year` to the model. While the coefficient on `year` is statistically significant ($\beta$ = .66, $t$(114) = 8.93, $p$ < .001), the change in attrition by year is not linear. Visualizing the distribution of turnover rates by year will provide evidence that a linear year-over-year growth factor will result in some large residuals since it will not capture the more complex trend present in these data.
  
```{r ps-turnover-yrly-dist, out.width = "100%", echo = FALSE, fig.cap = 'Turnover rate distribution by year for remote (left) and non-remote (right) groups. Red dashed line reflects linear relationship between year and turnover rate, with $y$-intercept lowered by 1.64 for remote group.', fig.align = 'center', message = FALSE, warning = FALSE}

# Model linear relationship between year and turnover rate, grouped by remote vs. non-remote
ps.lin.fit <- lm(turnover_rate ~ year + remote, data = ps_turnover)

# Build plots to visualize turnover rate distribution across years, grouped by remote status
p_ps_rem <- ggplot2::ggplot(data = subset(ps_turnover, remote == 'Yes'), aes(x = year, y = turnover_rate, group = year)) + 
            ggplot2::labs(title = "Remote", x = "Year", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::geom_point(color = "#B8BDBF") +
            ggplot2::geom_function(fun = function(x) {ps.lin.fit$coefficients[[2]]*x + ps.lin.fit$coefficients[[3]] + ps.lin.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_nrem <- ggplot2::ggplot(data = subset(ps_turnover, remote == 'No'), aes(x = year, y = turnover_rate, group = year)) + 
             ggplot2::labs(title = "Non-Remote", x = "Year", y = "Turnover Rate") +
             ggplot2::theme_bw() +
             ggplot2::geom_point(color = "#B8BDBF") +
             ggplot2::geom_function(fun = function(x) {ps.lin.fit$coefficients[[2]]*x + ps.lin.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
             ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Display distribution visualizations
ggpubr::ggarrange(p_ps_rem, p_ps_nrem, ncol = 2, nrow = 1)

``` 

  Given the cubic nature of the change in turnover year-over-year, let's add quadratic and cubic terms for `year` to examine changes in model performance:

```{r ps-cube-yrmnthrem2-output, out.width = "100%", echo = FALSE, fig.cap = 'Cubic model output for regression of turnover rate onto year, month, and remote.', fig.align = 'center', message = FALSE, warning = FALSE}

# Fit linear, quadratic, and cubic models to ps_turnover df
ps.cube.fit <- lm(turnover_rate ~ year + I(year^2) + I(year^3) + month + I(month^2) + I(month^3) + remote, data = ps_turnover)

# Produce tabular summary of regression model output
flextable::as_flextable(ps.cube.fit)

```

  The inclusion of higher-order polynomials on `year` result in a perfect fit to these data ($R^2$ = 1). This indicates that the slope of the relationship between `month` and `turnover_rate` is perfectly consistent across years and within remote and non-remote groups.
  
  Our resulting equation for estimating `turnover_rate` on the basis of a combination of linear and non-linear values of `year`, `month`, and `remote` is defined by:
  
  $$ \hat y = -1.87 + 5.91year - 2.71year^2 + .36year^3 + 2.41month - .41month^2 + .02month^3 - 1.64remote + \epsilon $$

  The performance of this model may initially seem like a cause for celebration, but the probability is low that this model would estimate future turnover with such a high degree of accuracy. While these data were generated with a goal to simplify illustrations and facilitate a working knowledge of polynomial regression mechanics, data which conform to such a constant pattern of seasonality across multiple years is a highly improbable situation in practice. As stated earlier in this chapter, a model that results in a perfect fit is likely too flexible to generalize well to other data, and methods of evaluating how well models are likely to perform on future data will be covered in Chapter \@ref(pred-mod).

## Logistic Regression

  **Logistic regression** is a type of **generalized linear model**, which is a family of models for which key linear assumptions are relaxed. Logistic regression is an excellent tool for modeling relationships with outcomes that are not measured on a continuous scale (a key requirement for linear regression). Logistic regression is often leveraged to model the probability of observations belonging to different classes of a categorical outcome, and this type of modeling is known as **classification**. The context for classification can be binomial for two classes (e.g., active/inactive, promoted/not promoted), multinomial for multiple unordered classes (e.g., job family, location), or ordinal for multiple ordered classes (e.g., survey items measured on a Likert scale, performance level, education level). Regardless of the outcome variable's classes, logistic regression is in fact a type of *regression* analysis which uses a link function to generalize the linear model for non-continuous outcomes.

  You may be wondering why linear regression cannot be implemented when the categorical outcome is dummy coded as outlined in Chapter \@ref(data-wrang-prep). In a binary case, in which the categorical response has been coded as 1/0, least squares regression would produce an estimate for $\hat\beta X$ that represents the estimated probability of the outcome coded as 1 given $X$. For example, if attrition is the binary outcome and $Y = 1$ for employees who left and $Y = 0$ for employees who stayed, $\hat Y > .5$ could lend to a termination prediction assuming this is an appropriate probability threshold. Linear regression may produce estimates lower than 0 and higher than 1, however, which complicates the interpretation of estimates as probabilities. 
  
  This issue is not limited to binary categorical outcomes. Response variables with more than 2 categories cannot naturally be converted into quantitative values that are appropriate for linear regression. Instead of modeling the response directly as in linear regression, logistic regression models the probability of an outcome's class given values for one or more predictors. For example, we can leverage our `employees` dataset to model the probability of `active` given a value for `interview_rating`, which would be written as $Pr$(`active` = Yes | `interview_rating`) or simply, $p$(`inteview_rating`). A probability of `active` = Yes will be estimated for a given value of `interview_rating`, and the probability threshold for determining the predicted class needs to be defined based on the business context. If we want to minimize false positives (i.e., incorrectly flagging at-risk employees who do not actually leave), we may set the threshold to something north of .5 (e.g., .7) to gain more confidence that those classified into the termination class are highly likely to exit.

### Binomial Logistic Regression

  Since estimating a binary outcome using linear regression can result in $p(X)$ < 0 for some values of $X$ and $p$($X$) > 1 for others, we need a function that constrains the output to a [0,1] interval. For logistic regression, the *logistic* function is used. This function converts the linear model, $p(X) = \beta_0 + \beta_1 X$, to the following form:
  
  $$ p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1+e^{\beta_0 + \beta_1 X}} $$

  Irrespective of the value of $X$, the logistic function will always produce a sigmoidal (*S-shaped*) curve. 
  
  Taking the ratio of $\frac {p(X)}{1 - p(X)}$ will give the odds of the outcome, which ranges between 0 (very low) and $\infty$ (very high). The logarithm of this ratio, $\log(\frac {p(X)}{1 - p(X)})$, is known as the *log odds* or *logit*, and is fundamental to logistic regression. Log odds is a *monotonic transformation*, meaning the greater the odds, the greater the log of odds (and vice versa).
  
  Recall that in linear regression, the coefficient $\beta$ on a predictor is interpreted as the average change in $Y$ for a one-unit increase in the respective predictor's value. In logistic regression, the interpretation is similar but rather than $\beta$ representing the average change in $Y$, it represents the average unit change in the *log of the odds* for a one-unit increase in the predictor's value.
  
  In R, the `glm()` function is used in conjunction with the `family = binomial` argument to fit a logistic regression model. Recall from Chapter \@ref(inf-stats) that discrete probability distributions can be leveraged to model different types of nominal variables, and that the binomial distribution is appropriate for a sequence of independent observations with only two outcomes -- such as our `active` variable featuring only *yes* and *no* values. Therefore, we need to pass the `family = binomial` argument into the `glm()` function. The formula passed into the function is structured consistent with the `lm()` function used for linear regression: `glm(y ~ x, data)`.
  
```{r glm-rating, out.width = "100%", echo = FALSE, fig.cap = 'Logistic regression of active status onto interview rating.', fig.align = 'center', message = FALSE, warning = FALSE}

# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")

# Dummy code active status to 1/0
employees$active <- ifelse(employees$active == 'Yes', 1, 0)

# Fit a logistic regression model
glm.fit <- glm(active ~ interview_rating, data = employees, family = 'binomial')

# Produce tabular summary for model results using flextable
flextable::as_flextable(glm.fit)

```

  The output shown in Figure \@ref(fig:glm-rating) has some differences relative to the output of a linear regression model. 
  
  * `Estimate`: Average change in the log of odds for each one-unit increase in the value of the predictor
  * `z value`: Ratio of Estimate / Standard Error. Assuming $\alpha$ = .05, a $z$-value of 2 is a good rule of thumb for achieving statistical significance ($p$ < .05) per the properties of the normal distribution.
  * `Null deviance`: Measure of how well the response can be predicted by a model with only an intercept term; the lower the number, the better the fit.
  * `Residual deviance`: Measure of how well the response can be predicted by a model with $p$ predictors; the lower the number, the better the fit. The larger the delta between residual and null deviance, the better the model with $p$ predictors relative to the intercept-only model.
  
  Given the positive coefficient on `interview_rating`, we can interpret this to mean that for each one-unit increase in the average interviewer rating during the onsite stage of the employee's recruiting process, the log of the odds of the employee staying with the organization (`active` status of `Yes` coded as $1$) increases by 21.96.
  
  To illustrate why the logistic function is necessary, let's demonstrate differences in applying linear and logistic regression models by regressing a binary outcome `active` onto `interview_rating`.

```{r lm-glm-compare, out.width = "100%", echo = FALSE, fig.cap = 'Linear (left) and logistic (right) functions applied to models regressing active status (1/0) onto median interviewer rating.', fig.align = 'center', message = FALSE, warning = FALSE}

# Fit a linear model to illustrate why this should not be done with a binary outcome
lm.fit <- lm(active ~ interview_rating, data = employees)

# Construct plots
p_lm <- ggplot2::ggplot(data = employees, aes(x = interview_rating, y = active)) + 
        ggplot2::geom_point(color = "grey") +
        ggplot2::scale_y_continuous(breaks = c(0, .25, .50, .75, 1, 1.25)) +
        ggplot2::geom_function(fun = function(x) {lm.fit$coefficients[[2]]*x + lm.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
        ggplot2::labs(title = "Linear", x = "Interviewer Rating", y = "Active") +
        ggplot2::theme_bw() +
        ggplot2::theme(legend.position = "none") +
        ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_glm <- ggplot2::ggplot(data = employees, aes(x = interview_rating, y = active)) + 
         ggplot2::geom_point(color = "grey") +
         ggplot2::scale_y_continuous(breaks = c(0, .25, .50, .75, 1, 1.25)) +
         ggplot2::stat_smooth(method = "glm", colour = "red", size = .5, linetype = "dashed", se = FALSE, method.args = list(family = binomial)) +
         ggplot2::labs(title = "Logistic", x = "Interviewer Rating", y = "Active") +
         ggplot2::theme_bw() +
         ggplot2::theme(legend.position = "none") +
         ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Visualize linear and logistic functions
ggpubr::ggarrange(p_lm, p_glm, ncol = 2, nrow = 1)

```
  
  Figure \@ref(fig:lm-glm-compare) illustrates that for high values of `interview_rating`, a linear model would estimate probabilities for `active` that are greater than 1. Since probabilities range from 0 (impossible) to 1 (certain), anything outside the [0,1] interval does not make sense. On the other hand, the logistic function produces the *S-shaped* curve described previously. Using the logistic function, the probabilities are constrained to the [0,1] interval, and the visual reflects the fact that `active` can be perfectly predicted for low and high values of `interview_rating` but it is a mixed bag for values in the middle of the range (3.3 - 3.6).
  
  It is often helpful to explain the relationship between a predictor and binary outcome in terms of a percentage increase or decrease. When $\beta = 1$, this indicates that the likelihood of the outcome is identical between the two groups of the predictor. If we exponentiate the coefficients, we can convert the log odds into odds ratios to facilitate a more intuitive interpretation. Therefore, $(exp(\beta) - 1) * 100$ will provide the percentage increase or decrease in the odds of the *included* group relative to the *omitted* group. 
  
  For example, Figure \@ref(fig:glm-ot-lvl) provides the log odds for `active` regressed onto two binary predictors: `overtime` and `job_lvl2plus`.

```{r glm-ot-lvl, out.width = "100%", fig.cap = 'Logistic regression of active status onto overtime and job level 2+.', fig.align = 'center', message = FALSE, warning = FALSE}

# Create dummy-coded variable for job level 2+
employees$job_lvl2plus <- ifelse(employees$job_lvl > 1, 1, 0)

# Fit a logistic regression model
glm.fit <- glm(active ~ overtime + job_lvl2plus, data = employees, family = 'binomial')

# Produce tabular summary for model results using flextable
flextable::as_flextable(glm.fit)

```

  We can convert these coefficients into odds ratios by exponentiating the coefficients:
  
```{r exp-coef, out.width = "100%", fig.cap = 'Exponentiated logistic regression coefficients', fig.align = 'center', message = FALSE, warning = FALSE}

# Return exponentiated coefficients
exp(coef(glm.fit))

```

  The exponentiated coefficient on `overtime` is $exp(\beta) = .25$, so there is a $(1 - .25) * 100 = 75%$ percent *decrease* in the odds of being active for employees who work overtime (since `overtime = Yes` is the included group) relative to those who do not work overtime. The exponentiated coefficient on `job_lvl2plus` is $exp(\beta) = 3.41$, so there is a $(3.41 - 1) * 100 = 241%$ percent *increase* in the odds of being active for those with a job level of 2 or greater relative to those with a job level of 1 (i.e., attrition is a larger concern for level 1 employees).

  Generalized linear mixed models can be fitted using the `glmer()` function from the same `lme4` library used for multilevel linear models in Chapter \@ref(lm). The syntax is identical with the exception of needing to define `family` as an additional argument. 
  
  A juxtaposition of `glm()` and `glmer()` fits for logistic regression is shown in the following block of code. The mixed model example features an additional random (group-level) effect on `business_travel` via `1 | business_travel`, and fixed (observation-level) effects on remaining predictors which are consistent with the standard logistic regression model:
  
```{r, message = FALSE, warning = FALSE}

# Load library
library(lme4)

# Logistic model
glm(active ~ overtime + job_lvl2plus, data = employees, family = 'binomial')

# Logistic mixed model
lme4::glmer(active ~ overtime + job_lvl2plus + (1 | business_travel), data = employees, family = 'binomial')

```
  
### Multinomial Logistic Regression

  **Multinomial logistic regression** is used to estimate the probability of an unordered categorical response with $K>2$ classes. With an understanding of binomial logistic regression, extending the binomial model to a multinomial logistic regression model should be relatively intuitive.
  
  To extend the binomial model to a multinomial context, we need to first identify a reference level. This decision may be arbitrary or guided by the research question or hypothesis, but the decision is nonetheless important as it impacts how the model coefficients are interpreted -- always relative to the reference level. With the reference level defined, we can then express the multinomial logistic regression model as:
  
  $$ Pr(Y = k | X = x) = \frac{e^{\beta_{k0} + \beta_{k1} x_1 + ... + \beta_{kp} x_p}}{1+ \displaystyle\sum_{j=1}^{K-1} e^{\beta_{j0} + \beta_{j1} x_1 + ... + \beta_{jp} x_p}}, $$
  
  where $K$ is the reference class, $j$ is a $K-1$ non-reference level, and $k$ is the specified class for which the probability is being estimated on the basis of values for one or more $X$ predictors. 
  
  Consider `dept` from our `employees` data, which has values of `Research & Development`, `Sales`, and `Human Resources`. This is a nominal variable because differences in these levels are not ordered in the same way job levels ranging from 1 to 10 or Likert scales ranging from 1 to 5 are. Employees in the Sales department may be greater in number relative to employees in the Human Resources department, for example, but it would not be appropriate to assign to the Sales department a numeric value that indicates it is *higher* or *better* relative to the Human Resources department.
  
  Multinomial models are essentially a collection of binomial models which compare the log-odds of each non-reference category to the specified reference category. If the Human Resources department is identified as the reference category $K$, then $\beta_{k0}$ for $k = Sales$ can be interpreted as the log odds of Sales department membership relative to Human Resources department membership in the following equation:
  
  $$ log(\frac{Pr(Y = k | X = x)} {Pr(Y = K | X = x)}) = \beta_{k0} + \beta_{k1}x_1 + ... + \beta_{xp}x_p $$
  
  Depending on the research objective, it may be appropriate to compute the odds of one category relative to all other categories. This can actually be accomplished using binomial regression if the category of interest is coded as 1 and all other categories are coded as 0. In this case, the reference for the binomial model is a *collection* of $K-1$ categories. If understanding the odds of *each category* relative to a reference category is more appropriate based on the research objective, multinomial logistic regression is the proper model.
  
  Let's illustrate how to implement multinomial logistic regression by determining how variables in the `employees` dataset help in classifying employees into departments. To build this model in R, we will use the `multinom` function from the `nnet` package. It is important that the nominal response variable is defined as a factor before implementing multinomial logistic regression, so we will first convert the data type of `dept` from its native character type to a factor. We also need to identify the reference department against which the probability of each of the other departments will be evaluated; we will define this using the `ref` argument within the `relevel()` function:

```{r multi-nom-mdl, out.width = "100%", fig.cap = 'Multinomial logistic regression of department onto overtime and business travel.', fig.align = 'center', message = FALSE, warning = FALSE}

# Load library
library(nnet)

# Convert dept to factor
employees$dept <- factor(employees$dept)

# Specify reference level
employees$dept <- relevel(employees$dept, ref = "Human Resources")

# Fit multinomial logistic regression model
# An omitted group for categorical variables is defined by default
multinom.fit <- multinom(dept ~ overtime + ed_field, data = employees)

# Summarize results from model object
summary(multinom.fit)

```

  Notice the output from the `multinom()` function is quite limited relative to `glm()` and `lm()`. Coefficients and standard errors are provided, but $p$-values are not available in the output so we will need to calculate them separately.
  
  A new statistical measure is included in the output of this model, **Akaike Information Criterion (AIC)**, which is a score that is helpful for model selection. AIC is calculated by:
  
  $$ AIC = -2\frac{\ell}{n} + 2\frac{k}{n}, $$
  where $n$ is the number of observations, $k$ is the number of parameters (predictors + intercept), and $\ell$ is the log likelihood function where:
  
  $$ \ell = -\frac{n}{2}(1 + ln(2\pi) + ln(\frac{1}{n} \displaystyle\sum_{i=1}^{n}(y_i - \hat{y}_i)^2))  $$
  
  Just as we compared $R^2$ across linear regression models in Chapter \@ref(lm), AIC can be compared across models to determine which one is a better fit to the data; lower AIC values indicate better fit. Consistent with how the Adjusted $R^2$ statistic adjusts for variables that do not provide information, AIC penalizes models that use more parameters. Therefore, if two models explain the same amount of variance in the response, the model with fewer parameters will feature a lower AIC score. 
  
  At the time of this writing, the `as_flextable()` function used to format the output of `glm()` and `lm()` is not compatible with objects in which `multinom()` results are stored. Therefore, we will construct a basic data frame that improves readability.
  
  To determine whether the coefficients are statistically significant, we need to perform an additional step to compute $p$-values using $z$ scores:
  
```{r multi-nom-p-vals, out.width = "100%", fig.cap = '$p$-values for multinomial logistic regression coefficients.', fig.align = 'center', message = FALSE, warning = FALSE}

# Calculate z-scores
z_scores <- summary(multinom.fit)$coefficients / summary(multinom.fit)$standard.errors

# Produce p-values
p_values <- (1 - pnorm(abs(z_scores))) * 2

# Transpose and display rounded p-values 
data.frame(t(round(p_values, 3)))

```
  
  These $p$-values indicate that those who work overtime are not significantly more likely to work in either the Research & Development or Sales departments relative to the Human Resources department. In other words, a considerable portion of employees in all three departments work overtime, so this variable is not helpful in classifying employees into their correct departments. This is evident in Figure \@ref(fig:ot-dept).

```{r ot-dept, echo = FALSE, out.width = "100%", fig.cap = 'Department distribution by overtime status.', fig.align = 'center', message = FALSE, warning = FALSE}

# Visualize distribution of educational fields by department
ggplot2::ggplot(employees, aes(x = overtime, fill = dept)) +
ggplot2::labs(x = 'Overtime', y = 'Count') +
ggplot2::guides(fill = guide_legend(title = "Department")) +
ggplot2::geom_bar() +
ggplot2::theme_bw()

```  
  
  The $p$-values indicate that educational background is a strong predictor of department. This is evidenced in Figure \@ref(fig:ed-field-dept), which shows that each educational field is generally dominated by a single department. This means that we can achieve strong departmental *purity* on the basis of `ed_field` alone. All employees who studied Marketing work in Sales and all employees who studied HR work in said department. However, those who work in R&D have a variety of educational backgrounds, so the signal is not as clear for these cases and additional variables would be needed to more accurately assign these employees to the correct departments.
  
```{r ed-field-dept, echo = FALSE, out.width = "100%", fig.cap = 'Department distribution by educational field.', fig.align = 'center', message = FALSE, warning = FALSE}

# Visualize distribution of educational fields by department
ggplot2::ggplot(employees, aes(x = ifelse(ed_field == 'Human Resources', 'HR', ed_field), fill = dept)) +
ggplot2::labs(x = 'Education Field', y = 'Count') +
ggplot2::guides(fill = guide_legend(title = "Department")) +
ggplot2::geom_bar() +
ggplot2::theme_bw()

```  

  As we did for binomial logistic regression, we can compute the exponential of model coefficients for the multinomial logistic regression model to convert the log-odds to more intuitive odds ratios:

```{r, message = FALSE}

# Return exponentiated coefficients from model object
# Transpose rows to columns for improved readability
data.frame(t(exp(coef(multinom.fit))))

```

  Consistent with our approach for binomial logistic regression, we can interpret these exponentiated coefficients in terms of having greater or lesser odds. Importantly, in the multinomial context the odds are *relative to the reference category* -- the Human Resources department in this case.

  For example, the odds ratio associated with a Medical educational field is $exp(\beta) = 4.43$ for Research & Development and $exp(\beta) = 1.74$ for Sales. Therefore, those with a Medical education have $(4.43 - 1) * 100 = 343%$ greater odds of being in the Research & Development department and $(1.74 - 1) * 100 = 74%$ greater odds of being in the Sales department relative to the Human Resources department. We can ignore the odds ratios associated with `overtime` since this variable does not provide significant information. 

### Ordinal Logistic Regression

  Many projects in people analytics involve understanding how variables influence ordinal outcomes, such as performance ratings or survey items measured on a Likert scale. Stepwise changes in the levels of ordinal outcomes may or may not be consistent. For example, it may be easy for one to be promoted from job level 1 to 2 but relatively difficult to progress from 5 to 6. Linear regression should not be used in these settings, as linear assumptions are designed for data measured on a continuous scale and will not hold for ordinal data. This section will cover **ordinal logistic regression**, which is a modeling technique designed for understanding how variables influence stepwise changes in a multi-class ordinal outcome.

  Beyond the universal data screening procedures we have covered, such as ensuring problematic collinearity is not present, ordinal logistic regression features a unique **proportional odds assumption** that must be satisfied. This assumption, also known as the **parallel regression assumption**, requires that each independent variable has an equal effect at each level of the ordinal outcome. If the effect varies across levels of the outcome, separate models are needed to accurately reflect the associations with each pair of levels.
  
  Though other approaches exist for modeling ordinal outcomes, the proportional-odds model based on cumulative distribution probabilities is the most common. Its intercepts are dependent on the $j$ levels but slopes are equal as defined by:
  
  $$ log(\frac{Pr(Y \le j)} {Pr(Y > j)}) = \beta_{j0} - \displaystyle\sum_{i=1}^{n} \beta_{1i} x_{1i} + ... + \beta_{pi} x_{pi} $$
  
  The implementation of ordinal logistic regression will be demonstrated by evaluating the statistical drivers of engagement. First, we need to define `engagement` as an ordered factor:
  
```{r, message = FALSE}

# Define ordered factor
employees$engagement <- ordered(employees$engagement, levels = c(1, 2, 3, 4, 5))

# Verify structure of engagement variable
str(employees$engagement)

```

  Next, the proportional odds assumption will be checked using the **Brant test**. The Brant test is a set of comparisons of the separate binary logistic models underlying the overall model (Brant, 1990). This test evaluates whether $\beta_{j1}$ through $\beta_{jp}$ are consistent across each of the $j$ levels. This is done via a $\chi^2$ test to compare coefficients and determine whether observed differences are larger departures from what we would expect by chance. The null hypothesis states that coefficients are not statistically different across the $j$ levels; therefore, $p < .05$ indicates that significant differences in effects are present across the $j$ levels and, thus, the proportional odds assumption is violated.
  
  We can leverage the `brant` package in R for this, which is compatible with the `polr()` function from the `MASS` package that will be used to perform ordinal logistic regression. Since we will be evaluating model statistics, rather than merely using the model for prediction (the subject of Chapter \@ref(pred-mod)), we need to specify the `Hess = TRUE` argument in the `polr()` function to include the *Hessian matrix* (observed information matrix) in the output.
  
```{r, warning = FALSE, message = FALSE}

# Load library
library(MASS)
library(brant)

# Fit a ordinal logistic regression model
ord.fit <- polr(engagement ~ org_tenure, data = employees, Hess = TRUE)

# Test proportional odds assumption using Brant's test
brant(ord.fit)

```

  Notice the line in the output for the omnibus test, which shows an identical $\chi^2$, $df$, and $p$-value to the line associated with `org_tenure`. **Omnibus tests** are statistical tests which test for the significance of several parameters in a model at once. For example, a one-way ANOVA evaluating differences in mean commute time across three locations is an omnibus test since it has more than two parameters. As we covered in Chapter \@ref(aod), the null hypothesis is rejected in the context of ANOVA if there is at least *one* difference in complex contrasts -- even if the mean commute time is not significantly different between *all* groups. Test statistics are identical for this ordinal logistic regression model because there is a single predictor, but Brant's omnibus test investigates equality of coefficients for all predictors jointly in the case of more than two parameters (Martin, 2022). The null and alternative hypotheses for Brant's omnibus test are:
  
  $H_0$: The odds are proportional for all predictors in the model.
  $H_A$: The odds are non-proportional for at least one predictor.
  
  The results of Brant's test indicate that we fail to reject $H_0$ since $p = .82$ for the omnibus test, so the proportional odds assumption holds for these data. If the model featured more than one predictor, we could also evaluate the statistics on individual predictors -- but only to determine for which predictor(s) the proportional odds assumption is violated *if* $p < .05$ for the omnibus test. As the number of variables and tests increases, so too does our risk of incorrectly rejecting the proportional odds assumption; therefore, decisions regarding the proportional odds assumption should not be based on statistics for individual predictors alone. Even if the odds are truly proportional for each predictor independently, with 20 predictors we would expect to find one by chance for which $p < .05$ since our tolerance for a Type I error is 1 in 20 with $\alpha = .05$.
  
  Since the proportional odds assumption holds, let's review the model output:
  
```{r, warning = FALSE, message = FALSE}

# Summarize ordinal logistic regression model
summary(ord.fit)

```

  The output provides the average effect of a one-unit increase in `org_tenure` (Coefficients section) as well as intercepts on each pair of levels for `engagement` (Intercepts section). The effect of a one-unit increase in `org_tenure` in moving `engagement` from one ordinal level to the next is quite small ($\beta = -.007$). The intercepts are often referred to as cutpoints and can be roughly translated as thresholds. While the intercepts for each cutpoint vary, note that the single coefficient on `org_tenure` is only possible because our proportional odds assumption holds and the effect is consistent (proportional) across levels of our ordered factor, `engagement`. 
  
  Consistent with the default output from the `multinom()` function used for multinomial logistic regression, $p$-values are not provided in the standard output from the `polr()` function. However, we can calculate them for reasonably large samples by comparing the $t$-values against the standard normal distribution:
  
```{r ord-p-vals, out.width = "100%", fig.cap = '$p$-values for ordinal logistic regression coefficients.', fig.align = 'center', message = FALSE, warning = FALSE}

# Store coefficients to df
coef_df <- coef(summary(ord.fit))

# Produce p-values
p <- pnorm(abs(coef_df[, "t value"]), lower.tail = FALSE) * 2

# Combine p values with coefficients df
coef_df <- cbind(coef_df, "p value" = p)

# Display df contents
coef_df

```

  We can now estimate the likelihood of a particular observation having a specified level of $Y$, such as $Y \le 3$, as follows:
  
  $$ log(\frac{Pr(Y \le 3)} {Pr(Y > 3)}) =  2.17 - .007 x_{orgtenure} $$

## Review Questions

1. In what ways does polynomial regression differ from linear regression?

2. If the coefficient on the cubic term is not statistically significant ($p$ >= .05) in a cubic regression model, but the linear and quadratic terms are statistically significant ($p$ < .05), what does this indicate about the model's fit to the data?

3. Why is it helpful to calculate the exponential of log-odds in a logistic regression model?

4. What does an odds ratio of 1.25 indicate in a binomial context?

5. What does an odds ratio of 0.75 indicate in a multinomial context?

6. How does Akaike Information Criterion (AIC) compare to $R^2$ with respect to its purpose and function?

7. In what type of R object do ordinal data need to be stored in order to implement ordinal logistic regression?

8. Why can't linear regression be used to understand associations of predictors with an ordinal outcome?

9. What does the proportional odds (or parallel regression) assumption assume about model coefficients?

10. Why is it important to evaluate Brant's omnibus test -- over the test statistics on independent predictors alone -- when determining whether the proportional odds assumption for ordinal logistic regression holds?