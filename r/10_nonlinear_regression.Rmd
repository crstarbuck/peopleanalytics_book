# Non-Linear Regression {#lme}

  Linear regression is a powerful approach to understanding the relative strength of predictors' associations with a response variable. While linear models have advantages in interpretation, inference, and implementation simplicity, the linearity assumption often limits predictive power since this assumption is often a poor approximation of actual relationships in the data. In this chapter, we will discuss how to extend the linear regression framework and relax linear model assumptions to respect the non-linear nature of relationships.

## Polynomial Regression
  
  In a people analytics context, many data sets are cross-sectional and time-invariant, meaning they represent data collected at a single point in time. However, data collected across multiple points in time (time series data) are needed for forecasting future values of a dependent variable (e.g., workforce planning model that estimates hires by month).
  
  There is often a seasonality element inherent in the relationship between time and the outcome that is being estimated, which requires accounting for time-variant features (e.g., monthly attrition rate assumptions). **Seasonality** is the variation that occurs at regular intervals within a year. For example, companies with an annual bonus often experience a seasonal spike in voluntary attrition following bonus payouts (beginning in March for many organizations). Accounting for seasonality in models helps reduce error, but it requires estimating a more complex set of model coefficients relative to a more naive linear projection.
  
  The simple linear regression equation, $Y = \beta_0 + \beta_1 X + \epsilon$, can be easily extended to include higher order polynomial terms and achieve a more flexible fit. This is known as **polynomial regression**.
  
  * Quadratic (2nd Order Polynomial) Regression Equation: $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$
  * Cubic (3nd Order Polynomial) Regression Equation: $Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon$
  
  Figure \@ref(fig:poly-fun) illustrates how higher-order polynomial functions can fit more curvilinear trends relative to a simple linear projection.
  
```{r poly-fun, out.width = "100%", echo = FALSE, fig.cap = 'Left: Linear turnover trend for $y = .75x + 3.5$. Middle: Quadratic turnover trend for $y = 7.3x - .53x^2 - 6.97$. Right: Cubic turnover trend for $y = -12.48x + 2.47x^2 - .13x^3 + 31.01$.', fig.align = 'center', message = FALSE, warning = FALSE}

# Load library
library(ggplot2)

# Initialize empty data frame
poly_data = NULL
  
# Generate turnover relationships having quadratic and cubic relationships with months
for (i in 1:12){

  poly_data <- rbind(poly_data, cbind.data.frame(
                     month = i, 
                     attrition_lin = .75*i + 3.5,
                     attrition_quad = 7.3*i - .53*i^2 - 6.97,
                     attrition_cube = -12.48*i + 2.47*i^2 - .13*i^3 + 31.01))
}

# Visualize linear trend
p_lin <- ggplot2::ggplot(poly_data, aes(x = month, y = attrition_lin * .01)) + 
         ggplot2::labs(title = 'Linear', x = 'Month', y = 'Turnover Rate') + 
         ggplot2::geom_line() +
         ggplot2::scale_x_continuous(breaks = 1:12) +
         ggplot2::scale_y_continuous(labels = scales::percent) +
         ggplot2::theme_bw() +
         ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Visualize quadratic trend
p_quad <- ggplot2::ggplot(poly_data, aes(x = month, y = attrition_quad * .01)) + 
          ggplot2::labs(title = 'Quadratic', x = 'Month', y = 'Turnover Rate') + 
          ggplot2::geom_line() +
          ggplot2::scale_x_continuous(breaks = 1:12) +
          ggplot2::scale_y_continuous(labels = scales::percent) +
          ggplot2::theme_bw() +
          ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Visualize cubic trend
p_cube <- ggplot2::ggplot(poly_data, aes(x = month, y = attrition_cube * .01)) + 
          ggplot2::labs(title = 'Cubic', x = 'Month', y = 'Turnover Rate') + 
          ggplot2::geom_line() +
          ggplot2::scale_x_continuous(breaks = 1:12) +
          ggplot2::scale_y_continuous(labels = scales::percent) +
          ggplot2::theme_bw() +
          ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Display distribution visualizations
ggpubr::ggarrange(p_lin, p_quad, p_cube, ncol = 3, nrow = 1)

```

  It is important to note that adding higher order terms to the regression equation usually increases $R^2$ due to a more flexible fit to the data, but the additional coefficients are not necessarily significant. $R^2$ will approach 1 as the power of $x$ approaches $n-1$ since the fit line will connect every data point. However, a model that results in a perfect -- or near perfect -- fit is likely too flexible to generalize well to other data. This problem is known as overfitting and will be covered in Chapter \@ref(pred-mod). As a general rule, it is best not to add polynomial terms beyond the second or third orders to protect against overfitting the model.
  
  Comparing the Adjusted $R^2$ for models with higher-order terms to one with only linear terms will help in determining whether higher-order polynomials add value to the model in explaining incremental variance in the response. Evaluating whether the coefficients on higher-order polynomials are statistically significant is important in determining *which variables* are contributing to any observed increases in Adjusted $R^2$.
  
  Let's demonstrate how to fit a regression model with polynomial terms in R using the `turnover_trends` dataset. First, we will subset this data frame to level 4 People Scientists who work remotely, based on the notion that turnover varies by `level` and `remote`, and then visualize the turnover trend to understand month-over-month variation across years.
  
```{r ps-turnover-trends, out.width = "100%", echo = FALSE, fig.cap = 'Year 1-5 turnover trends for level 4 People Scientists, stratified by remote (dark grey line) vs. non-remote (light grey line).', fig.align = 'center', message = FALSE, warning = FALSE}

# Load library
library(dplyr)

# Load employee data
turnover <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/turnover_trends.csv")

# Subset data
ps_turnover <- subset(turnover, job == 'People Scientist' & level == 4)

p_ps_yr1 <- ggplot2::ggplot(data = subset(ps_turnover, year == 1), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 1", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_yr2 <- ggplot2::ggplot(data = subset(ps_turnover, year == 2), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 2", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_yr3 <- ggplot2::ggplot(data = subset(ps_turnover, year == 3), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 3", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_yr4 <- ggplot2::ggplot(data = subset(ps_turnover, year == 4), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 4", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_yr5 <- ggplot2::ggplot(data = subset(ps_turnover, year == 5), aes(x = month, y = turnover_rate, colour = remote)) + 
            ggplot2::geom_line() +
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::scale_y_continuous(breaks = 1:10) +
            ggplot2::scale_color_manual(values = c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Year 5", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Display distribution visualizations
ggpubr::ggarrange(p_ps_yr1, p_ps_yr2, p_ps_yr3, p_ps_yr4, p_ps_yr5, ncol = 3, nrow = 2)

```
  
  As we can see in Figure \@ref(fig:ps-turnover-trends), the relationship between month and turnover rate is non-linear, and level 4 People Scientists who work remotely leave at lower rates relative to those who do not work remotely. There is a clear seasonal pattern that is consistent across all five years as well as remote vs. non-remote groups; namely, there is a spike in turnover between March and June as well as later in the year (November/December). Fitting a model to these data will require non-linear terms.
  
  Adding polynomial terms requires an indicator variable `I()` in which the value of $x$ is raised to the desired order (e.g., $x^2$ = `I(x^2)`). Let's start by fitting linear, quadratic, and cubic regression models (to compare performance) using only `month` as a predictor. Notice that the shape of the trends resemble the cubic function shown in Figure \@ref(fig:poly-fun) in that there are two discernible inflection points at which the trend reverses directions.

```{r, message = FALSE, warning = FALSE}

# Fit linear, quadratic, and cubic models to ps_turnover data
ps.lin.fit <- lm(turnover_rate ~ month, data = ps_turnover)
ps.quad.fit <- lm(turnover_rate ~ month + I(month^2), data = ps_turnover)
ps.cube.fit <- lm(turnover_rate ~ month + I(month^2) + I(month^3), data = ps_turnover)

```

```{r ps-lm-mnth-output, out.width = "100%", echo = FALSE, fig.cap = 'Linear model output for regression of turnover rate onto month.', fig.align = 'center'}

# Load library
library(flextable)

# Produce tabular summary of regression model output
flextable::as_flextable(ps.lin.fit)

```  

```{r ps-quad-mnth-output, out.width = "100%", echo = FALSE, fig.cap = 'Quadratic model output for regression of turnover rate onto month.', fig.align = 'center'}

# Produce tabular summary of regression model output
flextable::as_flextable(ps.quad.fit)

```

```{r ps-cube-mnth-output, out.width = "100%", echo = FALSE, fig.cap = 'Cubic model output for regression of turnover rate onto month.', fig.align = 'center'}

# Produce tabular summary of regression model output
flextable::as_flextable(ps.cube.fit)

```

  The linear ($F(1,118)$ = .71, $p$ = .40) and quadratic ($F(2,117)$ = 1.18, $p$ = .31) models are not significant. However, as expected based on the shape of the turnover trend, the cubic model is significant ($F(3,116)$ = 6.30, $p$ < .001) and the linear (`month`), quadratic (`I(month^2)`), and cubic (`I(month^3)`) terms all provide significant information in estimating turnover rates ($p$ < .001).
  
  While the cubic model achieved statistical significance at the $p$ < .001 level, 86% of the variance in monthly turnover rates remains unexplained (1 - $R^2$ = .86). To improve the performance of the model, our model needs to reflect the fact that turnover varies as a function of `year` and `remote` in addition to `month`.

```{r turnover-pred, out.width = "100%", echo = FALSE, fig.cap = 'Linear, quadratic, and cubic model fit (red dashed line) to remote (dark grey points) and non-remote (light grey points) groups.', fig.align = 'center', message = FALSE, warning = FALSE}

# Apply models to predict people scientist turnover rates for each month in year 1
ps_lin_pred <- data.frame(month = 1:12,
                          turnover_rate = predict(ps.lin.fit, subset(ps_turnover, year == 1, select = c(month, turnover_rate))))
ps_quad_pred <- data.frame(month = 1:12,
                           turnover_rate = predict(ps.quad.fit, subset(ps_turnover, year == 1, select = c(month, turnover_rate))))
ps_cube_pred <- data.frame(month = 1:12,
                           turnover_rate = predict(ps.cube.fit, subset(ps_turnover, year == 1, select = c(month, turnover_rate))))

# Plot data against regression line
p_ps_lin <- ggplot2::ggplot(data = ps_turnover, aes(x = month, y = turnover_rate, color = remote)) + 
            ggplot2::geom_point() +
            ggplot2::scale_x_continuous(breaks = 1:12) +
            ggplot2::geom_function(fun = function(x) {ps.lin.fit$coefficients[[2]]*x + ps.lin.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
            ggplot2::scale_color_manual(values = c("#B8BDBF", "#595959")) +
            ggplot2::labs(title = "Linear", x = "Month", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::theme(legend.position = "none") +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_quad <- ggplot2::ggplot(data = ps_turnover, aes(x = month, y = turnover_rate, color = remote)) + 
             ggplot2::geom_point() +
             ggplot2::scale_x_continuous(breaks = 1:12) +
             ggplot2::geom_function(fun = function(x) {ps.quad.fit$coefficients[[2]]*x + ps.quad.fit$coefficients[[3]]*x^2 + ps.quad.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
             ggplot2::scale_color_manual(values = c("#B8BDBF", "#595959")) +
             ggplot2::labs(title = "Quadratic", x = "Month", y = "Turnover Rate") +
             ggplot2::theme_bw() +
             ggplot2::theme(legend.position = "none") +
             ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_cube <- ggplot2::ggplot(data = ps_turnover, aes(x = month, y = turnover_rate, color = remote)) + 
             ggplot2::geom_point() +
             ggplot2::scale_x_continuous(breaks = 1:12) +
             ggplot2::geom_function(fun = function(x) {ps.cube.fit$coefficients[[2]]*x + ps.cube.fit$coefficients[[3]]*x^2 + ps.cube.fit$coefficients[[4]]*x^3 + ps.cube.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
             ggplot2::scale_color_manual(values = c("#B8BDBF", "#595959")) +
             ggplot2::labs(title = "Cubic", x = "Month", y = "Turnover Rate") +
             ggplot2::theme_bw() +
             ggplot2::theme(legend.position = "none") +
             ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Display distribution visualizations
ggpubr::ggarrange(p_ps_lin, p_ps_quad, p_ps_cube, ncol = 3, nrow = 1)

```

  As shown in Figure \@ref(fig:turnover-pred), the multidimensional data vary widely around estimates produced by the two-dimensional models (i.e., `turnover_rate` predicted on the basis of `month`). While the cubic regression model reflects the seasonality in month-over-month turnover, there are notable differences between remote and non-remote turnover rates as well as differences across years.

  Let's add `remote` to the cubic regression model to see how performance changes.

```{r ps-cube-mnthrem-output, out.width = "100%", echo = FALSE, fig.cap = 'Cubic model output for regression of turnover rate onto month and remote.', fig.align = 'center'}

# Fit linear, quadratic, and cubic models to ps_turnover df
ps.cube.fit <- lm(turnover_rate ~ month + I(month^2) + I(month^3) + remote, data = ps_turnover)

# Produce tabular summary of regression model output
flextable::as_flextable(ps.cube.fit)

```

  As shown in Figure \@ref(fig:ps-cube-mnthrem-output), accounting for remote status increases explained variance by 21% (.35 - .14). In addition to the change in explained variance $\Delta R^2$, the coefficient on `remote` is statistically significant ($\beta$ = -1.64, $t$(115) = -6.09, $p$ < .001). On average, the turnover rate for remote People Scientists is 1.64% lower than the turnover rate for non-remote People Scientists.

  Next, let's include `year` in the model since turnover rates also vary along this dimension.

```{r ps-cube-yrmnthrem-output1, out.width = "100%", echo = FALSE, fig.cap = 'Cubic model output for regression of turnover rate onto year, month, and remote.', fig.align = 'center'}

# Fit linear, quadratic, and cubic models to ps_turnover df
ps.cube.fit <- lm(turnover_rate ~ year + month + I(month^2) + I(month^3) + remote, data = ps_turnover)

# Produce tabular summary of regression model output
flextable::as_flextable(ps.cube.fit)

```

  Explained variance increases to 62% by adding `year` to the model. While the coefficient on `year` is statistically significant ($\beta$ = .66, $t$(114) = 8.93, $p$ < .001), the change in attrition by year is not linear. Visualizing the distribution of turnover rates by year will provide evidence that a linear year-over-year growth factor will result in some large residuals since it will not capture the more complex trend present in these data.
  
```{r ps-turnover_yrly_dist, out.width = "100%", echo = FALSE, fig.cap = 'Turnover rate distribution by year for remote (left) and non-remote (right) groups. Red dashed line reflects linear relationship between year and turnover rate, with $y$-intercept lowered by 1.64 for remote group.', fig.align = 'center'}

# Model linear relationship between year and turnover rate, grouped by remote vs. non-remote
ps.lin.fit <- lm(turnover_rate ~ year + remote, data = ps_turnover)

# Build plots to visualize turnover rate distribution across years, grouped by remote status
p_ps_rem <- ggplot2::ggplot(data = subset(ps_turnover, remote == 'Yes'), aes(x = year, y = turnover_rate, group = year)) + 
            ggplot2::labs(title = "Remote", x = "Year", y = "Turnover Rate") +
            ggplot2::theme_bw() +
            ggplot2::geom_point(color = "#B8BDBF") +
            ggplot2::geom_function(fun = function(x) {ps.lin.fit$coefficients[[2]]*x + ps.lin.fit$coefficients[[3]] + ps.lin.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
            ggplot2::scale_color_manual(values=c("#B8BDBF", "#595959")) +
            ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_ps_nrem <- ggplot2::ggplot(data = subset(ps_turnover, remote == 'No'), aes(x = year, y = turnover_rate, group = year)) + 
             ggplot2::labs(title = "Non-Remote", x = "Year", y = "Turnover Rate") +
             ggplot2::theme_bw() +
             ggplot2::geom_point(color = "#B8BDBF") +
             ggplot2::geom_function(fun = function(x) {ps.lin.fit$coefficients[[2]]*x + ps.lin.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
             ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Display distribution visualizations
ggpubr::ggarrange(p_ps_rem, p_ps_nrem, ncol = 2, nrow = 1)

``` 

  Given the cubic nature of the change in turnover year-over-year, let's add quadratic and cubic terms for `year` to examine changes in model performance:

```{r ps-cube-yrmnthrem2-output, out.width = "100%", echo = FALSE, fig.cap = 'Cubic model output for regression of turnover rate onto year, month, and remote.', fig.align = 'center'}

# Fit linear, quadratic, and cubic models to ps_turnover df
ps.cube.fit <- lm(turnover_rate ~ year + I(year^2) + I(year^3) + month + I(month^2) + I(month^3) + remote, data = ps_turnover)

# Produce tabular summary of regression model output
flextable::as_flextable(ps.cube.fit)

```

  The inclusion of higher-order polynomials on `year` result in a perfect fit to these data ($R^2$ = 1). This indicates that the slope of the relationship between `month` and `turnover_rate` is perfectly consistent across years and within remote and non-remote groups.
  
  Our resulting equation for estimating `turnover_rate` on the basis of a combination of linear and non-linear values of `year`, `month`, and `remote` is defined by:
  
  $$ \hat y = -1.87 + 5.91year - 2.71year^2 + .36year^3 + 2.41month - .41month^2 + .02month^3 - 1.64remote + \epsilon $$

  The performance of this model may initially seem like a cause for celebration, but the probability is low that this model would estimate future turnover with such a high degree of accuracy. While these data were generated with a goal to simplify illustrations and facilitate a working knowledge of polynomial regression mechanics, data which conform to such a constant pattern of seasonality across multiple years is a highly improbable situation in practice. As stated earlier in this chapter, a model that results in a perfect fit is likely too flexible to generalize well to other data, and methods of evaluating how well models are likely to perform on future data will be covered in Chapter \@ref(pred-mod).

## Logistic Regression

  **Logistic regression** is an excellent tool for modeling the probability of different classes of a categorical outcome -- a type of modeling often referred to as **classification**. The context for classification can be binomial for two classes (e.g., active/inactive, promoted/not promoted), multinomial for multiple unordered classes (e.g., job family, location), or ordinal for multiple ordered classes (e.g., survey items measured on a Likert scale, performance level, education level).

  You may be wondering if linear regression can be implemented when the categorical outcome is dummy coded as outlined in Chapter \@ref(data-wrang-prep). 
  
  In a binary case, in which the categorical response has been coded as 1/0, least squares regression would produce an estimate for $\hat\beta X$ that represents the estimated probability of the outcome coded as 1 given $X$. For example, if attrition is the binary outcome and $Y = 1$ for employees who left and $Y = 0$ for employees who stayed, $\hat Y$ > .5 would be indicative of a termination prediction. Linear regression may produce estimates lower than 0 and higher than 1, however, which complicates the interpretation of estimates as probabilities. Response variables with more than 2 categories cannot naturally be converted into quantitative values that are appropriate for linear regression. 
  
  Instead of modeling the response directly as in linear regression, logistic regression models the probability of an outcome's class given values for one or more predictors. For example, we can leverage our `employees` dataset to model the probability of `active` given a value for `engagement`, which would be written as $Pr$(`active` = Yes | `engagement`) or simply, $p$(`engagement`). A probability of `active` = Yes will be estimated for a given value of `engagement`, and the probability threshold for determining the predicted class needs to be defined based on the business context. If we want to minimize false positives (i.e., flag at-risk employees who do not actually leave), we may set the threshold to something north of .5 (e.g., .7) to gain more confidence that those classified into the termination class are highly likely to exit.

### Binomial Logistic Regression

  Since estimating a binary outcome using linear regression can result in $p(X)$ < 0 for some values of $X$ and $p$($X$) > 1 for others, we need a function that contrains the output to a [0,1] interval. For logistic regression, the *logistic* function is used. This function converts the linear model, $p(X) = \beta_0 + \beta_1 X$, to the following form:
  
  $$ p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1+e^{\beta_0 + \beta_1 X}} $$

  Irrespective of the value of $X$, the logistic function will always produce an *S-shaped* curve. 
  
```{r lm-glm-compare, out.width = "100%", echo = FALSE, fig.cap = 'Linear (left) and logistic (right) functions applied to models regressing active status (1/0) onto YTD sales for salespeople.', fig.align = 'center'}

# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")

# Subset employees data frame; leads are only applicable for those in sales positions
data <- subset(employees, job_title %in% c('Sales Executive', 'Sales Representative'))

# Dummy code active status to 1/0
data$active <- ifelse(data$active == 'Yes', 1, 0)

# Regress active on engagement using lm() and glm() functions
lm.fit <- lm(ytd_sales ~ engagement, data = data)
glm.fit <- glm(active ~ engagement, data = data, family = 'binomial')

# Construct plots
p_lm <- ggplot2::ggplot(data = data, aes(x = ytd_sales, y = active)) + 
        ggplot2::geom_point() +
        ggplot2::geom_function(fun = function(x) {lm.fit$coefficients[[2]]*x + lm.fit$coefficients[[1]]}, colour = "red", linetype = "dashed") +
        ggplot2::labs(title = "Linear", x = "YTD Sales", y = "Active") +
        ggplot2::theme_bw() +
        ggplot2::theme(legend.position = "none") +
        ggplot2::theme(plot.title = element_text(hjust = 0.5))

p_glm <- ggplot2::ggplot(data = data, aes(x = ytd_sales, y = active)) + 
         ggplot2::geom_point() +
         ggplot2::stat_smooth(method = "glm", colour = "red", size = .5, linetype = "dashed", se = FALSE, method.args = list(family = binomial)) +
         ggplot2::labs(title = "Logistic", x = "YTD Sales", y = "Active") +
         ggplot2::theme_bw() +
         ggplot2::theme(legend.position = "none") +
         ggplot2::theme(plot.title = element_text(hjust = 0.5))

# Visualize linear and logistic functions
ggpubr::ggarrange(p_lm, p_glm, ncol = 2, nrow = 1)

```

### Multinomial Logistic Regression



### Ordinal Logistic Regression



### Proportional Odds Logistic Regression



## Poisson Regression



## Review Questions

