# Unsupervised Learning {#unsup-lrn}

  The inferential and predictive models covered thus far can be categorized as **supervised learning** models. For each observation $x_i$ in the data, there is an associated response $y_i$ in a supervised learning setting, and the goal is to fit a model that relates $y$ to one or more predictors to understand relationships or predict future values on the basis of the identified associations. However, in an **unsupervised learning** setting, no response $y_i$ is associated with $x_i$. As a result, we cannot *supervise* the analysis and are limited to understanding how observations cluster or group together based on patterns across the available $p$ attributes.

  People analytics often involves the unique challenge of analyzing high-dimensional data with a large number of $p$ attributes but relatively few $n$ observations -- a phenomenon often referred to as the *curse of dimensionality*. Given the sample size requirements covered in previous chapters, we ideally want $n$ to be an order of magnitude larger than $p$ to support statistical power and increase our chances of detecting meaningful patterns and population effects in sample data. Since people data sets are often wide and short, **dimension reduction** is important for reducing the dimensions to a limited subset that captures the majority of the information and optimizes the $n:p$ ratio.
  
  Consider a case in which a colleague uses verbose rhetoric to convey a simple message that could be effectively communicated with fewer words. The superfluous language is unnecessary and does not provide additional information or value. This is analogous to dimension reduction in that we are interested in identifying a limited set of meaningful attributes and discarding redundant and unimportant information that does not contribute to the analysis objectives.
  
  Dimension reduction is most common in the context of surveys. Survey instrumentation with strong psychometric properties features highly correlated survey items for constructs, or individual dimensions of multidimensional constructs, that are relatively uncorrelated with survey items used to measure independent constructs. Intuitively, we know that highly correlated variables do not individually capture unique information, as one is a sufficient proxy to capture the available signal in the data. Dimension reduction is particularly important in survey research because longer surveys are costly and may result in lower response rates due to the increased completion time requirements.
  
  This chapter will cover dimension reduction fundamentals as well as technical implementations.

## Factor Analysis

  The development of survey instrumentation, whether a single item or a larger multidimensional scale, begins with a good theory. The theory provides the conceptual support for the construct -- the particular dimensions that characterize the construct, the antecedent variables which theoretically influence it, and the outcomes it will likely drive. With a strong theoretical framework, the researcher can begin proposing ways of *operationalizing* the conceptual scheme into a measurement approach.
  
  There are clear measurement approaches for business metrics such as leads generated, new business growth, cNPS, and net revenue. However, in the social sciences we often need indicators of latent constructs that are difficult -- or impossible -- to directly measure. If we want to understand the extent to which employees are engaged in their work, we need a comprehensive measure that captures facets of the theoretical frame. For example, vigor, absorption, and dedication are dimensions of Schaufeli, Bakker, and Salanova's (2006) conception of work engagement which were operationalized in the Utrecht Work Engagement Scale (UWES). 
  
  Quantifying the energy levels one brings to work (vigor), the extent to which one feels time passes quickly while working (absorption), and the level of one's commitment to seeing tasks through to completion (dedication) is challenging since we cannot leverage transactional data or digital exhaust to directly quantify this as we can with operational business metrics. We need a comprehensive -- yet parsimonious -- set of survey items that function as indicators of the dimensions of the latent work engagement construct.   

  **Exploratory Factor Analysis (EFA)**
  
  **Exploratory factor analysis (EFA)** is a variable reduction technique by which factors are extracted from data as part of the development process for new survey instruments. 
  
  A researcher may work with a panel of experts in a particular domain to develop an inventory of items that tap various aspects of the construct per the theoretical framework which underpins it. Based on how the items cluster together, the empirical data will be reconciled against the theoretical conception to define dimensions of the measure. Within a cluster of highly correlated items for a particular dimension, the researcher needs to decide which items are essential and which are redundant and candidates for removal. Outside these principal clusters (or factors), remaining items also need to be evaluated for their relevance and support for the underlying theory. If items are believed to be members of the theoretical dimensions but do not cluster together with other similar items, it may be indicative of poorly written items that have different interpretations among survey takers. EFA is the empirical process that supports these objectives.
  
  
  
```{r, message = FALSE, warning = FALSE}

# Load library
library(dplyr)

# Load survey response data
survey_dat <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/survey_responses.csv")

# Show dimensions of survey data
dim(survey_dat)

```
  
  **Confirmatory Factor Analysis (CFA)**
  
  **Confirmatory factor analysis (CFA)** is used to test how well data aligns with a theoretical factor structure.
  
  We expect items associated with a given construct to be highly correlated with one another but relatively uncorrelated with items associated with independent constructs. Consider engagement and retention, which are two independent -- yet likely correlated -- constructs. If multiple items are need to measure the theoretical dimensions of both engagement and retention, we would expect the engagement items to be more highly correlated with one another than with the retention items. Theory may suggest that retention likelihood increases as engagement increases, but there are many other factors which drive one's decision to leave an organization beyond engagement, so we would not expect levels of engagement to *always* be associated with a commensurate change in retention.

  We can illustrate this using `survey_responses` data, which contains three items for each engagement and retention. Let's evaluate pairwise relationships between the items: 

```{r ggpairs-eng-ret, out.width = "100%", echo = FALSE, message = FALSE, fig.cap = 'Bivariate correlations and data distributions for engagement and retention survey items.', fig.align = 'center'}

# Load library
library(GGally)

# Visualize correlation matrix
GGally::ggpairs(subset(survey_dat, select = c("eng_1", "eng_2", "eng_3", "ret_1", "ret_2", "ret_3")))

```
  As expected, `eng_1`, `eng_2`, and `eng_3` have stronger correlations with each other ($r \ge .64$) than with `ret_1`, `ret_2`, or `ret_3` ($r \le .47$).
  
  CFA enables us to move beyond inter-item correlations to quantify the extent to which latent variables in our data fit an expected theoretical model. We can leverage the `lavaan` package in R to perform CFA. 
  
  Step one is defining the model within a string per the syntax required by `lavaan`:

```{r, message = FALSE, warning = FALSE}

# Load library
library(lavaan)

# Model specification; each line represents a separate latent factor
model <- paste('engagement =~ eng_1 + eng_2 + eng_3
                retention =~ ret_1 + ret_2 + ret_3')

```

  Step two is fitting the model to the data using the `cfa()` function from the `lavaan` package:  

```{r, message = FALSE, warning = FALSE}

# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat)

```

  We can also create what is known as a **path diagram** to assist with understanding the CFA model. A path diagram is a symbolic visualization of the measurement model, with circles depicting latent variables (factors), rectangles representing observed indicators (survey items), and arrows indicating paths (relationships) between variables. The measurement model (CFA) together with the structural (path) model is known as **structural equation modeling (SEM)**; therefore, CFA is a subset of the SEM umbrella.
  
  The `lavaanPlot()` package can be used to create and visualize path diagrams in R:

```{r path-diagram, out.width = "100%", echo = FALSE, message = FALSE, fig.cap = 'Path diagram showing survey items as indicators of latent engagement and retention factors.', fig.align = 'center'}

# Load library
library(lavaanPlot)

# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)

```

  Step three is interpreting the output of the fitted model:

```{r, message = FALSE, warning = FALSE}

# Summarize the model
summary(cfa.fit, fit.measures = TRUE)

```




## Principal Components Analysis (PCA)


  
## Clustering


  
  **$\textbf k$**-Means Clustering**
  
  
  
  **Hierarchical Clustering**



## Review Questions

1. 

2. 

3. 

4. 

5. 

6. 

7. 

8. 

9. 

10. 