# Getting Started

## Guiding Principles

  <b> Pro Employee Thinking </b>

  "With Great Power Comes Great Responsibility."

  ‘Pro employee’ thinking is addressed first and for good reason. People analytics has the power to improve the lives of people in meaningful ways. Whether we are shedding light on an area of the business struggling with work-life balance or identifying developmental areas of which a group of leaders may be unaware, people analytics ideally improves employee well-being and effectively, the success of the business. It is important to embrace a ‘pro employee’ philosophy, as newfound knowledge could also have damaging repercussions if shared with the wrong people or if findings are disseminated without proper instruction on how to interpret and take action (e.g., disparate impact).

  One way to error on the side of caution when considering whether or not to disseminate insights is to ask the following: “With this knowledge, could the recipient act in a manner that is inconsistent with our ‘pro employee’ philosophy?” If the answer to this question is not a clear “no”, discuss with your HR, legal, and privacy partners and together, determine how best to proceed. The decision may be to not share the findings with the intended audience at all or to develop a proper communication and training plan to ensure there is consistency in how recipients interpret the insights and take action in response.

  <b> Quality </b> 

  "Garbage In, Garbage Out."

  Never compromise quality for greater velocity. It is unlikely that requestors of data and analytics will ask us to take longer to prepare the information. The onus is on us as analytics professionals to level set on a reasonable timeline for analyses based on many factors that can materially impact the quality of analyses and insights. All it takes is one instance of compromised quality to damage your reputation and cause consumers of your insights to view all findings as suspect. Be sure quality is atop your list of core values, and guard your team’s reputation at all costs. If users do not trust the insights provided, they will question what they receive which may in turn result in requests for additional reports to ‘tick-and-tie’ in order to gain confidence in the data. This is wasteful to both you and your user community.

  To be clear, by ‘quality’ I am referring to results, which is dependent on data integrity in the source systems, proper data preparation steps, and many other factors. The majority of the analyst’s time is spent on data preparation (data collection, cleaning and organizing, building training sets, mining for patterns, refining algorithms, etc.). If tight controls do not exist within the source application to support data integrity, data preparation efforts can only go so far in delivering reliable and valid findings. It is often the analysts who identify data integrity issues due to the nature of their work; therefore, close relationships should be formed with source application owners to put into place validation rules to proactively prevent the entry of erroneous data or at the very least, exception/audit reports to identify and address the issues soon after the fact. Close relationships with application owners can also facilitate application changes that will help reduce laborious data preparation steps. For example, if the source application collects information on employees’ education via free-form text entries, it may make sense to discuss populating a selection list of schools to free analysts from having to scrub “U”, “University”, “Univ.”, etc. to produce a clean, unique list. These enhancements can save you significant amounts of time down the road.

  While the allure may emerge to curtail important data preparation steps or make incorrect assumptions about the quality of data in the source and jump into modeling prematurely, resist the urge to take shortcuts. Ensure experienced analytics professionals are involved in the initial development of a roadmap so that decision makers who may not be as familiar with the technical minutia are better informed when creating timelines. If leaders broadcast deliverables that are not realistic, it will likely result in dangerous levels of pressure being applied to those doing the analysis which will increase the likelihood of shortcut exploitation to hit milestones. Be methodical in your approach and ensure you are progressing commensurate with a coherent and practical analytics roadmap. If quality falls to the bottom of the priority list, all other efforts are futile.

  <b> Prioritization </b> 
  
  "If everything is a priority, nothing is a priority."
  
  There will always be a supply-demand gap for analytics functions, which is okay as long as the unmet demand is largely requests for low-impact analyses. The maximize impact, it is crucial to be relentless about prioritizing strategically important projects with 'measurable' impact over merely interesting questions. According to the Pareto Principle, 80% of outcomes (or outputs) result from 20% of causes (or inputs). In analytics, it is important to be laser focused on identifying the 20% of inputs that will result in disproportionate value creation for stakeholders. There are some general customer-oriented questions I have found to be helpful for the intake process to optimize the allocation of time and resources:
  
  1. Does this support a company or departmental objective? If not, why should this be prioritized over something else?
  2. Who is the executive sponsor? If this is really important, there will be an executive-level sponsor.
  3. What quantitative and/or qualitative data can be provided as a rationale for this request? Is there data to support doing this, or is the problem statement rooted merely in anecdotes?
  4. Will this mitigate risk or enable opportunities?
  5. What actions can or will be taken as a result of this analysis?
  6. What is the scale of impact (# of impacted people)?
  7. What is the depth of impact (minimum --> significant)?
  8. Is this a dependency or blocker for another important deliverable?
  9. What is the impact of not doing (or delaying) this?
  10. What is the request date? Is there flexibility in this date and/or scope of the request (e.g., what does MVP look like)?
  
  These questions can be weighted and scored as well to support a more automated and data-driven approach to prioritization.
  
## Tools

  This book uses freely available software for statistics, modeling, and data visualization.

  <b> R </b>
  
  While many commercial-grade analytics toolsets are very costly, R is an open-source statistical software package that can be downloaded free of charge. It is incredibly powerful, and there is a package for just about any statistical technique you wish to utilize. It is also widely used in highly regulated environments. As of this writing, R Markdown -- the dynamic document creator in which I am writing this book -- allows for coding in 56 different languages! Therefore, the debate around whether to use Python, Julia, or something else is now moot; we need not sacrifice the advantages of other languages by choosing one. To get started, simply download the latest version of R and the R Studio IDE using the following links.

  R: https://www.r-project.org/
  <br />
  R Studio IDE: https://www.rstudio.com/products/rstudio/download/#download
  
  Please note that while R basics are covered, this is not a book on how to code. It is assumed that you already have an understanding of programming fundamentals. If this is not the case, an introductory programming course is highly recommended; this is one of the best investments you can make for a successful career in analytics. The ability to write code is now table stakes for anyone in an analytics-oriented field, as this is the best way to develop reproducible analyses.
  
  Libraries from several R packages will be utilized in this book. The line of code below can be executed within R to install all at once:
  
```{r, message = FALSE}

# Install required packages
install.packages(c("tidyverse", "corrplot"), dependencies = TRUE, repos = "http://cran.us.r-project.org")

```
  
  The goal of the code provided in this book is not to represent the most performant, succinct, or productionalizable approaches. The code herein is intended only to facilitate understanding and demonstrate how concepts can be implemented in people analytics settings. Programming expertise is important for optimizing these approaches for production applications.

  <b> Google Data Studio </b>

  Like statistical software, there are many options for BI tools that provide interactive dashboards and robust data visualization. In fact, R has robust data visualization capabilities via packages like ggplot and Shiny -- which can be extended with CSS themes, html widgets, and JavaScript actions. In people analytics, dashboards are rarely (if ever) published publicly; we need interactive web-based dashboards with strong authentication and row-level security that have connectors to popular databases such as BigQuery, RedShift, and PostgreSQL to facilitate frequent and automated data refreshes. There are several free options that meet these criteria, and one which tends to be highly underrated is Google's Data Studio. While it is not as feature-rich as popular solutions like Tableau and Power BI, Google is continuously improving the product and the key features needed to be successful in data visualization and dashboarding are already available. It is also very intuitive and easy to use relative to building apps in a tool such as R Shiny. For this reason, Data Studio has been selected as the companion tool to R for this book.
  
  If your organization is a Google shop, Data Studio will integrate nicely with your productivity tools (e.g., sourcing data from Sheets, sharing with those in your Active Directory). If not, it is simple to register company emails as Google accounts to enable sharing and activate row-level security. All you need is a free Google account to get started.

  Data Studio: https://datastudio.google.com/

## 4D Framework

  Adherence to a lightweight framework over hastily rushing into an analysis full of assumptions generally lends to better outcomes. A framework ensures (a) the problem statement is understood and well-defined; (b) prior research and analyses are reviewed; (c) the measurement strategy is sound; (d) the analysis approach is suitable for the hypotheses being tested; and (e) results and conclusions are valid and communicated in a way that resonates with the target audience. This chapter will outline a recommended framework as well as other important considerations that should be reviewed early in the project.

  It is important to develop a clear understanding of the key elements of research. Scientific research is the systematic, controlled, empirical, and critical investigation of natural phenomena guided by theory and hypotheses about the presumed relations among such phenomena (Kerlinger & Lee, 2000). In other words, research is an organized and systematic way of finding answers to questions. If you are in the business of analytics, I encourage you to think of yourself as a scientist -- regardless of plans to publish outside your organization.

  As we will discover when exploring the laws of probability in a later chapter, there is a 1 in 20 chance of finding a significant result when none actually exists. Therefore, it is important to remain disciplined and methodical to protect against backward research wherein the researcher mines data for interesting relationships or differences and then develops hypotheses which they know the data support. There have been many examples of bad research over the years, which often presents in the form of p-hacking or data dredging -- the act of finding data to confirm what the researcher wants to prove. This can occur by running an exhaustive number of experiments in an effort to find one that supports the hypothesis, or by using only a subset of data which features the expected patterning.

  Academics at elite research institutions are often under immense pressure to publish in top-tier journals which have a track record of accepting new ground-breaking research over replication studies or unsupported hypotheses, and incentives have unfortunately influenced some to compromise integrity. As my PhD advisor told me many years ago, an unsupported hypothesis -- while initially disappointing given the exhaustive literature review that precedes its development -- is actually a meaningful empirical contribution given theory suggests the opposite should be true.

  If you participated in a science fair as a child, you are likely already familiar with the scientific method. The scientific method is the standard scheme of organized and systematic inquiry, and this duly applies to people analytics practitioners striving to promote the robustness of analyses and recommendations.

<i>
```{r, echo = FALSE, fig.cap = 'The Scientific Method'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/scientific_method.png")

```
</i>

<br />

  Over the years, I have adapted the scientific method into a curtailed four-dimensional framework which is intended to elevate the rigor applied to the end-to-end analytical process. The four dimensions are (a) Discover, (b) Design, (c) Develop, and (d) Deliver, and this book will be organized around these. A checklist with general questions and considerations across the analytics lifecycle can be found in the Appendix.

<i>
```{r, echo = FALSE, fig.cap = '4D Framework'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/4d_framework_overview.png")

```
</i>

<br />

<b>1. Discover</b>

  You are likely familiar with the old adage: "An ounce of prevention is worth a pound of cure." Such is the case with respect to planning in an analytics context. During the Discover phase, it is important to remain in the problem zone; seek to understand your clients' needs through active listening and questions. This is not the time for solutioning or committing to any specific deliverables. If the client's needs are ambiguous, proceeding will likely be an exercise in futility. Outlined below is a set of general questions that should be considered during this initial phase to prevent allocating scarce time and resourcing to a project that ultimately misses the mark.

  
<br />
  
<b>2. Design</b>

  Perhaps the most important initial question to answer in the design phase is: "Does anything already exist that addresses part, or all, of the client’s objectives?" If the existing solution will suffice, it's possible that there is simply a communication/education gap, and you can allocate time and resources elsewhere.
  
  The end-user experience is of paramount importance during the Design phase, as solutions should have a consistent look and feel regardless of who developed the product. To achieve this, it is important to resist siloed thinking and consider the broader set of analytics solutions the team has delivered -- or is in the process of delivering.


<br />

<b>3. Develop</b>



<br />

<b>4. Deliver</b>

  The Deliver phase can take many forms depending on the solution being released. If the solution is designed for a large user base, a series of recorded trainings may be in order so that there is a helpful reference for those unable to attend the live sessions or new joiners in the future. It is important to monitor success measures, which could be insights aligned to research hypotheses, dashboard utilization metrics, or any number of others defined within the Discover phase.
