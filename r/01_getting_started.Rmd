# Getting Started {#getting-started}

  **People analytics** is the evidence-based practice of surfacing actionable insights from data to help people and organizations thrive. Relative to other functions such as Finance and Marketing, data-informed decisions in the talent domain is a recent concept. People analytics has significant potential to transform organizations by surfacing subtle barriers to success and optimizing for both the employee experience and shareholder value.
  
  The increased focus on the future of work, employee well-being, candidate and employee experience, worker productivity and collaboration, diversity, equity, inclusion, and belonging (DEIB), and retention of critical talent has resulted in companies making growing investments in people analytics capabilities. The nature of workforce challenges is increasingly too complex and nuanced for legacy HR skills, as critical decisions related to new operating models (e.g., remote/hybrid work) and strategies to address unprecedented phenomena (e.g., The Great Resignation) require robust analytics and insights.
  
  Many organizations struggle to progress beyond basic operational reporting and dashboards, but reports and dashboards are not people analytics; they merely help inform what questions to ask based on unanticipated observations (e.g., surprising changes and trajectories) which are often the impetus for people analytics projects. The ability to think critically and reason through the available data to properly frame problems and theoretical explanations are perhaps the most essential skills for success in people analytics.

  Analyses should always have a strong value proposition -- a clear expectation of how an analysis will support a General Manager, People Partner, Salesperson, or other member of the organization. Nothing in this book will increase the value of an analysis no one needs. There should be a clear business need and commitment to action before implementing an analysis. While curiosity is important, it is not a justification for an analysis. This chapter will cover a set of guiding principles as well as a project management framework that will help ensure analytics are anchored in well-defined problem statements to increase the likelihood of meaningful ROI.
  
  In addition, this book will cut through the fluff and teach you how to do stuff. Knowledge of concepts is futile without an understanding of how to apply them to people analytics use cases. The goal of this book is not to boil the ocean by implementing an exhaustive set of analysis methods in every tool. This book is guided by a goal of optimizing for the *fewest* number of concepts and applications required to successfully design and execute the *majority* of projects in the people analytics domain. While this is a technical book, it indexes more heavily on concepts and practical applications to people analytics than on mathematical underpinnings.
  
  Whether you are a people leader, individual contributor, or aspiring analytics practitioner, this book is for you. This book will serve as a guide through the analytics lifecycle, curating the key concepts and applications germane to common questions and hypotheses within people analytics and providing a repeatable framework for successful analytics projects.

## Guiding Principles

  Among the many principles guiding how analytics teams operate, there are three that I have found to be universally applicable and critical to the success of an analytics capability.

### Pro-Employee Thinking

  With great power comes great responsibility.

  ‘Pro-employee’ thinking is addressed first and for good reason. People analytics has the power to improve the lives of people in meaningful ways. Whether we are shedding light on an area of the business struggling with work-life balance or identifying developmental areas of which a group of leaders may be unaware, people analytics ideally improves employee well-being and effectively, the success of the business. It is important to embrace a pro-employee philosophy, as newfound knowledge could also have damaging repercussions if shared with the wrong people or if findings are disseminated without proper instruction on how to interpret and act.

  One way to error on the side of caution when considering whether to disseminate insights is to ask the following: “With this knowledge, could the recipient act in a manner that is inconsistent with our pro-employee philosophy?” If the answer to this question is not a clear “no”, discuss with your HR, legal, and privacy partners and together, determine how best to proceed. The decision may be to not share the findings with the intended audience at all or to develop a proper communication and training plan to ensure there is consistency in how recipients interpret the insights and act in response. Employment Law and Data Privacy Counsel are our friends, and it is important to build strong relationships with these critical partners.

### Quality

  Garbage in, garbage out.

  Never compromise quality for greater velocity. If quality falls to the bottom of the priority list, all other efforts are pointless. It is unlikely that requestors of data and analytics will ever ask us to take longer to prepare the information. The onus is on us as analytics professionals to level set on a reasonable timeline for analyses based on many factors that can materially impact the quality of analyses and insights. A single instance of compromised quality can have lasting damage on the reputation of the analytics function and cause consumers of insights to view all findings as suspect. Be sure quality is consistently a top value and guard your team’s reputation at all costs. If stakeholders lose trust, there will likely be additional data requests for validation; this is wasteful to both you and your user community and detracts from the bigger story that needs to be conveyed.

  To be clear, by ‘quality’ I am referring to results, which is dependent on data integrity in the source systems, proper data preparation steps, and many other factors. Most of the analyst’s time is spent on data preparation (data collection, cleaning and organizing, building training sets, mining for patterns, refining algorithms, etc.). If tight controls do not exist within the source application to support data integrity, data preparation efforts can only go so far in delivering reliable and valid findings. It is often the analysts who identify data integrity issues due to the nature of their work; therefore, close relationships should be formed with source application owners to put into place validation rules to proactively prevent the entry of erroneous data or at the very least, exception/audit reports to identify and address the issues soon after the fact.

### Prioritization
  
  If everything is a priority, nothing is a priority.
  
  If there is not a supply-demand gap on the analytics team, the team likely isn't asking enough questions. A backlog of projects can lend support for business cases requesting incremental funding to accelerate and expand the impact of people analytics. It is crucial to be relentless about prioritizing strategically important projects with 'measurable' impact over merely interesting questions that few care to answer. According to the Pareto Principle, 80% of outcomes (or outputs) result from 20% of causes (or inputs). In analytics, it is important to be laser focused on identifying the 20% of inputs that will result in disproportionate value creation for stakeholders. There are some general customer-oriented questions I have found to be helpful for the intake process to optimize the allocation of time and resources:
  
  1. Does this support a company or departmental objective? If not, why should this be prioritized over something else?
  2. Who is the executive sponsor? Really important projects will have an executive-level sponsor.
  3. What quantitative and/or qualitative data can be provided as a rationale for this request? Is there data to support doing this, or is the problem statement rooted merely in theories and anecdotes?
  4. Will this mitigate risk or enable opportunities?
  5. What actions can or will be taken as a result of this analysis?
  6. What is the *scale* of impact (# of impacted people)?
  7. What is the *depth* of impact (minimum --> significant)?
  8. Is this a dependency or blocker for another important deliverable?
  9. What is the impact of not doing (or delaying) this?
  10. What is the request date? Is there flexibility in this date and/or scope of the request (e.g., what does MVP look like)?
  
  These questions can be weighted and scored as well to support a more automated and algorithmic approach to prioritization.
  
## Tooling

  Applications in this book are demonstrated in R, which is open-sourced statistical and data visualization software that can be downloaded free of charge. It is incredibly powerful, and there is a package (or at least the ability to easily create one) for every conceivable statistical technique and data visualization. R is also widely used in highly regulated environments (e.g., clinical trials).
  
  As of this writing, R Markdown -- the dynamic document creator in which this book is written -- allows for coding in 56 different languages! Therefore, debating whether to use Python, Julia, or something else is unproductive; we need not sacrifice the advantages of other languages by choosing one. All code has been written such that it is fully reproducible should you choose to follow along on your machine (and I highly recommend you do). 
  
  Please note that while R basics are covered, this is not a book on how to code. An introductory programming course is highly recommended, and this is one of the best investments you can make for a successful career in analytics. The ability to write code is now table stakes for anyone in an analytics-oriented field, as this is the best way to develop reproducible analyses. Coding is to analytics professionals what typing was for Baby Boomers decades ago; a lack of coding proficiency is a major limiting factor on one's potential in this field.
  
  The goal of the code provided in this book is not to represent the most performant, succinct, or productionalizable approaches. The code herein is intended only to facilitate understanding and demonstrate how concepts can be implemented in people analytics settings. The most performant approaches are often at odds with more intuitive alternatives. Programming expertise is important for optimizing these approaches for production applications.
  
## Data

### Employees

  The primary data set used in this book is `employees`, which contains information on active and terminated employees. Fields are defined in the data dictionary provided below:
  
  * `employee_id`: Unique identifier for each employee
  * `active`: Flag set to *Yes* for active employees and *No* for inactive employees
  * `stock_opt_lvl`: Stock option level
  * `trainings`: Number of trainings completed within the past year
  * `age`: Employee age in years
  * `commute_dist`: Commute distance in miles
  * `ed_lvl`: Education level, where 1 = *High School*, 2 = *Associate Degree*, 3 = *Bachelor's Degree*, 4 = *Master's Degree*, and 5 = *Doctoral Degree*
  * `ed_field`: Education field associated with most recent degree
  * `gender`: Gender self-identification
  * `marital_sts`: Marital status
  * `dept`: Department of which an employee is a member
  * `engagement`: Employee engagement score measured on a 4-point Likert scale, where 1 = *Highly Disengaged* and 4 = *Highly Engaged*
  * `job_lvl`: Job level, where 1 = *Junior* and 5 = *Senior*
  * `job_title`: Job title
  * `overtime`: Flag set to *Yes* if the employee is nonexempt and works overtime and *No* if the employee does not work overtime
  * `business_travel`: Business travel frequency
  * `hourly_rate`: Hourly rate calculated irrespective of hourly/salaried employees
  * `daily_comp`: Hourly rate * 8
  * `monthly_comp`: Hourly rate * 2080 / 12
  * `annual_comp`:  Hourly rate * 2080
  * `ytd_leads`: Year-to-date (YTD) number of leads generated for employees in Sales Executive and Sales Representative positions
  * `ytd_sales`: Year-to-date (YTD) sales measured in USD for employees in Sales Executive and Sales Representative positions
  * `standard_hrs`: Expected working hours over a two-week payroll cycle
  * `salary_hike_pct`: The percent increase in salary for the employee's most recent compensation adjustment (whether due to a standard merit increase, off-cycle adjustment, or promotion)
  * `perf_rating`: Most recent performance rating, where 1 = *Needs Improvement*, 2 = *Core Contributor*, 3 = *Noteworthy*, and 4 = *Exceptional*
  * `prior_emplr_cnt`: Number of prior employers
  * `env_sat`: Environment satisfaction score measured on a 4-point Likert scale, where 1 = *Highly Dissatisfied* and 4 = *Highly Satisfied* 
  * `job_sat`: Job satisfaction score measured on a 4-point Likert scale, where 1 = *Highly Dissatisfied* and 4 = *Highly Satisfied* 
  * `rel_sat`: Colleague relationship satisfaction score measured on a 4-point Likert scale, where 1 = *Highly Dissatisfied* and 4 = *Highly Satisfied* 
  * `wl_balance`: Work-life balance score measured on a 4-point Likert scale, where 1 = *Poor Balance* and 4 = *Excellent Balance* 
  * `work_exp`: Total years of work experience
  * `org_tenure`: Years at current company
  * `job_tenure`: Years in current job
  * `last_promo`: Years since last promotion
  * `mgr_tenure`: Years under current manager
  * `interview_rating`: Average rating across the interview loop for the onsite stage of the employee's recruiting process, where 1 = *Definitely Not* and 5 = *Definitely Yes*

  Most of these fields have also been broken into separate topical data sets to support data wrangling examples in Chapter \@ref(sql-intro): `benefits`, `demographics`, `job`, `payroll`, `performance`, `prior_employment`, `status`, `survey_response`, and `tenure`.

### Turnover Trends

  The `turnover_trends` data set contains trailing 12-month turnover rates for each month across a five-year period. Fields are defined in the data dictionary provided below:
  
  * `year`: Integer representing the year, which ranges from 1 (earliest) to 5 (most recent)
  * `month`: Integer representing the month, which ranges from 1 (January) to 12 (December)
  * `job`: Job title
  * `level`: Job level, where 1 = *Junior* and 5 = *Senior*
  * `remote`: Flag set to *Yes* for a remote worker and *No* for a non-remote worker
  * `turnover_rate`: monthly turnover rate, calculated by dividing the termination count into the average headcount (beginning headcount + ending headcount / 2) for the respective month
  
### Survey Responses
  
  The `survey_responses` data set contains responses to survey items measured on a 5-point Likert scale, where 1 = *Highly Unfavorable* and 5 = *Highly Favorable*. Each observation represents a unique survey respondent.
  
  * `belong`: belonging
  * `effort`: discretionary effort
  * `incl`: inclusion
  * `eng_1`: engagement item 1 of 3
  * `eng_2`: engagement item 2 of 3
  * `eng_3`: engagement item 3 of 3
  * `happ`: happiness
  * `psafety`: psychological safety
  * `ret_1`: retention item 1 of 3
  * `ret_2`: retention item 2 of 3
  * `ret_3`: retention item 3 of 3
  * `ldrshp`: senior leadership

## 4D Framework

  In practical analytics settings, we generally operate with respect to five primary constraints: timeliness, client expectation, accuracy, reliability, and cost (Bartlett, 2013). Adherence to a lightweight framework over hastily rushing into an analysis full of assumptions generally lends to better outcomes that respect these constraints. A framework ensures (a) the problem statement is understood and well-defined; (b) relevant literature and prior research are reviewed; (c) the measurement strategy is sound; (d) the analysis approach is suitable for the hypotheses being tested; and (e) results and conclusions are valid and communicated in a way that resonates with the target audience. This chapter will outline a recommended framework as well as other important considerations that should be reviewed early in the project.

  It is important to develop a clear understanding of the key elements of research. Scientific research is the systematic, controlled, empirical, and critical investigation of natural phenomena guided by theory and hypotheses about the presumed relations among such phenomena (Kerlinger & Lee, 2000). In other words, research is an organized and systematic way of finding answers to questions. If you are in the business of analytics, I encourage you to think of yourself as a research scientist -- regardless of whether you are wearing a lab coat or have plans to publish.

  As we will discover when exploring the laws of probability in a later chapter, there is a 1 in 20 chance of finding a significant result when none exists. Therefore, it is important to remain disciplined and methodical to protect against backward research wherein the researcher mines data for interesting relationships or differences and then develops hypotheses which they know the data support. There have been many examples of bad research over the years, which often presents in the form of p-hacking or data dredging: the act of finding data to confirm what the researcher wants to prove. This can occur by running an exhaustive number of experiments to find one that supports the hypothesis, or by using only a subset of data which features the expected patterning.

  Academics at elite research institutions are often under immense pressure to publish in top-tier journals which have a track record of accepting new ground-breaking research over replication studies or unsupported hypotheses, and incentives have unfortunately influenced some to compromise integrity. As my PhD advisor told me many years ago, an unsupported hypothesis -- while initially disappointing given the exhaustive literature review that precedes its development -- is a meaningful empirical contribution given theory suggests the opposite should be true.

  If you participated in a science fair as a child, you are likely already familiar with the scientific method. The scientific method is the standard scheme of organized and systematic inquiry, and this duly applies to people analytics practitioners in the promotion of robust analyses and recommendations.

```{r sci-method, out.width = "75%", echo = FALSE, fig.cap = 'The Scientific Method', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_book/graphics/scientific_method.png")

```

  An important feature of the Scientific Method, as reflected in Figure \@ref(fig:sci-method), is that the process is akin to an infinite loop in coding; that is, it never ends. New knowledge resulting from hypothesis testing prompts a new set of questions and hypotheses, which initiates a new lifecycle of scientific inquiry.
  
  Over the years, I have adapted the scientific method into a curtailed four-step framework to help ensure a rigorous and disciplined approach is applied to the end-to-end analytical process. The four steps are (a) Discover, (b) Design, (c) Develop, and (d) Deliver.

```{r 4d-framework, out.width = "100%", echo = FALSE, fig.cap = '4D Framework', fig.align = 'center'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_book/graphics/4d_framework_overview.png")

```

### Discover

  You are likely familiar with the following adage: "An ounce of prevention is worth a pound of cure." Such is the case with respect to planning in an analytics context. During the Discover phase, it is important to remain in the problem zone; seek to understand your clients' needs through active listening and questions. This is not the time for solutioning or committing to any specific deliverables or timelines. If the client's needs are ambiguous, proceeding without clarity is unlikely to result in a favorable outcome.
  
  It is generally helpful to think about analytics solutions -- whether a dashboard with basic metrics and trends or an advanced analysis -- like a Product Owner thinks about the initial and subsequent releases of a commercial product. A **Minimum Viable Product (MVP)** is a version of the solution with the minimum number of features to be useful to early customers who can provide feedback for future enhancements. It is important to clarify that the MVP version of solutions often has both a limited number of users and features, which protects against the tendency to boil the ocean by striving to address every question for every stakeholder. Breaking down large projects into small sets of features that are easier to communicate and adopt provides space for agility and real-time adjustments to the product roadmap per user feedback.
  
  There are some initial questions and considerations that will help frame an analytics project and support its success.
  
  **Client**

  Who is the client? A client can be a person or organization contracting you for consulting services, or an internal stakeholder within your organization who has need. What is important to them?

  **Primary Objective**

  + What is the client ultimately hoping to accomplish? 
  + Is the request merely to satisfy one’s curiosity, or are there actions that can realistically be taken to materially influence said objective?

  **Problem Statement**

  + One of the most important early steps is clearly defining the problem statement. If your understanding of the problem -- after translating from the business terms in which it was likely initially expressed -- is misaligned with the client's needs, none of the subsequent steps matter.

  **Guiding Theories**

  + What theoretical explanations can the client offer as potential rationalizations for the phenomena of interest?
  + Are there existing theories in the organizational literature that should guide how the problem is tackled (e.g., findings from similar research implemented in other contexts)?

  **Research Questions**

  To respect the nuances of the problem statement, it is important to unpack it and frame as a set of overarching questions to guide the research.

  + Q1: ...
  + Q2: ...
  + Q3: ...

  To ensure it is possible (and appropriate) to take action in response to these questions, consider the following after drafting each question: "What would I do if I knew the answer?"

  **Research Hypotheses**

  Once research questions are developed, what do you expect to find based on anecdotal stories or empirical findings? As a next step, these expectations should be expressed in the form of research hypotheses. Please note that these research hypotheses are different from statistical hypotheses (both will be covered in detail in later chapters).

  + H1: ...
  + H2: ...
  + H3: ...

  To ensure the hypotheses lend to actionable analyses, it is important to consider the following: "What does success look like?" In other words, once the project is complete, against which success measures will the project's success be determined? Curiosity is not a business reason and hope is not a reasonable strategy. The following questions may prove helpful in the promotion of actionable -- over merely interesting -- outcomes:

  + What will be done if the hypotheses are supported?
  + What will be done if the hypotheses are *not* supported?

  **Assumptions**

  At this point, it's helpful to consider what assumptions may be embedded in this discovery work. Are the questions and hypotheses rooted in what the client has theorized, or are these the product of an ambiguous understanding of the client's needs?

  **Cadence**
 
  + Is this analysis a one-off, or could there be a need to refresh this analysis on a regular cadence?
  + Are there dates associated with programs or actions this analysis is intended to support?

  **Aggregation**

  Is there a need for individual-level detail supporting the analysis? Aaggregate data should generally be the default unless a compelling justification exists and approval from legal and privacy partners is granted. One important role of analysts is to help keep the audience focused on the bigger picture and findings. Access to individual-level detail can not only introduce unnecessary legal and compliance risk but can also lead to questions and probing that can delay taking needed actions based on the results.

  **Deliverable**

  What is the preferred method of communicating the results of the analysis (e.g., interactive dashboard, static slide deck, document)? It is important to determine this early so that subsequent efforts can be structured to support the preferred deliverable. For example, if an interactive dashboard is preferred, does your Engineering department need to prioritize dependent tasks such as data pipelines, row-level security, BI development, and migrations to production servers?

  **Filters & Dimensions**

  How does your client prefer to segment the workforce? Some common grouping dimensions are business unit, division, team, job family, location, tenure, and management level, but the client may have custom segmentation requirements that will be important to identify and define early in the project.
  
### Design

  Perhaps the most important initial question to answer in the design phase is: "Does anything already exist that addresses part, or all, of the client’s objectives?" If an existing solution will suffice, or a previous analysis can be easily refreshed with recent data, it may be possible to allocate time and resources elsewhere. If related or complimentary analyses have already been performed, they may accelerate new analyses.
  
  The end-user experience is of paramount importance during the Design phase, as solutions should have a consistent look and feel regardless of who developed them. Defining and implementing design guidelines will ensure consistency across analytics projects, as well as within large projects in which multiple analysts are collaborating on various elements of the solution.

  **Data Privacy**

  Are there potential concerns with the study’s objective, planned actions, and/or requested data elements from an employee privacy or legal perspective? A cross-functional data governance committee can help with efficient and consistent decisioning on requests for people data and analytics.
  
  In cases where sensitive attributes such as gender, ethnicity, age, sexual orientation, and disability status are requested, it's always best to exercise a 'safety first' mentality and consult with legal and privacy partners to ensure there is comfort with the intended use of the data. The decision on whether or not to include these sensitive data elements is often less about what the audience can view (e.g., People Partners may already have access to the information at the person level in the source system) and more anchored in what they plan to do with the information.

  **Data Sources & Elements**
  
  Is the required data already accessible in a data warehouse or other analytics environment? If not, does it need to be? What is required to achieve this?

  + What data sources are required?
  + What data elements are required?

  **Data Quality**

  It is important to understand the data generative process and never make assumptions about how anomalies or missing data should be interpreted. After identifying what data sources will be required for a particular analysis, it is important to meet with source system owners and data stewards to deeply understand the business processes by which data are generated in the system(s). An ideal should always be *unimpeachable* data quality at the analysis presentation stage, and this begins with an early understanding and investigation of data quality in the source systems. Below are some helpful questions to consider:
  
  * Is there a better data source, such as data generated further upstream?
  * Are the data actively validated with automated data quality checks and regular exceptions reports?
  * What is the SLA for the data refreshes, and how often is this SLA met?
  * Is there a clear (and accessible) data source owner/steward?
  * Is there comprehensive documentation and field definitions?
  * Do the tables drive business critical deliverables?
  * Who are the other consumers of the data?

  **Variables**

  How will the constructs be measured (e.g., survey instrument, derived attribute, calculated field)?

  **Analysis Method**

  What are the appropriate analysis methods based on the research hypotheses? If modeling is required, is it more important to index on accuracy or interpretability?

  **Dependencies**

  Are other teams required to develop this solution? What is the nature of the work each dependent team will perform? Are there required system configuration changes? Do these teams have capacity to support?

  **Change Management**

  Will this solution impact current processes or solutions? If so, what is the change management plan to facilitate a seamless transition and user experience?

  **Sign-Off**

  Generally, it is best for the client to signoff on the problem statement, analysis approach, and wire frame for the deliverable (if applicable) before providing an ETA and proceeding to the development phase. This ensures alignment on the client's needs and the perceived utility of the solution in addressing those needs.

### Develop

  While development patterns can vary widely across analytics teams, establishing a set of standards can pay dividends in the form of greater efficiency and reliability over time. Pattern-based development ensures analysts who were not involved in a particular project can access the code and easily and quickly understand each step of the analysis: data extraction --> wrangling --> cleaning --> analysis --> visualization.
  
  This is one of the many reasons tools like Excel will not be covered in this book. Software like R and Python allows analysts to organize and annotate steps of the analytical process in a manner that is both logical and reproducible. It bears repeating that learning to code is likely the best investment one can make in the pursuit of a career in analytics.
  
  **Development Patterns**

  + Are there development patterns that should guide the development approach to support consistency? 
  + Are there existing calculated fields that can/should be leveraged for derived data?
  + Are there best practices that should be employed to optimize performance (e.g., load time for dashboards, executing complex queries during non-peak times)?
  + Are there standard color palettes that should be applied?

  **Productionalizable Code**

  + How do models and data science pipelines need to be developed to facilitate a seamless migration from lower to upper environments? For example, initial exploratory data analysis (EDA) may be performed using curated data in flat files for the purpose of identifying meaningful trends, relationships, and differences, but where will this data need to be sourced in production to automate the refresh of models at a regular interval? If the data were provided from multiple source systems, what joins are required to integrate the data? What transformation logic or business rules need to be applied to reproduce the curated data?

  **Unit Testing**

  + What test cases will ensure the veracity of data?
  + Who will perform the testing?

  **UAT Testing**

  + In the spirit of agility and constant contact with the client to prevent surprises, it is generally a good idea to have the client take the solution for a test run within the UAT environment and then provide sign-off before migrating to production. If the deliverable is a deck or doc with results from a model, UAT may surface clarifying questions that can be addressed before releasing to the broader audience.

### Deliver

  The Deliver phase can take many forms depending on the solution being released. If the solution is designed for a large user base, a series of recorded trainings may be in order so that there is a helpful reference for those unable to attend the live sessions or new joiners in the future. It is important to monitor success measures, which could be insights aligned to research hypotheses, dashboard utilization metrics, progress following data-informed interventions, or any number of others defined within the Discover phase.