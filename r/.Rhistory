# Perform Cohen's d
effsize::cohen.d(comp_mgr, comp_rsci)
# Create dummy-coded promotion variable
employees$promo <- ifelse(employees$last_promo == 1, 1, 0)
# Create numeric engagement vectors for promo groups
no_promo <- unlist(subset(employees, promo == 0, select = engagement))
promo <- unlist(subset(employees, promo == 1, select = engagement))
# Perform the Mann-Whitney U (aka Wilcoxon rank sum) test
wilcox.test(no_promo, promo)
# Run Cliff's Delta
effsize::cliff.delta(no_promo, promo)
# Perform Cohen's d
effsize::cohen.d(treat_metrics$post_ind, treat_metrics$pre_ind, paired = TRUE)
# Load library
library(ggpubr)
# Calculate pre/post differences
treat_metrics$diff <- treat_metrics$post_ind - treat_metrics$pre_ind
# Set seed for reproducible results
set.seed(1234)
# Derive happiness index from survey variables
employees$happiness_ind <- (employees$engagement + employees$env_sat + employees$job_sat + employees$rel_sat) / 4
# Sample size of frequent travelers
n = nrow(subset(employees, business_travel == 'Travel_Frequently', select = employee_id))
# Randomly assign half of frequent travelers to treatment and control groups
treat_ids <- sample(unlist(subset(employees, business_travel == 'Travel_Frequently', select = employee_id)), floor(n * .5))
ctrl_ids <- unlist(subset(employees, business_travel == 'Travel_Frequently' & !employee_id %in% treat_ids, select = employee_id))
# Initialize dfs for pre/post metrics
treat_metrics = data.frame(pre_ind = numeric(length(treat_ids)),
rand_num = rnorm(length(treat_ids), mean = 15, sd = 5) * .001,
post_ind = numeric(length(treat_ids)),
diff = numeric(length(treat_ids)))
ctrl_metrics = data.frame(pre_ind = numeric(length(ctrl_ids)),
rand_num = rnorm(length(ctrl_ids), mean = 0, sd = 1) * .001,
post_ind = numeric(length(ctrl_ids)),
diff = numeric(length(ctrl_ids)))
# Store happiness indices for treatment and control groups
treat_metrics$pre_ind <- unlist(subset(employees, employee_id %in% treat_ids, select = happiness_ind))
ctrl_metrics$pre_ind <- unlist(subset(employees, employee_id %in% ctrl_ids, select = happiness_ind))
# Create vectors with artificially inflated post-intervention happiness indices
treat_metrics$post_ind <- treat_metrics$pre_ind + treat_metrics$rand_num
ctrl_metrics$post_ind <- ctrl_metrics$pre_ind + ctrl_metrics$rand_num
# Force an upper bound of 4 to adjusted index scores (variables were measured using a 4-point Likert scale)
treat_metrics$post_ind <- if(treat_metrics$post_ind > 4) {4} else {treat_metrics$post_ind}
ctrl_metrics$post_ind <- if(ctrl_metrics$post_ind > 4) {4} else {ctrl_metrics$post_ind}
# Load library
library(ggpubr)
# Calculate pre/post differences
treat_metrics$diff <- treat_metrics$post_ind - treat_metrics$pre_ind
ctrl_metrics$diff <- ctrl_metrics$post_ind - ctrl_metrics$pre_ind
# Histogram for distribution of pre/post treatment group differences
p_treat <- ggplot2::ggplot() +
ggplot2::aes(treat_metrics$diff) +
ggplot2::labs(title = "Treatment Group", x = "Happiness Index Differences", y = "Frequency") +
ggplot2::geom_histogram(fill = "#414141") +
ggplot2::theme_bw() +
ggplot2::theme(plot.title = element_text(hjust = 0.5))
# Histogram for distribution of pre/post control group differences
p_ctrl <- ggplot2::ggplot() +
ggplot2::aes(ctrl_metrics$diff) +
ggplot2::labs(title = "Control Group", x = "Happiness Index Differences", y = "Frequency") +
ggplot2::geom_histogram(fill = "#414141") +
ggplot2::theme_bw() +
ggplot2::theme(plot.title = element_text(hjust = 0.5))
# Display histograms side-by-side
ggpubr::ggarrange(p_treat, p_ctrl, ncol = 2, nrow = 1)
# Compute Shapiro-Wilk test of normality
shapiro.test(treat_metrics$diff)
shapiro.test(ctrl_metrics$diff)
# Perform Cohen's d
effsize::cohen.d(treat_metrics$post_ind, treat_metrics$pre_ind, paired = TRUE)
mean(treat_metrics$post_ind)
mean(treat_metrics$pre_ind)
# Run Cliff's Delta
effsize::cliff.delta(treat_metrics$post_ind, treat_metrics$pre_ind, paired = TRUE)
knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/contingency_table.png")
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Load library
library(ggplot2)
# Sample from chi-square distribution for various df
chisq <- data.frame(dens = c(rchisq(1:10000000, df = 5), rchisq(1:10000000, df = 10), rchisq(1:10000000, df = 15)),
df = rep(c("df = 5", "df = 10", "df = 15"), each = 10000000))
# Order data by df for plotting
chisq$df <- with(chisq, reorder(df, dens))
# Plot chi-square distributions
ggplot2::ggplot(chisq, aes(x = dens, fill = df)) +
ggplot2::labs(x = "x", y = "Density") +
ggplot2::geom_density(alpha = 0.6) +
ggplot2::scale_fill_manual(values = c("#0099FF", "0066FF", "0033FF")) +
ggplot2::theme_bw()
# Load library
library(dplyr)
# Set seed for reproducible random numbers
set.seed(123)
# Fill sample size vector
n_counts <- c(100, 1000, 10000)
# Fill standard deviation vector
sds <- c(25, 50, 75)
# Initialize lists
p = list()
t_test = list()
# Initialize index
i = 1
for (n in n_counts){
for (sd in sds){
# Draw random numbers from normal distribution per defined parameters
a <- data.frame(x = rnorm(n, 100, sd))
b <- data.frame(x = rnorm(n, 120, sd))
# Label groups and combine within single df
a$group <- 'a'
b$group <- 'b'
ab <- rbind(a, b)
# Store mean values in df
mean_df <- ab %>%
group_by(group) %>%
summarize(mean = mean(x))
# Calculate absolute mean difference
xbar_delta <- abs(round(mean(ab[ab$group == 'b', 'x']) - mean(ab[ab$group == 'a', 'x']), 0))
t_test <- t.test(ab[ab$group == 'a', 'x'], ab[ab$group == 'b', 'x'])
# Store viz to object
p[[i]] <- ggplot2::ggplot(ab, aes(x, fill = group)) +
ggplot2::labs(title = paste0("MD = ", xbar_delta, "\n t = ", round(t_test$statistic, 1), ifelse(t_test$p.value < .05, ", p < .05", ", p > .05")), x = "x", y = "Density") +
ggplot2::geom_density(alpha = 0.6) +
ggplot2::scale_fill_manual(values = c("skyblue", "lightgrey")) +
ggplot2::geom_vline(data = mean_df, aes(xintercept = mean), colour = c("blue", "#3D3D3D"), size = .5, linetype = "dashed") +
ggplot2::theme_bw() +
ggplot2::theme(plot.title = element_text(hjust = 0.5)) +
ggplot2::theme(legend.position = "none")
# Increment counter variable by 1
i = i + 1
}
}
# Visualize density plots side-by-side
ggpubr::ggarrange(p[[1]], p[[2]], p[[3]], p[[4]], p[[5]], p[[6]], p[[7]], p[[8]], p[[9]],
ncol = 3, nrow = 3)
# Subset data
data <- subset(employees, job_title %in% c('Manager', 'Research Scientist'), select = c(annual_comp, job_title))
# Produce boxplots to visualize compensation distribution by job title
ggplot2::ggplot(data, aes(x = as.factor(job_title), y = annual_comp, color = job_title)) +
ggplot2::labs(x = "Job Title", y = "Annual Compensation") +
ggplot2::guides(col = guide_legend("Job Title")) +
ggplot2::theme_bw() +
ggplot2::geom_boxplot()
?shapiro.test
# Compute Shapiro-Wilk test of normality
shapiro.test(subset(employees, job_title == 'Manager', select = annual_comp))
# Compute Shapiro-Wilk test of normality
shapiro.test(unlist(subset(employees, job_title == 'Manager', select = annual_comp)))
shapiro.test(unlist(subset(employees, job_title == 'Research Scientist', select = annual_comp)))
# Compute Shapiro-Wilk test of normality
with(employees, shapiro.test(annual_comp[job_title == 'Manager']))
# Compute Shapiro-Wilk test of normality for each group
with(employees, shapiro.test(annual_comp[job_title == 'Manager']))
with(employees, shapiro.test(annual_comp[job_title == 'Research Scientist']))
# Compute Shapiro-Wilk test of normality for each group
with(employees, shapiro.test(annual_comp[job_title == 'Manager']))
View(employees)
unique(employees$job_title)
# Compute Shapiro-Wilk test of normality for each group
with(employees, shapiro.test(annual_comp[job_title == 'Manager']))
# Compute Shapiro-Wilk test of normality for each group
with(employees, shapiro.test(annual_comp[job_title == 'Sales Representative']))
# Compute Shapiro-Wilk test of normality for each group
with(employees, shapiro.test(annual_comp[job_title == 'Manufacturing Director']))
# Compute Shapiro-Wilk test of normality for each group
with(employees, shapiro.test(annual_comp[job_title == 'Healthcare Representative']))
# Compute Shapiro-Wilk test of normality for each group
with(employees, shapiro.test(annual_comp[job_title == 'Healthcare Representative']))
with(employees, shapiro.test(annual_comp[job_title == 'Research Scientist']))
# Compute Shapiro-Wilk test of normality for each group
with(employees, shapiro.test(annual_comp[job_title == 'Manager']))
with(employees, shapiro.test(annual_comp[job_title == 'Research Scientist']))
# Set seed for reproducible results
set.seed(1234)
# Derive happiness index from survey variables
employees$happiness_ind <- (employees$engagement + employees$env_sat + employees$job_sat + employees$rel_sat) / 4
# Sample size of frequent travelers
n = nrow(subset(employees, business_travel == 'Travel_Frequently', select = employee_id))
# Randomly assign half of frequent travelers to treatment and control groups
treat_ids <- sample(unlist(subset(employees, business_travel == 'Travel_Frequently', select = employee_id)), floor(n * .5))
ctrl_ids <- unlist(subset(employees, business_travel == 'Travel_Frequently' & !employee_id %in% treat_ids, select = employee_id))
# Initialize dfs for pre/post metrics
treat_metrics = data.frame(pre_ind = numeric(length(treat_ids)),
rand_num = rnorm(length(treat_ids), mean = 15, sd = 5) * .001,
post_ind = numeric(length(treat_ids)),
diff = numeric(length(treat_ids)))
ctrl_metrics = data.frame(pre_ind = numeric(length(ctrl_ids)),
rand_num = rnorm(length(ctrl_ids), mean = 0, sd = 1) * .001,
post_ind = numeric(length(ctrl_ids)),
diff = numeric(length(ctrl_ids)))
# Store happiness indices for treatment and control groups
treat_metrics$pre_ind <- unlist(subset(employees, employee_id %in% treat_ids, select = happiness_ind))
ctrl_metrics$pre_ind <- unlist(subset(employees, employee_id %in% ctrl_ids, select = happiness_ind))
# Create vectors with artificially inflated post-intervention happiness indices
treat_metrics$post_ind <- treat_metrics$pre_ind + treat_metrics$rand_num
ctrl_metrics$post_ind <- ctrl_metrics$pre_ind + ctrl_metrics$rand_num
# Force an upper bound of 4 to adjusted index scores (variables were measured using a 4-point Likert scale)
treat_metrics$post_ind <- if(treat_metrics$post_ind > 4) {4} else {treat_metrics$post_ind}
ctrl_metrics$post_ind <- if(ctrl_metrics$post_ind > 4) {4} else {ctrl_metrics$post_ind}
# Load library for Levene's test
library(car)
# Perform Levene's test for homogeneity of variance
car::leveneTest(happiness_ind ~ pre_ind, post_ind, data = treat_metrics)
View(treat_metrics)
?leveneTest
# Perform Levene's test for homogeneity of variance
car::leveneTest(treat_metrics$pre_ind, treat_metrics$post_ind)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Nonparametric Kruskal one-way ANOVA
kruskal.test(annual_comp ~ job_sat, data = employees)
pairwise.wilcox.test(employees$annual_comp, employees$job_sat, p.adjust.method = "BH")
pairwise.wilcox.test(employees$annual_comp, employees$job_sat, p.adjust.method = "BH")
View(employees)
# Factorial ANOVA investigating mean differences in annual comp by job satisfaction and dept
factorial <- aov(annual_comp ~ job_sat + dept, data = employees)
summary(factorial)
# ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept
interaction <- aov(annual_comp ~ job_sat * trainings, data = employees)
summary(interaction)
# ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept
interaction <- aov(annual_comp ~ job_sat * dept, data = employees)
summary(interaction)
# Factorial ANOVA investigating mean differences in annual comp by job satisfaction and dept
factorial <- aov(annual_comp ~ job_sat + ed_lvl, data = employees)
summary(factorial)
# ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept
interaction <- aov(annual_comp ~ job_sat * ed_lvl, data = employees)
summary(interaction)
# Factorial ANOVA investigating mean differences in annual comp by job satisfaction and dept
factorial <- aov(annual_comp ~ job_sat + ed_field, data = employees)
summary(factorial)
# ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept
interaction <- aov(annual_comp ~ job_sat * ed_field, data = employees)
summary(interaction)
# Factorial ANOVA investigating mean differences in annual comp by job satisfaction and dept
factorial <- aov(annual_comp ~ job_sat + overtime, data = employees)
summary(factorial)
# ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept
interaction <- aov(annual_comp ~ job_sat * overtime, data = employees)
summary(interaction)
# Factorial ANOVA investigating mean differences in annual comp by job satisfaction and dept
factorial <- aov(annual_comp ~ job_sat + stock_opt_lvl, data = employees)
summary(factorial)
# ANOVA investigating interaction between mean differences in annual comp by job satisfaction x dept
interaction <- aov(annual_comp ~ job_sat * stock_opt_lvl, data = employees)
summary(interaction)
aggregate(annual_comp ~ job_sat + stock_opt_levl, employees, mean )
aggregate(annual_comp ~ job_sat + stock_opt_lvl, employees, mean )
View(factorial)
ggplot2::ggplot(employees, aes(x = job_sat, y = annual_comp, group = stock_opt_lvl)) +
ggplot2::geom_line(aes(color = supp))+
ggplot2::geom_point(aes(color = supp))
library(ggplot)
library(ggplot2)
ggplot2::ggplot(employees, aes(x = job_sat, y = annual_comp, group = stock_opt_lvl)) +
ggplot2::geom_line(aes(color = supp))+
ggplot2::geom_point(aes(color = supp))
ggplot2::ggplot(employees, aes(x = job_sat, y = annual_comp, group = stock_opt_lvl)) +
ggplot2::geom_line(aes(color = stock_opt_lvl))+
ggplot2::geom_point(aes(color = stock_opt_lvl))
ggplot2::ggplot(employees, aes(x = job_sat, y = mean(annual_comp), group = stock_opt_lvl)) +
ggplot2::geom_line(aes(color = stock_opt_lvl))+
ggplot2::geom_point(aes(color = stock_opt_lvl))
# Calculate mean for each IV pair
df <- aggregate(annual_comp ~ job_sat + stock_opt_lvl, employees, mean)
df
View(df)
ggplot2::ggplot(df, aes(x = job_sat, y = annual_comp, group = stock_opt_lvl)) +
ggplot2::geom_line(aes(color = stock_opt_lvl))+
ggplot2::geom_point(aes(color = stock_opt_lvl))
ggplot2::ggplot(df, aes(x = job_sat, y = annual_comp, group = stock_opt_lvl)) +
ggplot2::geom_line(aes(color = stock_opt_lvl)) +
ggplot2::theme_bw() +
ggplot2::geom_point(aes(color = stock_opt_lvl))
ggplot2::ggplot(df, aes(x = job_sat, y = annual_comp, group = stock_opt_lvl)) +
ggplot2::geom_line(aes(color = stock_opt_lvl)) +
ggplot2::labs(x = "Job Satisfaction", y = "Annual Compensation") +
ggplot2::guides(col = guide_legend("Stock Option Level")) +
ggplot2::theme_bw() +
ggplot2::geom_point(aes(color = stock_opt_lvl))
# Calculate mean for each IV pair
combos <- aggregate(annual_comp ~ job_sat + stock_opt_lvl, employees, mean)
combos
ggplot2::ggplot(combos, aes(x = job_sat, y = annual_comp, group = stock_opt_lvl)) +
ggplot2::geom_line(aes(color = stock_opt_lvl)) +
ggplot2::labs(x = "Job Satisfaction", y = "Annual Compensation") +
ggplot2::guides(col = guide_legend("Stock Option Level")) +
ggplot2::theme_bw() +
ggplot2::geom_point(aes(color = stock_opt_lvl))
# Load data sets
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
status <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/status.csv")
benefits <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/benefits.csv")
demographics <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/demographics.csv")
job <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/job.csv")
payroll <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/payroll.csv")
performance <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/performance.csv")
prior_employment <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/prior_employment.csv")
survey_response <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/survey_response.csv")
tenure <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/tenure.csv")
# Return row and column counts
dim(employees)
# Load SQL library
library(sqldf)
# Store SQL query as a character string using the paste() function
sql_string <- paste("SELECT
employee_id
FROM
employees
WHERE
dept = 'Research & Development'
LIMIT 10")
# Execute SQL query
sqldf::sqldf(sql_string)
# Store SQL query as a character string
sql_string <- paste("SELECT
DISTINCT(job_title) AS job_title,
COUNT(*) AS employee_count,
commute_dist
FROM
(SELECT
demographics.employee_id,
job_title,
commute_dist,
RANK () OVER (PARTITION BY job_title ORDER BY commute_dist DESC) AS commute_dist_rank
FROM
demographics
LEFT JOIN
job
ON
demographics.employee_id = job.employee_id
INNER JOIN
(SELECT employee_id FROM tenure WHERE org_tenure > 1) ids
ON
demographics.employee_id = ids.employee_id
WHERE
dept = 'Research & Development'
ORDER BY
job_title,
commute_dist_rank) tbl
WHERE
tbl.commute_dist_rank = 1
GROUP BY
job_title")
# Execute SQL query
sqldf::sqldf(sql_string)
# Store SQL query as a character string
sql_string <- paste("WITH max_commute_job
AS
(SELECT
demographics.employee_id,
job_title,
commute_dist,
RANK () OVER (PARTITION BY job_title ORDER BY commute_dist DESC) AS commute_dist_rank
FROM
demographics
LEFT JOIN
job
ON
demographics.employee_id = job.employee_id
INNER JOIN
(SELECT employee_id FROM tenure WHERE org_tenure > 1) ids
ON
demographics.employee_id = ids.employee_id
WHERE
dept = 'Research & Development'
ORDER BY
job_title,
commute_dist_rank)
SELECT
DISTINCT(job_title) AS job_title,
COUNT(*) AS employee_count,
commute_dist
FROM
max_commute_job
WHERE
commute_dist_rank = 1
GROUP BY
job_title")
# Execute SQL query
sqldf::sqldf(sql_string)
knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/discrete_differences_test_table.png")
# Load data viz library
library(ggplot2)
library(ggpubr)
# Create function to visualize distribution
dist.viz <- function(data, x) {
viz <- ggplot2::ggplot() +
ggplot2::aes(data) +
ggplot2::labs(title = paste("Job Sat = ", x), x = "Annual Compensation", y = "Frequency") +
ggplot2::geom_histogram(fill = "#414141") +
ggplot2::theme_bw() +
ggplot2::theme(plot.title = element_text(hjust = 0.5))
return(viz)
}
# Produce annual compensation vectors for each job satisfaction level
# Unlist() is needed to convert the default object from subset() into a numeric vector
group_1 <- unlist(subset(employees, job_sat == 1, select = annual_comp))
group_2 <- unlist(subset(employees, job_sat == 2, select = annual_comp))
group_3 <- unlist(subset(employees, job_sat == 3, select = annual_comp))
group_4 <- unlist(subset(employees, job_sat == 4, select = annual_comp))
# Call UDF to build annual comp histogram for each job satisfaction level
viz_1 <- dist.viz(data = group_1, x = 1)
viz_2 <- dist.viz(data = group_2, x = 2)
viz_3 <- dist.viz(data = group_3, x = 3)
viz_4 <- dist.viz(data = group_4, x = 4)
# Display distribution visualizations
ggpubr::ggarrange(viz_1, viz_2, viz_3, viz_4,
ncol = 2, nrow = 2)
# Generate residuals for each group
residuals <- c(group_1 - mean(group_1), group_2 - mean(group_2), group_3 - mean(group_3), group_4 - mean(group_4))
# Create a Q-Q plot of residuals
ggpubr::ggqqplot(residuals)
# Compute Shapiro-Wilk test of normality
shapiro.test(residuals)
# One-way ANOVA investigating mean differences in annual comp by job satisfaction
one.way <- aov(annual_comp ~ job_sat, data = employees)
summary(one.way)
knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/4d_framework_overview.png")
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
View(employees)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ job_lvl, data = employees)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ overtime, data = employees)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ job_tenure, data = employees)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ work_exp, data = employees)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ engagement, data = employees)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ commute_dist, data = employees)
summary(lm.fit)
?subset
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ commute_dist, data = subset(employees, select = job_title %in% c('Sales Executive', 'Sales Representative')))
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ commute_dist, data = subset(employees, select = job_title %in% c('Sales Executive', 'Sales Representative'), select = c('ytd_sales', 'commute_dist')))
subset(employees, select = job_title %in% c('Sales Executive', 'Sales Representative'), select = c('ytd_sales', 'commute_dist')
subset(employees, select = job_title %in% c('Sales Executive', 'Sales Representative'), select = c('ytd_sales', 'commute_dist'))
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ commute_dist, data = employees[employees$job_title %in% c('Sales Executive', 'Sales Representative'), ])
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ engagement, data = employees[employees$job_title %in% c('Sales Executive', 'Sales Representative'), ])
summary(lm.fit)
# Subset employees data frame
data <- subset(employees, job_title %in% c('Sales Executive', 'Sales Representative'))
data
View(data)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ engagement, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ env_sat, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ rel_sat, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ job_sat, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ work_exp, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ ed_lvl, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ age, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ trainings, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ overtime, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ last_promo, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ mgr_tenure, data)
summary(lm.fit)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Subset employees data frame
data <- subset(employees, job_title %in% c('Sales Executive', 'Sales Representative'))
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ mgr_tenure, data)
summary(lm.fit)
# Build (or 'fit' as it is often called) a linear model using all predictor variables, and output the results to the screen.
lm.fit <- lm(ytd_sales ~ engagement, data)
summary(lm.fit)
?lm
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Subset employees data frame; sales are only applicable for those in sales positions
data <- subset(employees, job_title %in% c('Sales Executive', 'Sales Representative'))
# Regress YTD sales on engagement
lm.fit <- lm(ytd_sales ~ engagement, data)
# Output model results
summary(lm.fit)
# Output model results
summary(lm.fit)
