# Principal axis factoring using 3 factors and oblimin rotation
efa.fit <- psych::fa(survey_dat, nfactors = 3, rotate = 'oblimin')
# Display factor loadings
efa.fit$loadings
psych::fa.diagram(efa.fit)
# Load library
library(GGally)
# Visualize correlation matrix
GGally::ggpairs(subset(survey_dat, select = c("eng_1", "eng_2", "eng_3", "ret_1", "ret_2", "ret_3")))
# Load library
library(lavaan)
# Model specification; each line represents a separate latent factor
model <- paste('engagement =~ eng_1 + eng_2 + eng_3
retention =~ ret_1 + ret_2 + ret_3')
# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat)
# Load library
library(lavaanPlot)
# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
View(survey_dat)
survey_dat1 <- df[sample(nrow(survey_dat), 300), ]
survey_dat1 <- survey_dat[sample(nrow(survey_dat), 300), ]
GGally::ggpairs(subset(survey_dat1, select = c("eng_1", "eng_2", "eng_3", "ret_1", "ret_2", "ret_3")))
cfa.fit <- lavaan::cfa(model, data = survey_dat1)
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
survey_dat1 <- survey_dat[sample(nrow(survey_dat), 250), ]
# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat)
# Load library
library(lavaanPlot)
# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat1)
# Load library
library(lavaanPlot)
# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
survey_dat1 <- survey_dat[sample(nrow(survey_dat), 350), ]
# Load library
library(lavaan)
# Model specification; each line represents a separate latent factor
model <- paste('engagement =~ eng_1 + eng_2 + eng_3
retention =~ ret_1 + ret_2 + ret_3')
# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat1)
# Load library
library(lavaanPlot)
# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
survey_dat1 <- survey_dat[sample(nrow(survey_dat), 250), ]
# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat1)
# Load library
library(lavaanPlot)
# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
survey_dat1 <- survey_dat[sample(nrow(survey_dat), 250), ]
# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat1)
# Load library
library(lavaanPlot)
# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
survey_dat1 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat2 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat3 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat4 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat5 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat6 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat7 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat8 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat9 <- survey_dat[sample(nrow(survey_dat), 400), ]
survey_dat10 <- survey_dat[sample(nrow(survey_dat), 400), ]
cfa.fit <- lavaan::cfa(model, data = survey_dat1)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat2)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat3)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat4)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat5)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat6)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat7)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat8)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat9)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat10)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat2)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat2)
# Load library
library(lavaanPlot)
# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat2)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
write.csv(survey_dat2, "/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/survey_responses2.csv", row.names = FALSE)
# Load library
library(dplyr)
# Load survey response data
survey_dat <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/survey_responses.csv")
# Show dimensions of survey data
dim(survey_dat)
# Load library
library(psych)
# Kaiser-Meyer-Olkin (KMO) statistic
psych::KMO(survey_dat)
# Bartlett's Test of Sphericity
psych::cortest.bartlett(cor(survey_dat), nrow(survey_dat))
# Produce scree plot
psych::scree(survey_dat, pc = FALSE)
# Load library
library(ggplot2)
# Perform PCA
pca <- prcomp(survey_dat, scale = TRUE)
# Calculate explained variance for each principal component
pca_var = (pca$sdev^2 / sum(pca$sdev^2)) * 100
# Create scree plot
ggplot2::qplot(1:length(pca_var), pca_var) +
ggplot2::geom_line() +
ggplot2::scale_x_continuous(breaks = 1:length(pca_var)) +
ggplot2::xlab("Principal Component") +
ggplot2::ylab("Variance Explained (%)") +
ggplot2::theme_bw()
# Principal axis factoring using 3 factors and oblimin rotation
efa.fit <- psych::fa(survey_dat, nfactors = 3, rotate = 'oblimin')
# Display factor loadings
efa.fit$loadings
psych::fa.diagram(efa.fit)
# Load library
library(GGally)
# Visualize correlation matrix
GGally::ggpairs(subset(survey_dat, select = c("eng_1", "eng_2", "eng_3", "ret_1", "ret_2", "ret_3")))
# Load library
library(lavaan)
# Model specification; each line represents a separate latent factor
model <- paste('engagement =~ eng_1 + eng_2 + eng_3
retention =~ ret_1 + ret_2 + ret_3')
# Fit the model
cfa.fit <- lavaan::cfa(model, data = survey_dat)
# Load library
library(lavaanPlot)
# Visualize path diagram
lavaanPlot::lavaanPlot(model = cfa.fit, coefs = TRUE, stand = TRUE)
cfa.fit <- lavaan::cfa(model, data = survey_dat2)
# Summarize the model
summary(cfa.fit, fit.measures = TRUE)
# Load library
library(dplyr)
# Load survey response data
survey_dat <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/survey_responses.csv")
# Show dimensions of survey data
dim(survey_dat)
# Load library
library(psych)
# Kaiser-Meyer-Olkin (KMO) statistic
psych::KMO(survey_dat)
# Load library
library(ggplot2)
# Perform PCA
pca <- prcomp(survey_dat, scale = TRUE)
# Calculate explained variance for each principal component
pca_var = (pca$sdev^2 / sum(pca$sdev^2)) * 100
# Create scree plot
ggplot2::qplot(1:length(pca_var), pca_var) +
ggplot2::geom_line() +
ggplot2::scale_x_continuous(breaks = 1:length(pca_var)) +
ggplot2::xlab("Principal Component") +
ggplot2::ylab("Variance Explained (%)") +
ggplot2::theme_bw()
# Load library
library(factoextra)
install.packages("factoextra", dependencies = TRUE)
# Load library
library(factoextra)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Scale data
scale(employees)
library(dplyr)
# Scale data
employees_scaled <- employees %>% mutate_if(is.numeric, scale)
View(employees_scaled)
which(sapply(employees[vars], is.numeric))
which(sapply(employees, is.numeric))
idx <- which(sapply(employees, is.numeric))
foo <- employees[, idx]
foo
idx <- which(sapply(employees, is.numeric))
employees <- employees[, idx]
View(employees)
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
View(employees_trans)
employees <- subset(employees, select = -c("employee_id", "ytd_leads", "ytd_sales"))
employees <- subset(employees, select = -c(employee_id, ytd_leads, ytd_sales))
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Filter employee data to numeric variables
idx <- which(sapply(employees, is.numeric))
employees <- employees[, idx]
# Drop employee_id and sparsely populated sales variables
employees <- subset(employees, select = -c(employee_id, ytd_leads, ytd_sales))
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
View(employees_trans)
?fviz_nbclust
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
summary(employees_trans)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Filter employee data to numeric variables
idx <- which(sapply(employees, is.numeric)) # store indices of numeric variables
employees <- employees[, idx] # filter df using indices
# Drop unimportant and sparsely populated sales variables
employees <- subset(employees, select = -c(employee_id, standard_hrs, ytd_leads, ytd_sales))
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
factoextra::kmeans(employees_trans, centers = 3)
?kmeans
kmeans(employees_trans, centers = 3)
# Perform K-means clustering
km <- kmeans(employees_trans, centers = 3)
km$centers
km$cluster
km$withinss
km$size
fviz_cluster(km, data = employee_trans)
# Calculate mean of each cluster
aggregate(employee_trans, by = list(cluster = km$cluster), mean)
# Calculate mean of each cluster
aggregate(employees_trans, by = list(cluster = km$cluster), mean)
# Calculate mean of each cluster
aggregate(employees, by = list(cluster = km$cluster), mean)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Filter employee data to numeric variables
idx <- which(sapply(employees, is.numeric)) # store indices of numeric variables
employees <- employees[, idx] # filter df using indices
# Drop unimportant and sparsely populated sales variables
employees <- subset(employees, select = -c(employee_id, standard_hrs, ytd_leads, ytd_sales))
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
# Perform K-means clustering
km <- kmeans(employees_trans, centers = 3)
# Return n-count of clusters
km$size
# Calculate mean of each cluster using original data
aggregate(employees, by = list(cluster = km$cluster), mean)
592+603+275
# Add cluster assignment to df
employees$cluster <- cbind(employees, cluster = km$cluster)
View(employees)
# Add cluster assignment to df
employees$km_cluster <- cbind(employees, cluster = km$cluster)
View(employees)
# Add cluster assignment to df
employees['km_cluster'] <- cbind(employees, cluster = km$cluster)
View(employees)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Filter employee data to numeric variables
idx <- which(sapply(employees, is.numeric)) # store indices of numeric variables
employees <- employees[, idx] # filter df using indices
# Drop unimportant and sparsely populated sales variables
employees <- subset(employees, select = -c(employee_id, standard_hrs, ytd_leads, ytd_sales))
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
# Perform K-means clustering
km <- kmeans(employees_trans, centers = 3)
# Return n-count of clusters
km$size
# Calculate mean of each cluster using original data
aggregate(employees, by = list(cluster = km$cluster), mean)
# Add cluster assignment to df
employees['km_cluster'] <- cbind(employees, cluster = km$cluster)
View(employees)
employees %>% group_by(km_cluster)
library(dplyr)
employees %>% group_by(km_cluster)
employees %>% group_by(km_cluster) %>% summarise(n = n())
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Filter employee data to numeric variables
idx <- which(sapply(employees, is.numeric)) # store indices of numeric variables
employees <- employees[, idx] # filter df using indices
# Drop unimportant and sparsely populated sales variables
employees <- subset(employees, select = -c(employee_id, standard_hrs, ytd_leads, ytd_sales))
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
# Perform K-means clustering
km <- kmeans(employees_trans, centers = 3)
# Return n-count of clusters
km$size
# Return n-count of clusters
km$size
# Add cluster assignment to df
employees['km_cluster'] <- cbind(employees, cluster = km$cluster)
km$cluster
employees$cluster <- cbind(employees, cluster = km$cluster)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Filter employee data to numeric variables
idx <- which(sapply(employees, is.numeric)) # store indices of numeric variables
employees <- employees[, idx] # filter df using indices
# Drop unimportant and sparsely populated sales variables
employees <- subset(employees, select = -c(employee_id, standard_hrs, ytd_leads, ytd_sales))
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
# Perform K-means clustering
km <- kmeans(employees_trans, centers = 3)
# Return n-count of clusters
km$size
# Calculate mean of each cluster using original data
aggregate(employees, by = list(cluster = km$cluster), mean)
# Add cluster assignment to df
employees <- cbind(employees, cluster = km$cluster)
View(employees)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Filter employee data to numeric variables
idx <- which(sapply(employees, is.numeric)) # store indices of numeric variables
employees <- employees[, idx] # filter df using indices
# Drop unimportant and sparsely populated sales variables
employees <- subset(employees, select = -c(employee_id, standard_hrs, ytd_leads, ytd_sales))
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
# Perform K-means clustering
km <- kmeans(employees_trans, centers = 3)
# Perform K-means clustering
km <- kmeans(employees_trans, centers = 3)
# Return n-count of clusters
km$size
# Calculate mean of each cluster using original data
aggregate(employees, by = list(cluster = km$cluster), mean)
# Add cluster assignment to df
employees <- cbind(employees, cluster = km$cluster)
# Load employee data
employees <- read.csv("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/data/employees.csv")
# Filter employee data to numeric variables
idx <- which(sapply(employees, is.numeric)) # store indices of numeric variables
employees <- employees[, idx] # filter df using indices
# Drop unimportant and sparsely populated sales variables
employees <- subset(employees, select = -c(employee_id, standard_hrs, ytd_leads, ytd_sales))
# Center and scale data
employees_trans <- scale(employees, center = TRUE, scale = TRUE)
# Load library
library(factoextra)
# Determine optimal number of clusters
factoextra::fviz_nbclust(employees_trans, kmeans, method = "wss")
# Perform K-means clustering
km <- kmeans(employees_trans, centers = 3)
# Return n-count of clusters
km$size
# Calculate mean of each cluster using original data
aggregate(employees, by = list(cluster = km$cluster), mean)
# Add cluster assignment to df
employees <- cbind(employees, km_cluster = km$cluster)
View(employees)
subset(employees, select = -(km_cluster))
employees <- subset(employees, select = -(km_cluster))
# Define linkage methods
methods <- c("complete", "single", "average", "centroid", "ward")
names(methods) <- c("complete", "single", "average", "centroid", "ward")
# Create function to compute agglomerative coefficient
agg_coeff <- function(x) {
agnes(employees_trans, method = x)$ac
}
# Compute agglomerative coefficient for each linkage method
sapply(methods, agg_coeff)
library(factoextra)
# Compute agglomerative coefficient for each linkage method
sapply(methods, agg_coeff)
library(cluster)
# Load library
library(cluster)
# Define linkage methods
methods <- c("complete", "single", "average", "centroid", "ward")
names(methods) <- c("complete", "single", "average", "centroid", "ward")
# Create function to compute agglomerative coefficient
agg_coeff <- function(x) {
cluster::agnes(employees_trans, method = x)$ac
}
# Compute agglomerative coefficient for each linkage method
sapply(methods, agg_coeff)
# Load library
library(cluster)
# Define linkage methods
methods <- c("complete", "single", "average", "centroid", "ward")
names(methods) <- c("complete", "single", "average", "centroid", "ward")
# Create function to compute agglomerative coefficient
agg_coeff <- function(x) {
print(x)
cluster::agnes(employees_trans, method = x)$ac
}
# Compute agglomerative coefficient for each linkage method
sapply(methods, agg_coeff)
?agnes
# Load library
library(cluster)
# Define linkage methods
# Note: centroid is not available for forthcoming agnes() function
methods <- c("complete", "single", "average", "ward")
names(methods) <- c("complete", "single", "average", "ward")
# Create function to compute agglomerative coefficient
agg_coeff <- function(x) {
print(x)
cluster::agnes(employees_trans, method = x)$ac
}
# Compute agglomerative coefficient for each linkage method
sapply(methods, agg_coeff)
# Perform hierarchical clustering using Ward's linkage method
hclust <- cluster::agnes(employees_trans, method = "ward")
cluster::pltree(hclust, main = "Dendogram")
cluster::pltree(hclust, hang = -1, main = "Dendogram")
cluster::pltree(hclust, cex = .6, hang = -1, main = "Dendogram")
cluster::pltree(hclust, cex = .6, main = "Dendogram")
cluster::pltree(hclust, main = "Dendogram")
# Calculate gap statistic across 1-10 clusters
gap_stat <- cluster::clusGap(employee_trans, FUN = hcut, nstart = 25, K.max = 10, B = 50)
# Calculate gap statistic across 1-10 clusters
gap_stat <- cluster::clusGap(employees_trans, FUN = hcut, nstart = 25, K.max = 10, B = 50)
# Calculate gap statistic across 1-10 clusters
gap_stat <- cluster::clusGap(employees_trans, FUN = hcut, nstart = 25, K.max = 10, B = 50)
# Generate plot of gap statistic against cluster count
factoextra::fviz_gap_stat(gap_stat)
View(employees)
# Compute distance matrix
d_matrix <- dist(employees_trans, method = "euclidean")
# Perform hierarchical clustering using Ward's method
hclust_final <- hclust(d_matrix, method = "ward.D2" )
# Cut the dendogram into 7 clusters
groups <- cutree(hclust_final, k = 7)
# Append cluster labels to original data
employees <- cbind(employees, hier_cluster = groups)
library(peopledata)
install.packages("peopledata", dependencies = TRUE)
library(peopledata)
load(peopledata)
peopledata
library(people_data)
?peopledata
