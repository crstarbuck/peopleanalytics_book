# Appendix {#appendix}

## Exercise Solutions


**Intro to R**

1. What is the difference between a factor and character vector?

  Factors take on a limited number of predefined categorical values, while character vectors may contain any number of values (including numbers and dates treated as characters).

2. What is vectorization?

  Vectorization performs a given operation on each and every element of an object. This makes writing code more efficient and concise.

3. How do data frames differ from matrices?

  While both data frames and matrices are retangular objects (values organized within rows and columns), data frames can hold multiple types of data (e.g., integers, characters, dates, logical) while matrices are limited to single data type.

4. Executing the `Sum()` function will achieve the same result as `sum()`.
   <br />B. False
   
5. Executing `seq(1,10)` returns the same results as `1:10`.
   <br />A. True
   
6. The following command is sufficient for creating a vector recognized by R as having three dates: `dates <- c("2022-01-01", "2022-01-02", "2022-01-03")`.
   <br />B. False
   
7. How are `while` and `for` loops different?

  The main difference between a `while` and `for` loop is that a `for` loop is used when the number of iterations is known, compared to a `while` loop where the code runs until a given condition is satisified.
  
8. If vectors `x1` and `x2` each hold 100 integers, how can we add each element of one to the respective element of the other using a single line of code?

  Thanks to vectorization, this is as simple as executing the following code: `x1 + x2`.

9. List elements can be referenced by index using double brackets (e.g., `my_lst[[5]]` for the 5th element of my_lst).
   <br />A. True
   
10. What are some examples in which a user-defined function (UDF) is needed?

  The use cases for UDFs are many. In analytics contexts, domain-specific algorithms often need to be developed, and UDFs provide the flexibility to implement the logic at scale.
  

**Measurement & Sampling**

1. Parameters are descriptions or characteristics of a sample, while statistics are descriptions or characteristics of a population.
   <br />B. False

2. How does a sampling frame differ from a sample?

   A sampling frame represents the subset of the population to which the researcher has access, while the sample is the subset of the sampling frame that ultimately participates in the research.

3. How does cluster sampling differ from stratified random sampling?

   It is the *clusters* that are selected at random with cluster sampling rather than the *individuals* with stratified random sampling.

4. What is the primary benefit of probabilistic sampling methods over non-probabilistic sampling?

   Probability sampling allows us to make inferences about a larger population based on sample data, while this is not the case for non-probability sampling methods since the sample is unlikely to be representative of the population.

5. Nonresponse bias is only applicable in the context of surveys.
   <br />B. False

6. Which of the following variable types influences the strength of the effect one variable has on another?
   <br />C. Moderating Variable
   
7. 100 randomly selected employees in the Marketing department of an organization participated in a survey on career pathing for marketing professionals. What is the sample and what is the population sampled in this case?
   <br />B. Sample: 100 employees who completed the survey, Population: Marketing employees
   
8. Zero represents the absence of something for inveral scaled variables, whereas this is not the case for ratio scaled variables.
   <br />B. False
   
9. Discrete variables can have more than 2 values.
   <br />A. True
   
10. Which of the following are NOT probabilistic sampling methods?
   <br />B. Quota sampling
   <br />D. Purposive sampling


**Research Fundamentals**

1. What type of research method and design would be best suited for a study aiming to understand the effect of stay interviews on employee attrition?

  A quantitative research method and quasi-experimental design would be best suited for this study. While experimental designs are more rigorous than quasi-experiments, stay interviews are unlikely to be initiated via random processes; leaders will likely identify select people as having a higher risk of attrition and initiate stay interviews accordingly.

2. Why are quasi-experiments less rigorous than true experiments?

  Randomized experimental designs provide the most rigor with regard to causal validity, but random assignment is not performed in a quasi-experimental context.

3. Why aren't experimental designs always implemented when an experimenter-manipulated IV is warranted?

  Randomized experimental designs provide the most rigor with regard to causal validity. However, in social science research contexts, true experiments often are not possible due to ethical considerations.
  
4. What is the role of research questions?

  Research questions help focus the study, determine the appropriate methodology, and guide each stage of inquiry, analysis, and reporting.

5. What is the role of research hypotheses?

  Research hypotheses are testable statements about the expected outcome of a research project or experiment.
  
6. What is the difference between internal and external validity, and why are these concepts important in research?

  Internal validity reflects the robustness of the study -- the extent to which confounding variables are controlled. External validity refers to the extent to which study conclusions will hold in other contexts (for other people, in other places, at other times). Confidence in conclusions about causal effects or statistical relationships, and the likelihood of such conclusions to hold true in other contexts, is rooted in a study's internal and external validity.
  
7. What is an example of a mixed methods study?

  One example of a mixed methods study is a two-part study in which part 1 involves employee focus groups to surface themes that are top-of-mind for employees, and part 2 involves a survey to measure constructs related to the emergent themes in order to test statistical relationships with outcomes like engagement and retention.
  
8. What is the key difference between experimental and non-experimental research designs?

  Unlike experimental designs, non-experiments do not involve the manipulation of an IV. The goal of non-experiments is not to provide evidence for causal effects, but to study measured variables as they naturally occur.

9. What are the differences between cross-sectional, correlational, and observational non-experimental designs?

  Cross-sectional research compares two or more natural groups of people. Correlational research involves studying the statistical relationship between two variables. Observational research refers to studies in which the researcher gathers information without research subjects being explicitly involved in the recording of data.

10. How can the Hawthorne Effect impact the integrity of an experiment?

  Observed differences in a DV may not be attributable to the manipulation of an IV but merely due to research subjects having knowledge that they are being observed and temporarily modifying behavior.


**Univariate & Bivariate Analysis**

1. Which of the following measures of central tendency is least sensitive to extreme values (outliers)?
   <br />A. Median

2. The standard deviation represents the 'average' amount by which $x$ values deviate (or vary) from the mean. A large standard deviation indicates there is considerable spread in the data, whereas a small standard deviation indicates the mean is fairly representative of the data.
   <br />A. True
   
3. A positively skewed distribution has its largest allocation to the left and a negative distribution to the right.
   <br />B. False

4. Large covariance coefficients always indicate strong bivariate associations.
   <br />B. False
   
5. Which of the following can be found in boxplots?
   <br />A. Quartiles
   <br />B. Median
   <br />D. IQR
   <br />E. Outliers
   
6. The 3rd quartile (Q3) is equivalent to the 75th percentile.
   <br />A. True
   
7. Which of the following correlation coefficients can be used when evaluating the relationship between a pair of rank-ordered variables?
   <br />B. Spearman's Rank
   <br />E. Kendall's Rank
   
8. Which of the following correlation coefficients can be used when evaluating the relationship between a pair of dichotomous variables?
   <br />C. Phi
   
9. Platykurtic distributions are flat relative to mesokurtic distributions.
   <br />A. True
   
10. When using the Pearson method, values down the diagonal of a covariance matrix represent the variance for each variable.
   <br />A. True
   
   
**Inferential Statistics**
   
1. Which of the following is an example of a null hypothesis, where $\mu$ reflects the mean of a population?
   <br />A. $\mu_A = \mu_B$
   
2. Which of the following describes a Type I Error?
   <br />E. Both B and C

3. The primary purpose of inferential statistics is to make inferences about a population based on sample data. Inferential statistics allows these inferences to be made with defined levels of confidence that what is observed in a sample is also characteristic of the larger population.
   <br />A. True

4. A T-Test should be used when $\sigma$ is unknown and/or $n$ < 30.
   <br />A. True
   
5. Randomness is essential to probabilistic methods.
   <br />A. True

6. Which of the following is characteristic of the Bonferroni Correction?
   <br />A. Reducing the risk of a Type I Error
   <br />B. Increasing the risk of a Type II Error
   <br />C. Reducing the familywise error rate
   
7. Tolerance for a wider interval is an important tradeoff decision when increasing the level of confidence that a range of values contains an unknown population parameter.
   <br />A. True

8. A $CI$ represents the range of values we expect to include an unknown population parameter (often the mean) for a specified degree of confidence.
   <br />A. True
   
9. When population parameters are unknown, which of the following tests is most appropriate for testing $\mu_A = \mu_B$?
   <br />B. T-Test
   
10. According to the Empirical Rule, 95% of normally distributed data lie within how many standard deviations of the mean?
   <br />B. 2


** Data Wrangling and Preparation**

1. What are the differences between data lakes, data warehouses, and data marts?

  The main difference between a data lake and data warehouse is the type of data they are designed to store. A data lake stores myriad types of data -- both structured and unstructured -- in its native format until needed. A data warehouse (DW) is designed to support analytics across large collections of data, such as transactional data (e.g., point-of-sale systems), point-in-time snapshots (e.g., month-end close reports), survey responses, spreadsheets, and more. A data mart is a subset of a DW designed to easily deliver specific information to a certain set of users on a particular subject or for a well-defined use case.

2. What is the difference between a Type 1 and Type 2 table in a DW?

   A Type 1 table is overwritten on a regular cadence (usually daily) and contains no history -- only current values. A Type 2 table is a table in which a new record is inserted when a change occurs for one or more specified dimensions.

3. What two clauses must always be present in a SQL query?

  The `SELECT` and `FROM` clauses are required in a SQL query.

4. How do aggregate functions differ from window functions in SQL?

  Unlike the aggregate functions we've covered, window functions do not collapse rows into a single value; the calculated value is returned for each of the rows over which the calculation was performed.

5. What is a subquery?

  Subqueries are queries nested within other queries.

6. What is the difference between an INNER JOIN and LEFT JOIN?

  Inner joins result in the intersection of two tables, whereas outer joins result in the union of two tables.

7. Why is it dangeous to address missing values without domain knowledge of how the data are generated?

  There may be a logical reason for missing data, or the missing data may be informative regarding an outcome of interest. Addressing missing values without an understanding of the underlying data generative process, or missingness patterning, could result in improper handling that could bias analyses.

8. How can missing values be addressed when impacted records cannot be eliminated from a data set?

  Data imputation is a common way to address missing data, which involves replacing missing values with a descriptive statistic such as the mean, median, or mode based on available data.

9. When is one-hot encoding required for categorical variables?

  For statistical and ML modeling applications, categorical variables must be converted into $k-1$ variables, where $k$ is the number of categories, using binary (1/0) coding.

10. Why should variables with low to no variability be dropped?

  Variables with low variability often do not provide sufficient information for identifying patterns in data. When working with survey data, constant responses across all survey items (i.e., straightlining) may be evidence that the respondent lost motivation or was not attentive and thoughtful when taking the survey.
  

## 4D Framework

<i>
```{r 4d-overview, out.width = "75%", echo = FALSE, fig.cap = 'Figure 1: 4D Framework'}

knitr::include_graphics("/Users/craig.starbuck/Library/Mobile Documents/com~apple~CloudDocs/Documents/People Analytics Book/GitHub/peopleanalytics_lifecycle_book/graphics/4d_framework.png")

```
</i>

<br />

<b>1. Discover</b>

  You are likely familiar with the old adage: "An ounce of prevention is worth a pound of cure." Such is the case with respect to planning in an analytics context. During the Discover phase, it is important to remain in the problem zone; seek to understand your clients' needs through active listening and questions. This is not the time for solutioning or committing to any specific deliverables. If the client's needs are ambiguous, proceeding will likely be an exercise in futility. Outlined below is a set of general questions that should be considered during this initial phase to prevent allocating scarce time and resourcing to a project that ultimately misses the mark.

* Client

  Who is the client? A client can be a person or organization that has contracting you for consulting services, or an internal stakeholder within your organization who has need. What is important to them?

* Primary Objective

  + What is the client ultimately hoping to accomplish? 
  + Is the request merely to satisfy one’s curiosity, or are there actions that can realistically be taken to materially influence said objective? 

* Problem Statement

  + One of my most important early steps is clearly defining the problem statement. If your understanding of the problem -- after translating from the business terms in which it was initially expressed -- is misaligned with the client's needs, none of the subsequent steps matter.

* Guiding Theories

  + What theoretical explanations can the client offer as potential rationalizations for the phenomena of interest?
  + Are there existing theories in the organizational literature that should guide how the problem is tackled (e.g., findings from similar research implemented in other contexts)?

* Research Questions

  To respect the nuances of the problem statement, it is important to unpack it and frame as a set of overarching questions to guide the research.

  + Q1: ...
  + Q2: ...
  + Q3: ...

* Research Hypotheses

  Once research questions are developed, what do you expect to find based on anecdotal stories or empirical findings? As a next step, these expectations should be expressed in the form of research hypotheses. Please note that these research hypotheses are different from statistical hypotheses.

  + H1: ...
  + H2: ...
  + H3: ...

  To ensure the hypotheses lend themselves to actionable analyses, it is important to consider the following: "What does success look like?" In other words, once the project is complete, against which success measures will the project's success be determined? Curiosity is not a business reason and hope is not a reasonable strategy. The following questions may prove helpful in the promotion of actionable -- over merely interesting outcomes:

  + What will be done if the hypotheses are empirically supported?
  + What will be done if the hypotheses are <b> not </b> empirically supported?

* Assumptions

  At this point, it's helpful to consider what assumptions may be embedded in this discovery work. Are the questions and hypotheses rooted in what the client has theorized, or are these the product of an ambiguous understanding of the client's needs?

* Cadence
 
  + Is this analysis a one-off, or could there be a need to refresh this analysis on a regular cadence?
  + Are there dates associated with programs, actions, etc. this analysis is intended to support?

* Aggregation

  Is there a need for individual-level detail supporting the analysis? Aaggregate data should generally be the default unless a compelling justification exists and approval from legal and privacy partners is granted. One important role of analysts is to help keep the audience focused on the bigger picture and findings. Access to individual-level detail can not only introduce unnecessary legal and compliance risk but can also lead to questions and probing that can delay taking needed actions based on the results.

* Deliverable

  What is the preferred method of communicating the results of the analysis (e.g., interactive dashboard, static slide deck, document)? It is important to determine this early so that subsequent efforts can be structured to support the preferred deliverable. For example, if an interactive dashboard is preferred, does your Engineering department need to prioritize dependent tasks such as data feeds, row-level security, BI development, and production server migrations?

* Filters & Dimensions

  How does your client prefer to segment the workforce? Some common grouping dimensions are business unit, division, team, job family, location, tenure, and management level.
  
<br />
  
<b>2. Design</b>

  Perhaps the most important initial question to answer in the design phase is: "Does anything already exist that addresses part, or all, of the client’s objectives?" If the existing solution will suffice, it's possible that there is simply a communication/education gap, and you can allocate time and resources elsewhere.
  
  The end-user experience is of paramount importance during the Design phase, as solutions should have a consistent look and feel regardless of who developed the product. To achieve this, it is important to resist siloed thinking and consider the broader set of analytics solutions the team has delivered -- or is in the process of delivering.

* Data Privacy

  Are there potential concerns with the study’s objective, planned actions, and/or requested data elements from an employee privacy or legal perspective? A cross-functional data governance committee can help with efficient and consistent decisioning on requests for people data and analytics.

* Data Sources & Elements

  + What data sources are required?
  + What data elements are required?
  
  In cases where sensitive attributes such as gender, ethnicity, age, sexual orientation, and disability status are requested, it's always best to exercise a 'safety first' mentality and consult with legal and privacy partners to ensure there is comfort with the intended use of the data. The decision on whether or not to include these sensitive data elements is often less about what the audience can view (e.g., People Partners may already have access to the information at the person level in the source system) and more anchored in what they plan to do with the information.

  Is the required data already accessible in a data warehouse or other analytics environment? If not, does it need to be? What is required to achieve this?

* Data Quality

  It is important to understand the data generative process and never make assumptions about how anomalies or missing data should be interpreted. After identifying what data sources will be required for a particular analysis, it is important to meet with source system owners and data stewards to deeply understand the business processes by which data are generated in the system(s). Are there data quality concerns that need to be explored and addressed?

* Variables

  How will the constructs be measured (e.g., survey instrument, derived attribute, calculated field)?

* Analysis Method

  What are the appropriate analysis methods based on the research hypotheses? If modeling is required, is it more important to index on accuracy or interpretability?

* Dependencies

  Are other teams required to develop this solution? What is the nature of the work each dependent team will perform? Are there required system configuration changes? Do these teams have capacity to support?

* Change Management

  Will this solution impact current processes or solutions? If so, what is the change management plan to facilitate a seamless transition and user experience?

* Sign-Off

  Generally, it is best for the client to signoff on the problem statement, analysis approach, and wire frame for the deliverable (if applicable) before providing an ETA and proceeding to the development phase. This ensures alignment on the client's needs and the perceived utility of the solution in addressing those needs.

<br />

<b>3. Develop</b>

* Development Patterns

  + Are there development patterns that should guide the development approach to support consistency? 
  + Are there existing calculated fields that can/should be leveraged for derived data?
  + Are there best practices that should be employed to optimize performance (e.g., load time for dashboards, executing complex queries during non-peak times)?
  + Are there standard color palettes that should be applied?

* Productionalizable Code

  + How do models and data science pipelines need to be developed to facilitate a seamless migration from lower to upper environments? For example, initial exploratory data analysis (EDA) may be performed using curated data in flat files for the purpose of identifying meaningful trends, relationships, and differences, but where will this data need to be sourced in production to automate the refresh of models at a regular interval? If the data were provided from multiple source systems, what joins are required to integrate the data? What transformation logic or business rules need to be applied to reproduce the curated data?

* Unit Testing

  + What test cases will ensure the veracity of data?
  + Who will perform the testing?

* UAT Testing

  + In the spirit of agility and constant contact with the client to prevent surprises, it is generally a good idea to have the client take the solution for a test run within the UAT environment and then provide sign-off before migrating to production. If the deliverable is a deck or doc with results from a model, UAT may surface clarifying questions that can be addressed before releasing to the broader audience.

<br />

<b>4. Deliver</b>

  The Deliver phase can take many forms depending on the solution being released. If the solution is designed for a large user base, a series of recorded trainings may be in order so that there is a helpful reference for those unable to attend the live sessions or new joiners in the future. It is important to monitor success measures, which could be insights aligned to research hypotheses, dashboard utilization metrics, or any number of others defined within the Discover phase.
